{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3783084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import resnet50\n",
    "from torchvision.models.vgg import vgg16_bn\n",
    "from torchvision.models.densenet import densenet121\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from learn2learn.data import MetaDataset\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=16, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding_network = nn.Sequential(\n",
    "            nn.Linear(patch_size * patch_size * 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embed_dim),\n",
    "            nn.LayerNorm(embed_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # Split image into patches\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5)  # (B, H/16, W/16, C, patch_size, patch_size)\n",
    "        \n",
    "        # Flatten patches while keeping spatial information\n",
    "        patch_vectors = patches.reshape(B, H//self.patch_size, W//self.patch_size, -1)\n",
    "        \n",
    "        # Create embeddings for each patch\n",
    "        embeddings = self.embedding_network(patch_vectors)  # (B, 14, 14, embed_dim)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class EfficientNetPatchB4(nn.Module):\n",
    "    def __init__(self, patch_size=16, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate patch embeddings with spatial information preserved\n",
    "        patch_embeddings = self.patch_embed(x)  # (batch_size, 14, 14, embed_dim)\n",
    "        patch_embeddings = F.normalize(patch_embeddings, p=2, dim=-1)\n",
    "        return patch_embeddings\n",
    "\n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super().__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.embedding_net(x1)\n",
    "        output2 = self.embedding_net(x2)\n",
    "        output3 = self.embedding_net(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)\n",
    "\n",
    "class SpatialTripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(dim=-1)  # Sum over embedding dimension\n",
    "        distance_negative = (anchor - negative).pow(2).sum(dim=-1)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root=None, transforms=None):\n",
    "        img_folder = ImageFolder(root=root)\n",
    "        meta_data = MetaDataset(img_folder)\n",
    "        \n",
    "        self.img_list = img_folder.imgs\n",
    "        self.labels_to_indices = meta_data.labels_to_indices\n",
    "        self.indices_to_labels = meta_data.indices_to_labels\n",
    "        self.labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_anchor = self.img_list[index][0]\n",
    "        label_anchor = self.img_list[index][1]\n",
    "\n",
    "        idx_positive = random.choice(self.labels_to_indices[label_anchor])            \n",
    "        img_positive = self.img_list[idx_positive][0]\n",
    "        \n",
    "        aux_class = random.choice(list(set(self.labels) - set([label_anchor])))\n",
    "        idx_negative = random.choice(self.labels_to_indices[aux_class])\n",
    "        img_negative = self.img_list[idx_negative][0]\n",
    "\n",
    "        img_anchor = Image.open(img_anchor).convert('RGB')\n",
    "        img_positive = Image.open(img_positive).convert('RGB')\n",
    "        img_negative = Image.open(img_negative).convert('RGB')\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_anchor = self.transforms(img_anchor)\n",
    "            img_positive = self.transforms(img_positive)\n",
    "            img_negative = self.transforms(img_negative)\n",
    "\n",
    "        return (img_anchor, img_positive, img_negative), []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "def get_triplet_dataloader(root=None, batch_size=1, transforms=None):\n",
    "    dataset = TripletDataset(root=root, transforms=transforms)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def get_train_transforms():\n",
    "    return T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.RandomHorizontalFlip(0.5),\n",
    "        T.RandomVerticalFlip(0.5),\n",
    "        T.RandomApply([T.RandomRotation(10)], 0.25),\n",
    "        T.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, device, log_interval=100):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            img_a, img_p, img_n = data\n",
    "            img_a, img_p, img_n = img_a.to(device), img_p.to(device), img_n.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(img_a, img_p, img_n)\n",
    "            loss = loss_fn(*embeddings)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(f'Epoch {epoch}: [{batch_idx * len(img_a)}/{len(train_loader.dataset)}] '\n",
    "                      f'Loss: {total_loss / (batch_idx + 1):.6f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in val_loader:\n",
    "                img_a, img_p, img_n = data\n",
    "                img_a, img_p, img_n = img_a.to(device), img_p.to(device), img_n.to(device)\n",
    "                embeddings = model(img_a, img_p, img_n)\n",
    "                val_loss += loss_fn(*embeddings).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch {epoch}: Validation Loss: {val_loss:.6f}')\n",
    "\n",
    "# Initialize model and training\n",
    "path_data = 'C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "embedding_net = EfficientNetPatchB4(patch_size=16, embed_dim=128)\n",
    "triplet_model = TripletNet(embedding_net=embedding_net)\n",
    "loss_fn = SpatialTripletLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(triplet_model.parameters(), lr=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    triplet_model = triplet_model.cuda()\n",
    "\n",
    "# Create dataloaders\n",
    "triplet_train_loader = get_triplet_dataloader(\n",
    "    root=path_data + '/train/',\n",
    "    batch_size=5,\n",
    "    transforms=get_train_transforms()\n",
    ")\n",
    "triplet_val_loader = get_triplet_dataloader(\n",
    "    root=path_data + '/val/',\n",
    "    batch_size=5,\n",
    "    transforms=get_val_transforms()\n",
    ")\n",
    "\n",
    "# Train\n",
    "n_epochs = 100\n",
    "fit(triplet_train_loader, triplet_val_loader, triplet_model, loss_fn, optimizer, scheduler, n_epochs, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
