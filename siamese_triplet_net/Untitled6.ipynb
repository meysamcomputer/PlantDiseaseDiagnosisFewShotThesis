{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc587a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "import cv2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dataloaders import get_train_transforms, get_val_transforms, get_triplet_dataloader\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score , precision_score , recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "device = torch.cuda.is_available()\n",
    "# استخراج ویژگی‌ها با استفاده از مدل Siamese\n",
    " \n",
    "def generate_embeddings(data_loader, model):\n",
    "    with torch.no_grad():\n",
    "        #device = 'cuda'\n",
    "        model.eval()\n",
    "        #model.to(device)\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        for batch_imgs, batch_labels in data_loader:\n",
    "            if device:\n",
    "                batch_imgs = batch_imgs.cuda()\n",
    "            batch_E = model.get_embedding(batch_imgs)\n",
    "            embeddings.append(batch_E.cpu().numpy())\n",
    "            labels.append(batch_labels.numpy())\n",
    "    return np.concatenate(embeddings), np.concatenate(labels)\n",
    " \n",
    "\n",
    "# ذخیره مدل\n",
    "#torch.save(siamese_model.state_dict(), \"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/siamese_model_EmbeddingGEnerator_State_improved.h5\")\n",
    "#torch.save(siamese_model , \"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/siamese_model_EmbeddingGEnerator.h5\")\n",
    "\n",
    "siamese_model = torch.load(\"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/siamese_model_EmbeddingGEnerator.h5\",map_location=torch.device('cuda'))\n",
    "siamese_model.eval() \n",
    "print(\"siamese_model\")\n",
    "# تعریف مدل سفارشی ViT\n",
    "class RefinedViT(nn.Module):\n",
    "    def __init__(self, original_vit_model,embedding_dim,  num_classes):\n",
    "        super(RefinedViT, self).__init__()\n",
    "        # حفظ لایه‌های اصلی ViT\n",
    "        self.num_patches = (224 // 16) * (224 // 16)  # برای تصویر 224x224 با پچ 16x16\n",
    "        self.embed_dim = 768  # سایز استاندارد برای vit-base\n",
    "        \n",
    "        # تبدیل embedding به پچ‌ها\n",
    "        self.embedding_to_patch = nn.Linear(1792, self.num_patches * self.embed_dim)\n",
    "       # self.embedding_to_patch = nn.Linear(1280, self.num_patches * self.embed_dim)\n",
    "        #self.embedding_to_patch = nn.Linear(embedding_dim,  self.num_patches * self.embed_dim)\n",
    "        \n",
    "        # Position Embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.embed_dim))\n",
    "        \n",
    "        # CLS Token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=12,  # تعداد heads در self-attention\n",
    "            dim_feedforward=3072,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=12)\n",
    "        \n",
    "        # لایه‌های نهایی\n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc = nn.Linear(self.embed_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # تبدیل embedding به پچ‌ها\n",
    "        x = self.embedding_to_patch(x)  # (batch_size, num_patches * embed_dim)\n",
    "        x = x.view(batch_size, self.num_patches, self.embed_dim)  # (batch_size, num_patches, embed_dim)\n",
    "       \n",
    "        # اضافه کردن CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (batch_size, num_patches + 1, embed_dim)\n",
    "        \n",
    "        # اضافه کردن position embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # استفاده از CLS token برای طبقه‌بندی\n",
    "        x = x[:, 0]  # فقط CLS token\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x  \n",
    "    def get_embedding(self, x):\n",
    "        \"\"\"\n",
    "        دریافت embedding برای استفاده در t-SNE\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # تغییر شکل ورودی\n",
    "        if len(x.shape) == 4:\n",
    "            x = x.view(batch_size, -1)  # تبدیل به بردار یک‌بعدی\n",
    "        \n",
    "        # تغییر ابعاد برای تطابق با عملیات ضرب ماتریسی\n",
    "        x = x.reshape(batch_size, 1792, -1)  # تقسیم 150528 به 1792 و باقیمانده\n",
    "        x = x.mean(dim=-1)  # میانگین‌گیری روی بعد آخر\n",
    "        \n",
    "        # حالا x دارای ابعاد (batch_size, 1792) است\n",
    "        x = self.embedding_to_patch(x)  # عملیات خطی\n",
    "        x = x.view(batch_size, self.num_patches, self.embed_dim)\n",
    "        \n",
    "        # اضافه کردن CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # اضافه کردن position embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # استفاده از CLS token\n",
    "        x = x[:, 0]\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "     \n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=path_data + '/train/', transform=get_val_transforms())\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(root=path_data + '/test/', transform=get_val_transforms())\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# بارگذاری مدل ViT از Hugging Face\n",
    "vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model.classifier = torch.nn.Linear(vit_model.config.hidden_size, 15)  # تعداد کلاس‌ها\n",
    "\n",
    "embedding_dim = vit_model.config.hidden_size  # ابعاد embedding\n",
    "model = RefinedViT(vit_model, embedding_dim, num_classes=15)\n",
    "print(\"model\")\n",
    "# استخراج ویژگی‌ها از داده‌های آموزش\n",
    "train_embeddings, train_labels = generate_embeddings(train_loader, siamese_model )\n",
    "# استخراج ویژگی‌ها از داده‌های تست\n",
    "test_embeddings, test_labels = generate_embeddings(test_loader, siamese_model )\n",
    " \n",
    "# تبدیل داده‌ها به تنسور\n",
    "X_train, y_train = train_embeddings, train_labels\n",
    "X_val, y_val = test_embeddings, test_labels\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# انتقال مدل به دستگاه CUDA\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# تنظیمات loss function و optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)  # استفاده از AdamW\n",
    "\n",
    "# انتقال به GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# تنظیمات آموزش\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "print(\"train\")\n",
    "# آموزش\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor.to(device))\n",
    "    loss = criterion(outputs, y_train_tensor.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    " \n",
    "\n",
    "torch.save(model,\"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/RVIT_improved.h5\");\n",
    " \n",
    "# ارزیابی مدل\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_val_tensor.to(device))\n",
    "    _, predicted = torch.max(val_outputs.data, 1)\n",
    "\n",
    "# ذخیره مدل\n",
    "torch.save(model.state_dict(), \"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/RVIT_State_improved.h5\")\n",
    " \n",
    "\n",
    "\n",
    "# محاسبه دقت و امتیاز F1\n",
    "accuracy = accuracy_score(y_val_tensor.cpu(), predicted.cpu())\n",
    "f1 = f1_score(y_val_tensor.cpu(), predicted.cpu(), average='weighted')\n",
    "precision = precision_score(y_val_tensor.cpu(), predicted.cpu(), average='macro') \n",
    "recall = recall_score(y_val_tensor.cpu(), predicted.cpu(), average='macro')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
