{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ee0cc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 47:20, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.703568</td>\n",
       "      <td>0.189229</td>\n",
       "      <td>0.126547</td>\n",
       "      <td>0.196929</td>\n",
       "      <td>0.189229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.629801</td>\n",
       "      <td>0.283115</td>\n",
       "      <td>0.216089</td>\n",
       "      <td>0.389690</td>\n",
       "      <td>0.283115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.575210</td>\n",
       "      <td>0.331878</td>\n",
       "      <td>0.278573</td>\n",
       "      <td>0.420165</td>\n",
       "      <td>0.331878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.512361</td>\n",
       "      <td>0.405871</td>\n",
       "      <td>0.372301</td>\n",
       "      <td>0.454413</td>\n",
       "      <td>0.405871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.464804</td>\n",
       "      <td>0.437894</td>\n",
       "      <td>0.409648</td>\n",
       "      <td>0.463512</td>\n",
       "      <td>0.437894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.413861</td>\n",
       "      <td>0.467977</td>\n",
       "      <td>0.441855</td>\n",
       "      <td>0.489784</td>\n",
       "      <td>0.467977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.372236</td>\n",
       "      <td>0.476468</td>\n",
       "      <td>0.450578</td>\n",
       "      <td>0.505219</td>\n",
       "      <td>0.476468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.339723</td>\n",
       "      <td>0.488355</td>\n",
       "      <td>0.467950</td>\n",
       "      <td>0.521171</td>\n",
       "      <td>0.488355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.299177</td>\n",
       "      <td>0.485929</td>\n",
       "      <td>0.457268</td>\n",
       "      <td>0.522016</td>\n",
       "      <td>0.485929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.258552</td>\n",
       "      <td>0.505095</td>\n",
       "      <td>0.479813</td>\n",
       "      <td>0.538071</td>\n",
       "      <td>0.505095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.218892</td>\n",
       "      <td>0.522319</td>\n",
       "      <td>0.493823</td>\n",
       "      <td>0.545190</td>\n",
       "      <td>0.522319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.181857</td>\n",
       "      <td>0.528142</td>\n",
       "      <td>0.506213</td>\n",
       "      <td>0.559970</td>\n",
       "      <td>0.528142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.160447</td>\n",
       "      <td>0.534692</td>\n",
       "      <td>0.515093</td>\n",
       "      <td>0.561412</td>\n",
       "      <td>0.534692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.119111</td>\n",
       "      <td>0.562106</td>\n",
       "      <td>0.548504</td>\n",
       "      <td>0.589151</td>\n",
       "      <td>0.562106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.106748</td>\n",
       "      <td>0.552402</td>\n",
       "      <td>0.536193</td>\n",
       "      <td>0.584028</td>\n",
       "      <td>0.552402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.079833</td>\n",
       "      <td>0.561378</td>\n",
       "      <td>0.546251</td>\n",
       "      <td>0.596773</td>\n",
       "      <td>0.561378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.044590</td>\n",
       "      <td>0.566715</td>\n",
       "      <td>0.549430</td>\n",
       "      <td>0.592601</td>\n",
       "      <td>0.566715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.030180</td>\n",
       "      <td>0.574478</td>\n",
       "      <td>0.562851</td>\n",
       "      <td>0.606874</td>\n",
       "      <td>0.574478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.995983</td>\n",
       "      <td>0.581514</td>\n",
       "      <td>0.567487</td>\n",
       "      <td>0.603700</td>\n",
       "      <td>0.581514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.982008</td>\n",
       "      <td>0.589034</td>\n",
       "      <td>0.576307</td>\n",
       "      <td>0.617550</td>\n",
       "      <td>0.589034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.949668</td>\n",
       "      <td>0.587821</td>\n",
       "      <td>0.573439</td>\n",
       "      <td>0.610564</td>\n",
       "      <td>0.587821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.936037</td>\n",
       "      <td>0.592916</td>\n",
       "      <td>0.581060</td>\n",
       "      <td>0.611586</td>\n",
       "      <td>0.592916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.919598</td>\n",
       "      <td>0.600194</td>\n",
       "      <td>0.591388</td>\n",
       "      <td>0.618155</td>\n",
       "      <td>0.600194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.885202</td>\n",
       "      <td>0.599951</td>\n",
       "      <td>0.584803</td>\n",
       "      <td>0.614078</td>\n",
       "      <td>0.599951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.881371</td>\n",
       "      <td>0.605531</td>\n",
       "      <td>0.594678</td>\n",
       "      <td>0.623027</td>\n",
       "      <td>0.605531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.865606</td>\n",
       "      <td>0.607472</td>\n",
       "      <td>0.599772</td>\n",
       "      <td>0.624793</td>\n",
       "      <td>0.607472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.843779</td>\n",
       "      <td>0.609656</td>\n",
       "      <td>0.599657</td>\n",
       "      <td>0.629678</td>\n",
       "      <td>0.609656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.833452</td>\n",
       "      <td>0.606987</td>\n",
       "      <td>0.599092</td>\n",
       "      <td>0.624259</td>\n",
       "      <td>0.606987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.811938</td>\n",
       "      <td>0.617176</td>\n",
       "      <td>0.609384</td>\n",
       "      <td>0.633010</td>\n",
       "      <td>0.617176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.800897</td>\n",
       "      <td>0.613052</td>\n",
       "      <td>0.605882</td>\n",
       "      <td>0.627984</td>\n",
       "      <td>0.613052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.789904</td>\n",
       "      <td>0.617904</td>\n",
       "      <td>0.610832</td>\n",
       "      <td>0.636156</td>\n",
       "      <td>0.617904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.780671</td>\n",
       "      <td>0.621300</td>\n",
       "      <td>0.617389</td>\n",
       "      <td>0.636282</td>\n",
       "      <td>0.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.755662</td>\n",
       "      <td>0.619117</td>\n",
       "      <td>0.612715</td>\n",
       "      <td>0.633987</td>\n",
       "      <td>0.619117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.752453</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.636633</td>\n",
       "      <td>0.618632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.741799</td>\n",
       "      <td>0.622999</td>\n",
       "      <td>0.618639</td>\n",
       "      <td>0.637974</td>\n",
       "      <td>0.622999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.728734</td>\n",
       "      <td>0.625667</td>\n",
       "      <td>0.621350</td>\n",
       "      <td>0.643883</td>\n",
       "      <td>0.625667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.718574</td>\n",
       "      <td>0.623726</td>\n",
       "      <td>0.618769</td>\n",
       "      <td>0.639074</td>\n",
       "      <td>0.623726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.704539</td>\n",
       "      <td>0.629549</td>\n",
       "      <td>0.623227</td>\n",
       "      <td>0.644769</td>\n",
       "      <td>0.629549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.701500</td>\n",
       "      <td>0.625425</td>\n",
       "      <td>0.622698</td>\n",
       "      <td>0.644556</td>\n",
       "      <td>0.625425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.694498</td>\n",
       "      <td>0.624212</td>\n",
       "      <td>0.621079</td>\n",
       "      <td>0.642847</td>\n",
       "      <td>0.624212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.682657</td>\n",
       "      <td>0.630034</td>\n",
       "      <td>0.627841</td>\n",
       "      <td>0.647556</td>\n",
       "      <td>0.630034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.671876</td>\n",
       "      <td>0.629306</td>\n",
       "      <td>0.624808</td>\n",
       "      <td>0.645041</td>\n",
       "      <td>0.629306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.668107</td>\n",
       "      <td>0.627851</td>\n",
       "      <td>0.625463</td>\n",
       "      <td>0.645895</td>\n",
       "      <td>0.627851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.663724</td>\n",
       "      <td>0.625182</td>\n",
       "      <td>0.622735</td>\n",
       "      <td>0.644222</td>\n",
       "      <td>0.625182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.657430</td>\n",
       "      <td>0.629064</td>\n",
       "      <td>0.627152</td>\n",
       "      <td>0.646640</td>\n",
       "      <td>0.629064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.647921</td>\n",
       "      <td>0.629791</td>\n",
       "      <td>0.627040</td>\n",
       "      <td>0.647369</td>\n",
       "      <td>0.629791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.639719</td>\n",
       "      <td>0.630034</td>\n",
       "      <td>0.628051</td>\n",
       "      <td>0.648008</td>\n",
       "      <td>0.630034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.635994</td>\n",
       "      <td>0.637312</td>\n",
       "      <td>0.634614</td>\n",
       "      <td>0.652681</td>\n",
       "      <td>0.637312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.631626</td>\n",
       "      <td>0.633916</td>\n",
       "      <td>0.631844</td>\n",
       "      <td>0.649870</td>\n",
       "      <td>0.633916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.625647</td>\n",
       "      <td>0.632945</td>\n",
       "      <td>0.630357</td>\n",
       "      <td>0.650841</td>\n",
       "      <td>0.632945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.623777</td>\n",
       "      <td>0.631490</td>\n",
       "      <td>0.629239</td>\n",
       "      <td>0.648991</td>\n",
       "      <td>0.631490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.624489</td>\n",
       "      <td>0.631490</td>\n",
       "      <td>0.629904</td>\n",
       "      <td>0.651850</td>\n",
       "      <td>0.631490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.614311</td>\n",
       "      <td>0.636099</td>\n",
       "      <td>0.634188</td>\n",
       "      <td>0.653229</td>\n",
       "      <td>0.636099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.610631</td>\n",
       "      <td>0.634401</td>\n",
       "      <td>0.632856</td>\n",
       "      <td>0.651887</td>\n",
       "      <td>0.634401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.609782</td>\n",
       "      <td>0.633188</td>\n",
       "      <td>0.631126</td>\n",
       "      <td>0.651476</td>\n",
       "      <td>0.633188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.603533</td>\n",
       "      <td>0.634401</td>\n",
       "      <td>0.631202</td>\n",
       "      <td>0.651152</td>\n",
       "      <td>0.634401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.605114</td>\n",
       "      <td>0.632703</td>\n",
       "      <td>0.631543</td>\n",
       "      <td>0.653494</td>\n",
       "      <td>0.632703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.595849</td>\n",
       "      <td>0.635614</td>\n",
       "      <td>0.633289</td>\n",
       "      <td>0.651908</td>\n",
       "      <td>0.635614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.595271</td>\n",
       "      <td>0.633430</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.652976</td>\n",
       "      <td>0.633430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.592843</td>\n",
       "      <td>0.633673</td>\n",
       "      <td>0.632063</td>\n",
       "      <td>0.652680</td>\n",
       "      <td>0.633673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.590960</td>\n",
       "      <td>0.636099</td>\n",
       "      <td>0.634575</td>\n",
       "      <td>0.654294</td>\n",
       "      <td>0.636099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.588325</td>\n",
       "      <td>0.635614</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>0.654025</td>\n",
       "      <td>0.635614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.588338</td>\n",
       "      <td>0.632703</td>\n",
       "      <td>0.631580</td>\n",
       "      <td>0.652359</td>\n",
       "      <td>0.632703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.582848</td>\n",
       "      <td>0.635129</td>\n",
       "      <td>0.633463</td>\n",
       "      <td>0.653154</td>\n",
       "      <td>0.635129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.582541</td>\n",
       "      <td>0.635129</td>\n",
       "      <td>0.633353</td>\n",
       "      <td>0.654026</td>\n",
       "      <td>0.635129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>0.635129</td>\n",
       "      <td>0.633605</td>\n",
       "      <td>0.653637</td>\n",
       "      <td>0.635129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.578483</td>\n",
       "      <td>0.634643</td>\n",
       "      <td>0.633075</td>\n",
       "      <td>0.654112</td>\n",
       "      <td>0.634643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.576666</td>\n",
       "      <td>0.635129</td>\n",
       "      <td>0.633921</td>\n",
       "      <td>0.654919</td>\n",
       "      <td>0.635129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.574203</td>\n",
       "      <td>0.635614</td>\n",
       "      <td>0.634319</td>\n",
       "      <td>0.654994</td>\n",
       "      <td>0.635614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.572930</td>\n",
       "      <td>0.634886</td>\n",
       "      <td>0.633148</td>\n",
       "      <td>0.654080</td>\n",
       "      <td>0.634886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.570257</td>\n",
       "      <td>0.635129</td>\n",
       "      <td>0.633556</td>\n",
       "      <td>0.654451</td>\n",
       "      <td>0.635129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.569580</td>\n",
       "      <td>0.635371</td>\n",
       "      <td>0.634255</td>\n",
       "      <td>0.654964</td>\n",
       "      <td>0.635371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.567040</td>\n",
       "      <td>0.636099</td>\n",
       "      <td>0.635080</td>\n",
       "      <td>0.655762</td>\n",
       "      <td>0.636099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.566056</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.634781</td>\n",
       "      <td>0.655529</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.564509</td>\n",
       "      <td>0.635614</td>\n",
       "      <td>0.634427</td>\n",
       "      <td>0.655375</td>\n",
       "      <td>0.635614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.562703</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.634115</td>\n",
       "      <td>0.654628</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.562138</td>\n",
       "      <td>0.635371</td>\n",
       "      <td>0.634020</td>\n",
       "      <td>0.655374</td>\n",
       "      <td>0.635371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.560471</td>\n",
       "      <td>0.634401</td>\n",
       "      <td>0.632864</td>\n",
       "      <td>0.653597</td>\n",
       "      <td>0.634401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.559004</td>\n",
       "      <td>0.635129</td>\n",
       "      <td>0.633742</td>\n",
       "      <td>0.654708</td>\n",
       "      <td>0.635129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.557289</td>\n",
       "      <td>0.636827</td>\n",
       "      <td>0.635354</td>\n",
       "      <td>0.655901</td>\n",
       "      <td>0.636827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.557394</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.634397</td>\n",
       "      <td>0.655264</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.556033</td>\n",
       "      <td>0.634643</td>\n",
       "      <td>0.633030</td>\n",
       "      <td>0.653960</td>\n",
       "      <td>0.634643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.554314</td>\n",
       "      <td>0.635371</td>\n",
       "      <td>0.633728</td>\n",
       "      <td>0.654550</td>\n",
       "      <td>0.635371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.553673</td>\n",
       "      <td>0.635371</td>\n",
       "      <td>0.633819</td>\n",
       "      <td>0.654537</td>\n",
       "      <td>0.635371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.552601</td>\n",
       "      <td>0.635371</td>\n",
       "      <td>0.633666</td>\n",
       "      <td>0.654675</td>\n",
       "      <td>0.635371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.551650</td>\n",
       "      <td>0.634886</td>\n",
       "      <td>0.632997</td>\n",
       "      <td>0.653934</td>\n",
       "      <td>0.634886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.550903</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.633980</td>\n",
       "      <td>0.654549</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.550300</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.633937</td>\n",
       "      <td>0.654839</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.549481</td>\n",
       "      <td>0.635129</td>\n",
       "      <td>0.633360</td>\n",
       "      <td>0.654243</td>\n",
       "      <td>0.635129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.548876</td>\n",
       "      <td>0.635129</td>\n",
       "      <td>0.633419</td>\n",
       "      <td>0.654435</td>\n",
       "      <td>0.635129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.548606</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.633954</td>\n",
       "      <td>0.654776</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.548251</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.634206</td>\n",
       "      <td>0.655053</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.547654</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.634032</td>\n",
       "      <td>0.654986</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.547413</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.634044</td>\n",
       "      <td>0.654969</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.547359</td>\n",
       "      <td>0.635614</td>\n",
       "      <td>0.633779</td>\n",
       "      <td>0.654919</td>\n",
       "      <td>0.635614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.546970</td>\n",
       "      <td>0.635614</td>\n",
       "      <td>0.633968</td>\n",
       "      <td>0.655152</td>\n",
       "      <td>0.635614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.546898</td>\n",
       "      <td>0.635614</td>\n",
       "      <td>0.633836</td>\n",
       "      <td>0.654870</td>\n",
       "      <td>0.635614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.546692</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.634170</td>\n",
       "      <td>0.655271</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.546553</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.634092</td>\n",
       "      <td>0.655173</td>\n",
       "      <td>0.635856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.164400</td>\n",
       "      <td>1.546510</td>\n",
       "      <td>0.636099</td>\n",
       "      <td>0.634357</td>\n",
       "      <td>0.655360</td>\n",
       "      <td>0.636099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "{'eval_loss': 1.546509861946106, 'eval_accuracy': 0.636098981077147, 'eval_f1': 0.6343566647344295, 'eval_precision': 0.6553597648224023, 'eval_recall': 0.636098981077147, 'eval_runtime': 25.2495, 'eval_samples_per_second': 163.251, 'eval_steps_per_second': 2.574, 'epoch': 100.0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5efba1d91efd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pixel_values\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Ÿàÿ±ŸàÿØ€å‚ÄåŸáÿß€å Ÿæ€åÿ¥ Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ¥ÿØŸá ÿ®Ÿá ŸÖÿØŸÑ ŸÖ€å‚ÄåÿØŸá€åŸÖ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mtrain_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         outputs = self.vit(\n\u001b[0m\u001b[0;32m    841\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    617\u001b[0m             \u001b[0mpixel_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         embedding_output = self.embeddings(\n\u001b[0m\u001b[0;32m    620\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool_masked_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m    122\u001b[0m     ) -> torch.Tensor:\n\u001b[0;32m    123\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbool_masked_pos\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m    181\u001b[0m                     \u001b[1;34mf\" ({self.image_size[0]}*{self.image_size[1]}).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 )\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "#!pip install transformers datasets      \n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "#sys.path.insert(0,'C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "\n",
    "#!pip install transformers datasets      \n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "#sys.path.insert(0,'C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å feature extractor ÿßÿ≤ ViT\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# ÿ™ÿπÿ±€åŸÅ transform ŸÖŸÜÿßÿ≥ÿ® ÿ®ÿ±ÿß€å Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ™ÿµÿßŸà€åÿ±\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ÿ™ÿ∫€å€åÿ± ÿßŸÜÿØÿßÿ≤Ÿá ÿ®Ÿá 224x224\n",
    "    transforms.ToTensor(),          # ÿ™ÿ®ÿØ€åŸÑ ÿ™ÿµŸà€åÿ± ÿ®Ÿá Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ŸÜÿ±ŸÖÿßŸÑ‚Äåÿ≥ÿßÿ≤€å\n",
    "])\n",
    "\n",
    "# ÿ≥ÿßÿÆÿ™ €å⁄© Dataset ÿ≥ŸÅÿßÿ±ÿ¥€å\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        # ÿ™ÿ®ÿØ€åŸÑ ÿ™ÿµŸà€åÿ± ÿ®Ÿá PIL.Image Ÿà ÿ≥Ÿæÿ≥ Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ®ÿß ViTImageProcessor\n",
    "        image = transforms.ToPILImage()(image)  # ÿ™ÿ®ÿØ€åŸÑ ÿßÿ≤ Tensor ÿ®Ÿá PIL.Image\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": label}\n",
    "    \n",
    "path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "# ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿØÿßÿØŸá‚ÄåŸáÿß ÿ®ÿß ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ CustomImageDataset\n",
    "train_dataset = CustomImageDataset(path_data + '/train/', transform=train_transform)\n",
    "test_dataset = CustomImageDataset(path_data + '/test/', transform=train_transform)\n",
    "\n",
    "# ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ŸÖÿØŸÑ ViT ÿ®ÿ±ÿß€å ÿ∑ÿ®ŸÇŸá‚Äåÿ®ŸÜÿØ€å ÿ™ÿµŸà€åÿ±\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=16)  # ÿ™ÿ∫€å€åÿ± ÿ™ÿπÿØÿßÿØ ⁄©ŸÑÿßÿ≥‚ÄåŸáÿß ÿ®Ÿá 16\n",
    "\n",
    "# ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ÿ¢ŸÖŸàÿ≤ÿ¥\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit_finetuned\",         # ŸÖ⁄©ÿßŸÜ€å ÿ®ÿ±ÿß€å ÿ∞ÿÆ€åÿ±Ÿá ŸÖÿØŸÑ ŸÜŸáÿß€å€å\n",
    "    evaluation_strategy=\"epoch\",          # ÿßÿ±ÿ≤€åÿßÿ®€å ÿØÿ± Ÿæÿß€åÿßŸÜ Ÿáÿ± epoch\n",
    "    learning_rate=2e-5,                   # ŸÜÿ±ÿÆ €åÿßÿØ⁄Ø€åÿ±€å ÿ®ÿ±ÿß€å fine-tuning\n",
    "    per_device_train_batch_size=16,       # ÿßŸÜÿØÿßÿ≤Ÿá ÿ®⁄Ü ÿ®ÿ±ÿß€å ÿ¢ŸÖŸàÿ≤ÿ¥\n",
    "    per_device_eval_batch_size=64,        # ÿßŸÜÿØÿßÿ≤Ÿá ÿ®⁄Ü ÿ®ÿ±ÿß€å ÿßÿ±ÿ≤€åÿßÿ®€å\n",
    "    num_train_epochs=100,                   # ÿ™ÿπÿØÿßÿØ epochs ÿ®ÿ±ÿß€å ÿ¢ŸÖŸàÿ≤ÿ¥\n",
    "    weight_decay=0.01,                    # ÿ™ŸÜÿ∏€åŸÖÿßÿ™ weight decay\n",
    "    save_total_limit=2,                   # ÿ∞ÿÆ€åÿ±Ÿá ÿ™ŸÜŸáÿß ÿ¢ÿÆÿ±€åŸÜ €≤ checkpoint\n",
    "    logging_dir='./logs',                 # ŸÖÿ≥€åÿ± ÿ∞ÿÆ€åÿ±Ÿá ŸÑÿß⁄Ø‚ÄåŸáÿß\n",
    ")\n",
    "\n",
    "# ÿ™ÿßÿ®ÿπ ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÖÿ™ÿ±€å⁄©‚ÄåŸáÿß\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# ÿ±ÿßŸá‚ÄåÿßŸÜÿØÿßÿ≤€å Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                           # ŸÖÿØŸÑ\n",
    "    args=training_args,                    # ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ÿ¢ŸÖŸàÿ≤ÿ¥\n",
    "    train_dataset=train_dataset,           # ÿØÿßÿØŸá‚ÄåŸáÿß€å ÿ¢ŸÖŸàÿ≤ÿ¥€å\n",
    "    eval_dataset=test_dataset,             # ÿØÿßÿØŸá‚ÄåŸáÿß€å ÿßÿ±ÿ≤€åÿßÿ®€å\n",
    "    compute_metrics=compute_metrics        # ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÖÿ™ÿ±€å⁄©‚ÄåŸáÿß\n",
    ")\n",
    "\n",
    "# ÿ¥ÿ±Ÿàÿπ ÿ¢ŸÖŸàÿ≤ÿ¥ ŸÖÿØŸÑ\n",
    "trainer.train()\n",
    "\n",
    "# ÿßÿ±ÿ≤€åÿßÿ®€å ŸÖÿØŸÑ Ÿæÿ≥ ÿßÿ≤ ÿ¢ŸÖŸàÿ≤ÿ¥\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(results)\n",
    "\n",
    "# ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß Ÿà ÿ®ÿ±⁄Üÿ≥ÿ®‚ÄåŸáÿß ÿ®ÿ±ÿß€å t-SNE\n",
    "model.eval()\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "# ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß ÿßÿ≤ ŸÖÿØŸÑ\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = model(pixel_values=inputs)  # Ÿàÿ±ŸàÿØ€å‚ÄåŸáÿß€å Ÿæ€åÿ¥ Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ¥ÿØŸá ÿ®Ÿá ŸÖÿØŸÑ ŸÖ€å‚ÄåÿØŸá€åŸÖ\n",
    "        features = outputs.logits.cpu().numpy()\n",
    "        train_features.extend(features)\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ t-SNE ÿ®ÿ±ÿß€å ⁄©ÿßŸáÿ¥ ÿßÿ®ÿπÿßÿØ\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(train_features)\n",
    "\n",
    "# ÿ±ÿ≥ŸÖ ŸÜÿ™€åÿ¨Ÿá t-SNE\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=train_labels, cmap='viridis', s=10)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"t-SNE Visualization of ViT Features\")\n",
    "plt.show()\n",
    "\n",
    "# ÿ∞ÿÆ€åÿ±Ÿá ŸÖÿØŸÑ ÿ¢ŸÖŸàÿ≤ÿ¥‚ÄåÿØ€åÿØŸá\n",
    "trainer.save_model(\"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/vit_finetuned_model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import ViTForImageClassification, ViTFeatureExtractor, Trainer, TrainingArguments\n",
    "# from datasets import load_dataset\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Load CIFAR-10 dataset from Hugging Face\n",
    "# dataset = load_dataset(\"cifar10\")\n",
    "\n",
    "# # Split the dataset into train and validation sets\n",
    "# train_dataset = dataset['train']\n",
    "# val_dataset = dataset['test']\n",
    "\n",
    "# # Load pre-trained ViT feature extractor\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# # Preprocessing function for ViT\n",
    "# def preprocess_function(examples):\n",
    "#     return feature_extractor([x['img'] for x in examples], return_tensors=\"pt\")\n",
    "\n",
    "# # Preprocess the dataset\n",
    "# train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "# val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# # Set format for PyTorch\n",
    "# train_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "# val_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "\n",
    "# # Load pre-trained ViT model for image classification\n",
    "# model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=10)\n",
    "\n",
    "# # Define the training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./vit_finetuned\",         # Where to store the final model\n",
    "#     evaluation_strategy=\"epoch\",          # Evaluate at the end of each epoch\n",
    "#     learning_rate=2e-5,                   # Learning rate for fine-tuning\n",
    "#     per_device_train_batch_size=16,       # Batch size for training\n",
    "#     per_device_eval_batch_size=64,        # Batch size for evaluation\n",
    "#     num_train_epochs=3,                   # Number of training epochs\n",
    "#     weight_decay=0.01,                    # Weight decay for regularization\n",
    "#     save_total_limit=2,                   # Save only the last 2 checkpoints\n",
    "#     logging_dir='./logs',                 # Directory for logs\n",
    "# )\n",
    "\n",
    "# # Define the metrics for evaluation\n",
    "# def compute_metrics(p):\n",
    "#     preds = np.argmax(p.predictions, axis=1)\n",
    "#     labels = p.label_ids\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     f1 = f1_score(labels, preds, average='weighted')\n",
    "#     precision = precision_score(labels, preds, average='weighted')\n",
    "#     recall = recall_score(labels, preds, average='weighted')\n",
    "\n",
    "#     return {\n",
    "#         \"accuracy\": acc,\n",
    "#         \"f1\": f1,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall\n",
    "#     }\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,                           # The model to be fine-tuned\n",
    "#     args=training_args,                    # Training arguments\n",
    "#     train_dataset=train_dataset,           # Training dataset\n",
    "#     eval_dataset=val_dataset,              # Validation dataset\n",
    "#     compute_metrics=compute_metrics        # Evaluation metrics\n",
    "# )\n",
    "\n",
    "# # Start fine-tuning\n",
    "# trainer.train()\n",
    "\n",
    "# # Evaluate the model after fine-tuning\n",
    "# results = trainer.evaluate()\n",
    "# print(\"Evaluation Results:\")\n",
    "# print(results)\n",
    "\n",
    "# # Get features and labels for t-SNE\n",
    "# model.eval()\n",
    "# train_features = []\n",
    "# train_labels = []\n",
    "\n",
    "# # Extract features from the model\n",
    "# with torch.no_grad():\n",
    "#     for batch in DataLoader(train_dataset, batch_size=32):\n",
    "#         inputs = batch['input_ids']\n",
    "#         labels = batch['label']\n",
    "#         outputs = model(**inputs)\n",
    "#         features = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "#         train_features.extend(features)\n",
    "#         train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# train_features = np.array(train_features)\n",
    "# train_labels = np.array(train_labels)\n",
    "\n",
    "# # Use t-SNE for dimensionality reduction\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# tsne_results = tsne.fit_transform(train_features)\n",
    "\n",
    "# # Plot the t-SNE result\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=train_labels, cmap='viridis', s=10)\n",
    "# plt.colorbar(scatter)\n",
    "# plt.title(\"t-SNE Visualization of ViT Features\")\n",
    "# plt.show()\n",
    "\n",
    "# # Save the fine-tuned model\n",
    "# trainer.save_model(\"./vit_finetuned_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc8384cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers[torch]\n",
    "# !pip install --upgrade torch\n",
    "#!pip show tokenizers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5320362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers==0.19.1\n",
      "  Using cached tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tokenizers==0.19.1) (0.24.6)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.67.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.3)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "Successfully installed tokenizers-0.19.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.46.3 requires tokenizers<0.21,>=0.20, but you have tokenizers 0.19.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tokenizers\n",
    "!pip install tokenizers==0.19.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f8af1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Using cached tokenizers-0.20.3-cp39-none-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed tokenizers-0.20.3\n",
      "Name: tokenizers\n",
      "Version: 0.20.3\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: Anthony MOI <m.anthony.moi@gmail.com>\n",
      "Author-email: Nicolas Patry <patry.nicolas@protonmail.com>, Anthony Moi <anthony@huggingface.co>\n",
      "License: \n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: huggingface-hub\n",
      "Required-by: transformers\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip show tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee109ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1884749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60c6cf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (4.28.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (1.24.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (0.24.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.28.0) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\tokenizers-0.13.3.dist-info\\\\METADATA'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94056a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbd317fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers==0.19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15cd70ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizers\n",
      "Version: 0.19.1\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: Anthony MOI <m.anthony.moi@gmail.com>\n",
      "Author-email: Nicolas Patry <patry.nicolas@protonmail.com>, Anthony Moi <anthony@huggingface.co>\n",
      "License: \n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: huggingface-hub\n",
      "Required-by: transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c2a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (4.28.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (0.24.6)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2022.7.9)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.28.0) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers==4.28.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (3.3)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed tokenizers-0.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a0c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers==0.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae85211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizers\n",
      "Version: 0.13.3\n",
      "Summary: Fast and Customizable Tokenizers\n",
      "Home-page: https://github.com/huggingface/tokenizers\n",
      "Author: Anthony MOI\n",
      "Author-email: anthony@huggingface.co\n",
      "License: Apache License 2.0\n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae06217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate>={ACCELERATE_MIN_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9a257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
