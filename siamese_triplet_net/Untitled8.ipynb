{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee102659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from transformers import ViTModel\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TripletViT(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TripletViT, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.embedding = nn.Linear(768, 512)\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        x = self.vit(x).last_hidden_state[:, 0, :]\n",
    "        embedding = self.embedding(x)\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, anchor, positive, negative=None):\n",
    "        anchor_emb = self.forward_one(anchor)\n",
    "        positive_emb = self.forward_one(positive) if positive is not None else None\n",
    "        \n",
    "        if negative is not None:\n",
    "            negative_emb = self.forward_one(negative)\n",
    "            return anchor_emb, positive_emb, negative_emb\n",
    "        \n",
    "        return self.classifier(anchor_emb)\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "\n",
    "def train_epoch(model, train_loader, criterion_triplet, criterion_ce, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        anchor, positive, negative, labels = batch\n",
    "        anchor, positive = anchor.to(device), positive.to(device)\n",
    "        negative, labels = negative.to(device), labels.to(device)\n",
    "        \n",
    "        # Compute embeddings and losses\n",
    "        anchor_emb, positive_emb, negative_emb = model(anchor, positive, negative)\n",
    "        loss_triplet = criterion_triplet(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        # Classification\n",
    "        pred = model(anchor, None, None)\n",
    "        loss_ce = criterion_ce(pred, labels)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = loss_triplet + loss_ce\n",
    "        \n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        pred_labels = torch.argmax(pred, dim=1)\n",
    "        total_acc += accuracy_score(labels.cpu(), pred_labels.cpu())\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_acc / num_batches\n",
    "\n",
    "def validate(model, val_loader, criterion_triplet, criterion_ce, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    num_batches = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            anchor, positive, negative, labels = batch\n",
    "            anchor, positive = anchor.to(device), positive.to(device)\n",
    "            negative, labels = negative.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            anchor_emb, positive_emb, negative_emb = model(anchor, positive, negative)\n",
    "            loss_triplet = criterion_triplet(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            pred = model(anchor, None, None)\n",
    "            loss_ce = criterion_ce(pred, labels)\n",
    "            loss = loss_triplet + loss_ce\n",
    "            \n",
    "            # Metrics\n",
    "            pred_labels = torch.argmax(pred, dim=1)\n",
    "            all_preds.extend(pred_labels.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += accuracy_score(labels.cpu(), pred_labels.cpu())\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / num_batches,\n",
    "        'accuracy': total_acc / num_batches,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, None, None)\n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(pred_labels.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Training setup\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    num_classes = 10\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    # Model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TripletViT(num_classes).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion_triplet = TripletLoss()\n",
    "    criterion_ce = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Load data\n",
    "    path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "    train_data = torchvision.datasets.ImageFolder(root=path_data + '/train/', transform=get_val_transforms())\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_data = torchvision.datasets.ImageFolder(root=path_data + '/val/', transform=get_val_transforms())\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "    \n",
    "    test_data = torchvision.datasets.ImageFolder(root=path_data + '/test/', transform=get_val_transforms())\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion_triplet, criterion_ce, optimizer, device\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = validate(model, val_loader, criterion_triplet, criterion_ce, device)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Val F1: {val_metrics['f1']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    # Test best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    test_metrics = test(model, test_loader, device)\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98204e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from transformers import ViTModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FewShotTripletViT(nn.Module):\n",
    "    def __init__(self, num_classes, n_way=15, k_shot=15):\n",
    "        super(FewShotTripletViT, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.embedding = nn.Linear(768, 512)\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        x = self.vit(x).last_hidden_state[:, 0, :]\n",
    "        embedding = self.embedding(x)\n",
    "        return F.normalize(embedding, p=2, dim=1)  # L2 normalization\n",
    "    \n",
    "    def forward(self, support_set, query):\n",
    "        n_way, k_shot = support_set.shape[:2]\n",
    "        \n",
    "        # Support set embeddings\n",
    "        support_embeddings = []\n",
    "        for i in range(n_way):\n",
    "            class_embeddings = []\n",
    "            for j in range(k_shot):\n",
    "                emb = self.forward_one(support_set[i, j])\n",
    "                class_embeddings.append(emb)\n",
    "            support_embeddings.append(torch.stack(class_embeddings).mean(0))\n",
    "        support_embeddings = torch.stack(support_embeddings)\n",
    "        \n",
    "        # Query embedding\n",
    "        query_embedding = self.forward_one(query)\n",
    "        \n",
    "        # Similarity scores\n",
    "        similarities = torch.mm(query_embedding, support_embeddings.t())\n",
    "        return similarities\n",
    "\n",
    "class FewShotDataset(Dataset):\n",
    "    def __init__(self, dataset, n_way=15, k_shot=15, n_query=15, n_episodes=1000):\n",
    "        self.dataset = dataset\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "        \n",
    "        # Group by label\n",
    "        self.label_to_indices = {}\n",
    "        for idx, (_, label) in enumerate(dataset):\n",
    "            if label not in self.label_to_indices:\n",
    "                self.label_to_indices[label] = []\n",
    "            self.label_to_indices[label].append(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        selected_classes = np.random.choice(\n",
    "            list(self.label_to_indices.keys()),\n",
    "            self.n_way,\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        support_images = []\n",
    "        query_images = []\n",
    "        query_labels = []\n",
    "        \n",
    "        for i, cls in enumerate(selected_classes):\n",
    "            indices = np.random.choice(\n",
    "                self.label_to_indices[cls],\n",
    "                self.k_shot + self.n_query,\n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            support_idx = indices[:self.k_shot]\n",
    "            query_idx = indices[self.k_shot:self.k_shot + self.n_query]\n",
    "            \n",
    "            for idx in support_idx:\n",
    "                img, _ = self.dataset[idx]\n",
    "                support_images.append((i, img))\n",
    "            \n",
    "            for idx in query_idx:\n",
    "                img, _ = self.dataset[idx]\n",
    "                query_images.append(img)\n",
    "                query_labels.append(i)\n",
    "        \n",
    "        support_images = torch.stack([img for _, img in sorted(support_images)])\n",
    "        support_images = support_images.view(self.n_way, self.k_shot, *support_images.shape[1:])\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "        \n",
    "        return support_images, query_images, query_labels\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    episodes = 0\n",
    "    \n",
    "    for support_images, query_images, query_labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        scores = model(support_images, query_images)\n",
    "        loss = F.cross_entropy(scores, query_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        pred = scores.argmax(dim=1)\n",
    "        acc = (pred == query_labels).float().mean()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        episodes += 1\n",
    "    \n",
    "    return total_loss / episodes, total_acc / episodes\n",
    "\n",
    "def evaluate(model, data_loader, device, mode=\"val\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    episodes = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for support_images, query_images, query_labels in tqdm(data_loader, desc=mode.capitalize()):\n",
    "            support_images = support_images.to(device)\n",
    "            query_images = query_images.to(device)\n",
    "            query_labels = query_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            scores = model(support_images, query_images)\n",
    "            loss = F.cross_entropy(scores, query_labels)\n",
    "            \n",
    "            # Predictions\n",
    "            pred = scores.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(query_labels.cpu().numpy())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            episodes += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / episodes,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    n_way = 15\n",
    "    k_shot = 15\n",
    "    n_query = 15\n",
    "    n_episodes = 1000\n",
    "    batch_size = 4\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    # Transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Datasets\n",
    "    path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "    train_dataset = FewShotDataset(\n",
    "        datasets.ImageFolder(path_data+'/train/', transform=transform),\n",
    "        n_way=n_way, k_shot=k_shot, n_query=n_query, n_episodes=n_episodes\n",
    "    )\n",
    "    val_dataset = FewShotDataset(\n",
    "        datasets.ImageFolder(path_data+'/val/', transform=transform),\n",
    "        n_way=n_way, k_shot=k_shot, n_query=n_query, n_episodes=200\n",
    "    )\n",
    "    test_dataset = FewShotDataset(\n",
    "        datasets.ImageFolder(path_data+'/test/', transform=transform),\n",
    "        n_way=n_way, k_shot=k_shot, n_query=n_query, n_episodes=200\n",
    "    )\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = FewShotTripletViT(num_classes=n_way, n_way=n_way, k_shot=k_shot).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = evaluate(model, val_loader, device, mode=\"val\")\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Val F1: {val_metrics['f1']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': best_val_f1,\n",
    "            }, 'best_model.pth')\n",
    "    \n",
    "    # Test best model\n",
    "    checkpoint = torch.load('best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_metrics = evaluate(model, test_loader, device, mode=\"test\")\n",
    "    \n",
    "    print(\"\\nFinal Test Results:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17892932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "class InnovativeFewShotViT(nn.Module):\n",
    "    def __init__(self, n_way=15, k_shot=15):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.embedding = nn.Linear(768, 512)\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.margin = 1.0\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        x = self.vit(x).last_hidden_state[:, 0, :]\n",
    "        embedding = self.embedding(x)\n",
    "        return F.normalize(embedding, p=2, dim=1)\n",
    "    \n",
    "    def forward(self, support_set, query, mode='train'):\n",
    "        n_way, k_shot = support_set.shape[:2]\n",
    "        \n",
    "        if mode == 'train':\n",
    "            # Triplet learning within support set\n",
    "            support_embeddings = []\n",
    "            triplet_loss = 0\n",
    "            \n",
    "            for i in range(n_way):\n",
    "                class_embeddings = []\n",
    "                for j in range(k_shot):\n",
    "                    emb = self.forward_one(support_set[i, j])\n",
    "                    class_embeddings.append(emb)\n",
    "                    \n",
    "                    # Create triplets within same class\n",
    "                    if j > 0:\n",
    "                        anchor = emb\n",
    "                        positive = class_embeddings[j-1]\n",
    "                        # Get negative from different class\n",
    "                        neg_class = (i + 1) % n_way\n",
    "                        negative = self.forward_one(support_set[neg_class, j])\n",
    "                        \n",
    "                        d_pos = F.pairwise_distance(anchor, positive)\n",
    "                        d_neg = F.pairwise_distance(anchor, negative)\n",
    "                        triplet_loss += F.relu(d_pos - d_neg + self.margin).mean()\n",
    "                \n",
    "                support_embeddings.append(torch.stack(class_embeddings).mean(0))\n",
    "            \n",
    "            support_embeddings = torch.stack(support_embeddings)\n",
    "            query_embedding = self.forward_one(query)\n",
    "            \n",
    "            # Prototypical network style classification\n",
    "            logits = -torch.cdist(query_embedding.unsqueeze(1), \n",
    "                                support_embeddings.unsqueeze(0)).squeeze(1)\n",
    "            \n",
    "            return logits, triplet_loss\n",
    "            \n",
    "        else: # Inference mode\n",
    "            support_embeddings = []\n",
    "            for i in range(n_way):\n",
    "                class_embs = []\n",
    "                for j in range(k_shot):\n",
    "                    emb = self.forward_one(support_set[i, j])\n",
    "                    class_embs.append(emb)\n",
    "                support_embeddings.append(torch.stack(class_embs).mean(0))\n",
    "            \n",
    "            support_embeddings = torch.stack(support_embeddings)\n",
    "            query_embedding = self.forward_one(query)\n",
    "            logits = -torch.cdist(query_embedding.unsqueeze(1),\n",
    "                                support_embeddings.unsqueeze(0)).squeeze(1)\n",
    "            return logits\n",
    "\n",
    "class FewShotDataset:\n",
    "    def __init__(self, dataset, n_way=15, k_shot=15, n_query=5, n_episodes=1000):\n",
    "        self.dataset = dataset\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "        \n",
    "        self.label_to_indices = {}\n",
    "        for idx, (_, label) in enumerate(dataset):\n",
    "            if label not in self.label_to_indices:\n",
    "                self.label_to_indices[label] = []\n",
    "            self.label_to_indices[label].append(idx)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        selected_classes = np.random.choice(\n",
    "            list(self.label_to_indices.keys()),\n",
    "            self.n_way,\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        support_images = []\n",
    "        query_images = []\n",
    "        query_labels = []\n",
    "        \n",
    "        for i, cls in enumerate(selected_classes):\n",
    "            indices = np.random.choice(\n",
    "                self.label_to_indices[cls],\n",
    "                self.k_shot + self.n_query,\n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            support_idx = indices[:self.k_shot]\n",
    "            query_idx = indices[self.k_shot:]\n",
    "            \n",
    "            for idx in support_idx:\n",
    "                img, _ = self.dataset[idx]\n",
    "                support_images.append((i, img))\n",
    "            \n",
    "            for idx in query_idx:\n",
    "                img, _ = self.dataset[idx]\n",
    "                query_images.append(img)\n",
    "                query_labels.append(i)\n",
    "                \n",
    "        support_images = torch.stack([img for _, img in sorted(support_images)])\n",
    "        support_images = support_images.view(self.n_way, self.k_shot, *support_images.shape[1:])\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "        \n",
    "        return support_images, query_images, query_labels\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    episodes = 0\n",
    "    \n",
    "    for support_images, query_images, query_labels in tqdm(train_loader):\n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "        \n",
    "        logits, triplet_loss = model(support_images, query_images, mode='train')\n",
    "        ce_loss = F.cross_entropy(logits, query_labels)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = ce_loss + 0.5 * triplet_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = logits.argmax(dim=1)\n",
    "        acc = (pred == query_labels).float().mean()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        episodes += 1\n",
    "    \n",
    "    return total_loss / episodes, total_acc / episodes\n",
    "\n",
    "def evaluate(model, data_loader, device, mode=\"val\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for support_images, query_images, query_labels in tqdm(data_loader):\n",
    "            support_images = support_images.to(device)\n",
    "            query_images = query_images.to(device)\n",
    "            query_labels = query_labels.to(device)\n",
    "            \n",
    "            logits = model(support_images, query_images, mode='test')\n",
    "            pred = logits.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(query_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    n_way = 15\n",
    "    k_shot = 15\n",
    "    n_query = 5\n",
    "    batch_size = 2\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset = FewShotDataset(\n",
    "        YourDataset('path/to/train', transform=transform),\n",
    "        n_way=n_way, k_shot=k_shot, n_query=n_query\n",
    "    )\n",
    "    val_dataset = FewShotDataset(\n",
    "        YourDataset('path/to/val', transform=transform),\n",
    "        n_way=n_way, k_shot=k_shot, n_query=n_query\n",
    "    )\n",
    "    test_dataset = FewShotDataset(\n",
    "        YourDataset('path/to/test', transform=transform),\n",
    "        n_way=n_way, k_shot=k_shot, n_query=n_query\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = InnovativeFewShotViT(n_way=n_way, k_shot=k_shot).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "        val_metrics = evaluate(model, val_loader, device, mode=\"val\")\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Acc: {val_metrics['accuracy']:.4f}, Val F1: {val_metrics['f1']:.4f}\")\n",
    "        \n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': best_val_f1,\n",
    "            }, 'best_model.pth')\n",
    "    \n",
    "    checkpoint = torch.load('best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_metrics = evaluate(model, test_loader, device, mode=\"test\")\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3badf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTModel\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)  # (N, L, E) -> (L, N, E)\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        return attn_output.transpose(0, 1)  # (L, N, E) -> (N, L, E)\n",
    "\n",
    "class InnovativeFewShotViT(nn.Module):\n",
    "    def __init__(self, n_way=15, k_shot=15, embed_dim=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.embedding = nn.Linear(768, embed_dim)\n",
    "        \n",
    "        # Multi-head attention for support set processing\n",
    "        self.support_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.query_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        # Few-shot parameters\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.margin = 1.0\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        # Extract features using ViT\n",
    "        x = self.vit(x).last_hidden_state[:, 0, :]  # Get [CLS] token\n",
    "        embedding = self.embedding(x)\n",
    "        return F.normalize(embedding, p=2, dim=1)\n",
    "    \n",
    "    def get_prototypes(self, support_embeddings):\n",
    "        # Process support set with attention\n",
    "        support_embeddings = support_embeddings.view(self.n_way * self.k_shot, -1)\n",
    "        attended_support = self.support_attention(support_embeddings)\n",
    "        \n",
    "        # Calculate prototypes\n",
    "        prototypes = attended_support.view(self.n_way, self.k_shot, -1).mean(1)\n",
    "        return prototypes\n",
    "    \n",
    "    def forward(self, support_set, query, mode='train'):\n",
    "        n_way, k_shot = support_set.shape[:2]\n",
    "        \n",
    "        if mode == 'train':\n",
    "            # Process support set\n",
    "            support_embeddings = []\n",
    "            triplet_loss = 0\n",
    "            \n",
    "            for i in range(n_way):\n",
    "                class_embeddings = []\n",
    "                for j in range(k_shot):\n",
    "                    # Get embedding for each support image\n",
    "                    emb = self.forward_one(support_set[i, j])\n",
    "                    class_embeddings.append(emb)\n",
    "                    \n",
    "                    # Triplet loss computation\n",
    "                    if j > 0:\n",
    "                        anchor = emb\n",
    "                        positive = class_embeddings[j-1]\n",
    "                        \n",
    "                        # Get negative from different class\n",
    "                        neg_class = (i + 1) % n_way\n",
    "                        negative = self.forward_one(support_set[neg_class, j])\n",
    "                        \n",
    "                        # Calculate distances for triplet loss\n",
    "                        d_pos = F.pairwise_distance(anchor, positive)\n",
    "                        d_neg = F.pairwise_distance(anchor, negative)\n",
    "                        triplet_loss += F.relu(d_pos - d_neg + self.margin).mean()\n",
    "                \n",
    "                support_embeddings.append(torch.stack(class_embeddings))\n",
    "            \n",
    "            support_embeddings = torch.stack(support_embeddings)\n",
    "            \n",
    "            # Get prototypes using attention\n",
    "            prototypes = self.get_prototypes(support_embeddings)\n",
    "            \n",
    "            # Process query with attention\n",
    "            query_embedding = self.forward_one(query)\n",
    "            attended_query = self.query_attention(query_embedding)\n",
    "            \n",
    "            # Calculate similarity scores\n",
    "            logits = -torch.cdist(attended_query.unsqueeze(1), \n",
    "                                prototypes.unsqueeze(0)).squeeze(1)\n",
    "            \n",
    "            return logits, triplet_loss\n",
    "            \n",
    "        else:  # Inference mode\n",
    "            support_embeddings = []\n",
    "            for i in range(n_way):\n",
    "                class_embs = []\n",
    "                for j in range(k_shot):\n",
    "                    emb = self.forward_one(support_set[i, j])\n",
    "                    class_embs.append(emb)\n",
    "                support_embeddings.append(torch.stack(class_embs))\n",
    "            \n",
    "            support_embeddings = torch.stack(support_embeddings)\n",
    "            prototypes = self.get_prototypes(support_embeddings)\n",
    "            \n",
    "            query_embedding = self.forward_one(query)\n",
    "            attended_query = self.query_attention(query_embedding)\n",
    "            \n",
    "            logits = -torch.cdist(attended_query.unsqueeze(1),\n",
    "                                prototypes.unsqueeze(0)).squeeze(1)\n",
    "            return logits\n",
    "\n",
    "# Loss function\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, triplet_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.triplet_weight = triplet_weight\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels, triplet_loss):\n",
    "        ce_loss = self.ce(logits, labels)\n",
    "        return ce_loss + self.triplet_weight * triplet_loss\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    episodes = 0\n",
    "    \n",
    "    for support_images, query_images, query_labels in train_loader:\n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, triplet_loss = model(support_images, query_images, mode='train')\n",
    "        \n",
    "        # Calculate combined loss\n",
    "        loss = criterion(logits, query_labels, triplet_loss)\n",
    "        \n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        pred = logits.argmax(dim=1)\n",
    "        acc = (pred == query_labels).float().mean()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        episodes += 1\n",
    "    \n",
    "    return total_loss / episodes, total_acc / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407339d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTModel\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "from dataloaders import get_train_transforms, get_val_transforms, get_triplet_dataloader\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)  # (N, L, E) -> (L, N, E)\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        return attn_output.transpose(0, 1)  # (L, N, E) -> (N, L, E)\n",
    "\n",
    "class InnovativeFewShotViT(nn.Module):\n",
    "    def __init__(self, n_way=15, k_shot=15, embed_dim=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.embedding = nn.Linear(768, embed_dim)\n",
    "        \n",
    "        # Multi-head attention for support set processing\n",
    "        self.support_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.query_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        # Few-shot parameters\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.margin = 1.0\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        # Extract features using ViT\n",
    "        x = self.vit(x).last_hidden_state[:, 0, :]  # Get [CLS] token\n",
    "        embedding = self.embedding(x)\n",
    "        return F.normalize(embedding, p=2, dim=1)\n",
    "    \n",
    "    def get_prototypes(self, support_embeddings):\n",
    "        # Process support set with attention\n",
    "        support_embeddings = support_embeddings.view(self.n_way * self.k_shot, -1)\n",
    "        attended_support = self.support_attention(support_embeddings)\n",
    "        \n",
    "        # Calculate prototypes\n",
    "        prototypes = attended_support.view(self.n_way, self.k_shot, -1).mean(1)\n",
    "        return prototypes\n",
    "    \n",
    "    def forward(self, support_set, query, mode='train'):\n",
    "        n_way, k_shot = support_set.shape[:2]\n",
    "        \n",
    "        if mode == 'train':\n",
    "            # Process support set\n",
    "            support_embeddings = []\n",
    "            triplet_loss = 0\n",
    "            \n",
    "            for i in range(n_way):\n",
    "                class_embeddings = []\n",
    "                for j in range(k_shot):\n",
    "                    # Get embedding for each support image\n",
    "                    emb = self.forward_one(support_set[i, j])\n",
    "                    class_embeddings.append(emb)\n",
    "                    \n",
    "                    # Triplet loss computation\n",
    "                    if j > 0:\n",
    "                        anchor = emb\n",
    "                        positive = class_embeddings[j-1]\n",
    "                        \n",
    "                        # Get negative from different class\n",
    "                        neg_class = (i + 1) % n_way\n",
    "                        negative = self.forward_one(support_set[neg_class, j])\n",
    "                        \n",
    "                        # Calculate distances for triplet loss\n",
    "                        d_pos = F.pairwise_distance(anchor, positive)\n",
    "                        d_neg = F.pairwise_distance(anchor, negative)\n",
    "                        triplet_loss += F.relu(d_pos - d_neg + self.margin).mean()\n",
    "                \n",
    "                support_embeddings.append(torch.stack(class_embeddings))\n",
    "            \n",
    "            support_embeddings = torch.stack(support_embeddings)\n",
    "            \n",
    "            # Get prototypes using attention\n",
    "            prototypes = self.get_prototypes(support_embeddings)\n",
    "            \n",
    "            # Process query with attention\n",
    "            query_embedding = self.forward_one(query)\n",
    "            attended_query = self.query_attention(query_embedding)\n",
    "            \n",
    "            # Calculate similarity scores\n",
    "            logits = -torch.cdist(attended_query.unsqueeze(1), \n",
    "                                prototypes.unsqueeze(0)).squeeze(1)\n",
    "            \n",
    "            return logits, triplet_loss\n",
    "            \n",
    "        else:  # Inference mode\n",
    "            support_embeddings = []\n",
    "            for i in range(n_way):\n",
    "                class_embs = []\n",
    "                for j in range(k_shot):\n",
    "                    emb = self.forward_one(support_set[i, j])\n",
    "                    class_embs.append(emb)\n",
    "                support_embeddings.append(torch.stack(class_embs))\n",
    "            \n",
    "            support_embeddings = torch.stack(support_embeddings)\n",
    "            prototypes = self.get_prototypes(support_embeddings)\n",
    "            \n",
    "            query_embedding = self.forward_one(query)\n",
    "            attended_query = self.query_attention(query_embedding)\n",
    "            \n",
    "            logits = -torch.cdist(attended_query.unsqueeze(1),\n",
    "                                prototypes.unsqueeze(0)).squeeze(1)\n",
    "            return logits\n",
    "\n",
    "# Loss function\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, triplet_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.triplet_weight = triplet_weight\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels, triplet_loss):\n",
    "        ce_loss = self.ce(logits, labels)\n",
    "        return ce_loss + self.triplet_weight * triplet_loss\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    episodes = 0\n",
    "    \n",
    "    for support_images, query_images, query_labels in train_loader:\n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, triplet_loss = model(support_images, query_images, mode='train')\n",
    "        \n",
    "        # Calculate combined loss\n",
    "        loss = criterion(logits, query_labels, triplet_loss)\n",
    "        \n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        pred = logits.argmax(dim=1)\n",
    "        acc = (pred == query_labels).float().mean()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        episodes += 1\n",
    "    \n",
    "    return total_loss / episodes, total_acc / episodes\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, \n",
    "                 criterion, optimizer, device, num_epochs=50):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        self.best_val_f1 = 0\n",
    "        self.train_metrics = []\n",
    "        self.val_metrics = []\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for support_images, query_images, query_labels in tqdm(self.train_loader):\n",
    "            support_images = support_images.to(self.device)\n",
    "            query_images = query_images.to(self.device)\n",
    "            query_labels = query_labels.to(self.device)\n",
    "            \n",
    "            logits, triplet_loss = self.model(support_images, query_images, mode='train')\n",
    "            loss = self.criterion(logits, query_labels, triplet_loss)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(query_labels.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.calculate_metrics(all_labels, all_preds, total_loss, len(self.train_loader))\n",
    "        self.train_metrics.append(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for support_images, query_images, query_labels in tqdm(self.val_loader):\n",
    "                support_images = support_images.to(self.device)\n",
    "                query_images = query_images.to(self.device)\n",
    "                query_labels = query_labels.to(self.device)\n",
    "                \n",
    "                logits = self.model(support_images, query_images, mode='test')\n",
    "                loss = self.criterion(logits, query_labels, torch.tensor(0.))\n",
    "                \n",
    "                preds = logits.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(query_labels.cpu().numpy())\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        metrics = self.calculate_metrics(all_labels, all_preds, total_loss, len(self.val_loader))\n",
    "        self.val_metrics.append(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for support_images, query_images, query_labels in tqdm(self.test_loader):\n",
    "                support_images = support_images.to(self.device)\n",
    "                query_images = query_images.to(self.device)\n",
    "                query_labels = query_labels.to(self.device)\n",
    "                \n",
    "                logits = self.model(support_images, query_images, mode='test')\n",
    "                preds = logits.argmax(dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(query_labels.cpu().numpy())\n",
    "        \n",
    "        return self.calculate_metrics(all_labels, all_preds)\n",
    "    \n",
    "    def calculate_metrics(self, labels, preds, loss=None, n_batches=None):\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average='weighted', zero_division=0\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "        }\n",
    "        \n",
    "        if loss is not None:\n",
    "            metrics['loss'] = loss / n_batches\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def plot_confusion_matrix(self, labels, preds, title='Confusion Matrix'):\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(title)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.train_metrics) + 1)\n",
    "        metrics = ['accuracy', 'f1', 'loss']\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            train_values = [m[metric] for m in self.train_metrics]\n",
    "            val_values = [m[metric] for m in self.val_metrics]\n",
    "            \n",
    "            plt.plot(epochs, train_values, 'b-', label='Train')\n",
    "            plt.plot(epochs, val_values, 'r-', label='Validation')\n",
    "            plt.title(f'{metric.capitalize()} vs Epochs')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = self.validate()\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1']:.4f}\")\n",
    "            print(f\"Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_metrics['f1'] > self.best_val_f1:\n",
    "                self.best_val_f1 = val_metrics['f1']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_f1': self.best_val_f1,\n",
    "                }, 'best_model.pth')\n",
    "        \n",
    "        # Plot training progress\n",
    "        self.plot_metrics()\n",
    "        \n",
    "        # Load best model and test\n",
    "        checkpoint = torch.load('best_model.pth')\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        test_metrics = self.test()\n",
    "        \n",
    "        print(\"\\nTest Results:\")\n",
    "        print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "        \n",
    "        return test_metrics\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    n_way = 15\n",
    "    k_shot = 15\n",
    "    n_query = 5\n",
    "    batch_size = 2\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    # Setup model, datasets, etc.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = InnovativeFewShotViT(n_way=n_way, k_shot=k_shot).to(device)\n",
    "    criterion = CombinedLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Setup data loaders\n",
    "    path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "    train_data = torchvision.datasets.ImageFolder(root=path_data + '/train/', transform=get_val_transforms())\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_data = torchvision.datasets.ImageFolder(root=path_data + '/val/', transform=get_val_transforms())\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "    \n",
    "    test_data = torchvision.datasets.ImageFolder(root=path_data + '/test/', transform=get_val_transforms())\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "   \n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate\n",
    "    test_metrics = trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from transformers import ViTModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    " \n",
    "import torchvision\n",
    "from dataloaders import get_train_transforms, get_val_transforms, get_triplet_dataloader\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        return attn_output.transpose(0, 1)\n",
    "\n",
    "class InnovativeFewShotViT(nn.Module):\n",
    "    def __init__(self, n_way=15, k_shot=15, embed_dim=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.embedding = nn.Linear(768, embed_dim)\n",
    "        self.support_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.query_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.margin = 1.0\n",
    "\n",
    "     \n",
    "    def forward_one(self, x):\n",
    "        # تنظیم ابعاد برای ViT\n",
    "        if len(x.shape) > 4:  # [B, N, C, H, W] یا بیشتر\n",
    "            x = x.view(-1, x.size(-3), x.size(-2), x.size(-1))  # [B*N, C, H, W]\n",
    "        elif len(x.shape) == 3:  # [C, H, W]\n",
    "            x = x.unsqueeze(0)  # [1, C, H, W]\n",
    "\n",
    "        # تبدیل به شکل استاندارد\n",
    "        if x.size(1) != 3:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # اطمینان از ابعاد درست\n",
    "        assert len(x.shape) == 4 and x.size(1) == 3, f\"نادرست shape ورودی: {x.shape}\"\n",
    "\n",
    "        x = self.vit(x).last_hidden_state[:, 0]\n",
    "        embedding = self.embedding(x)\n",
    "        return F.normalize(embedding, p=2, dim=1)    \n",
    "    \n",
    "    \n",
    "#     def forward(self, support_set, query, mode='train'):\n",
    "#         batch_size = support_set.size(0)\n",
    "#         support_embeddings = []\n",
    "#         triplet_loss = 0\n",
    "\n",
    "#         # پردازش support set\n",
    "#         for i in range(self.n_way):\n",
    "#             class_embeddings = []\n",
    "#             for j in range(self.k_shot):\n",
    "#                 # ورودی باید شکل [B, C, H, W] داشته باشد\n",
    "#                 emb = self.forward_one(support_set[:, i, j])\n",
    "#                 class_embeddings.append(emb)\n",
    "\n",
    "#             support_embeddings.append(torch.stack(class_embeddings, dim=1))\n",
    "\n",
    "#         support_embeddings = torch.stack(support_embeddings, dim=1)\n",
    "#         prototypes = self.get_prototypes(support_embeddings)\n",
    "\n",
    "#         # پردازش query\n",
    "#         query_emb = []\n",
    "#         for i in range(query.size(0)):\n",
    "#             q = query[i].unsqueeze(0)  # [1, C, H, W]\n",
    "#             q_emb = self.forward_one(q)\n",
    "#             query_emb.append(q_emb)\n",
    "#         query_embedding = torch.cat(query_emb, dim=0)\n",
    "#         # محاسبه logits\n",
    "#         logits = -torch.cdist(query_embedding.unsqueeze(1), prototypes).squeeze(1)\n",
    "\n",
    "#         return logits, triplet_loss if mode == 'train' else logits    \n",
    "#     def forward(self, support_set, query, mode='train'):\n",
    "#         batch_size = support_set.size(0)\n",
    "\n",
    "#         # پردازش support set برای هر تصویر\n",
    "#         support_embeddings = []\n",
    "#         for i in range(self.n_way):\n",
    "#             class_embeddings = []\n",
    "#             for j in range(self.k_shot):\n",
    "#                 emb = self.forward_one(support_set[:, i, j])\n",
    "#                 class_embeddings.append(emb)\n",
    "#             support_embeddings.append(torch.stack(class_embeddings))\n",
    "\n",
    "#         support_embeddings = torch.stack(support_embeddings)\n",
    "#         prototypes = self.get_prototypes(support_embeddings)\n",
    "\n",
    "#         # پردازش query set\n",
    "#         query_emb = self.forward_one(query)  # حذف حلقه\n",
    "#         logits = -torch.cdist(query_emb, prototypes.squeeze(0)).squeeze(1)\n",
    "\n",
    "#         return logits, torch.tensor(0.0, device=logits.device)  # triplet_loss موقتاً غیرفعال \n",
    "\n",
    "#     def get_prototypes(self, support_embeddings):\n",
    "#         batch_size = support_embeddings.size(0)\n",
    "#         # تغییر shape برای MultiheadAttention\n",
    "#         support_embeddings = support_embeddings.view(batch_size, self.n_way * self.k_shot, -1)\n",
    "#         attended_support = self.support_attention(support_embeddings)\n",
    "#         prototypes = attended_support.view(batch_size, self.n_way, self.k_shot, -1).mean(2)\n",
    "#         return prototypes\n",
    "\n",
    "    \n",
    "\n",
    "    def get_prototypes(self, support_embeddings):\n",
    "        batch_size = support_embeddings.size(0)\n",
    "        support_embeddings = support_embeddings.reshape(batch_size, self.n_way * self.k_shot, -1)\n",
    "        attended_support = self.support_attention(support_embeddings)\n",
    "        prototypes = attended_support.reshape(batch_size, self.n_way, self.k_shot, -1).mean(2)\n",
    "        return prototypes\n",
    "\n",
    "    def forward(self, support_set, query, mode='train'):\n",
    "        batch_size = support_set.size(0)\n",
    "\n",
    "        # پردازش support set\n",
    "        support_embeddings = torch.zeros(batch_size, self.n_way, self.k_shot, 512, device=support_set.device)\n",
    "        for i in range(self.n_way):\n",
    "            for j in range(self.k_shot):\n",
    "                emb = self.forward_one(support_set[:, i, j])\n",
    "                support_embeddings[:, i, j] = emb\n",
    "\n",
    "        prototypes = self.get_prototypes(support_embeddings)\n",
    "\n",
    "        # پردازش query\n",
    "        query_emb = self.forward_one(query.view(-1, 3, 224, 224))\n",
    "        logits = -torch.cdist(query_emb, prototypes.view(batch_size * self.n_way, -1))\n",
    "        logits = logits.view(batch_size, -1, self.n_way)\n",
    "\n",
    "        return logits.squeeze(1), torch.tensor(0.0, device=logits.device) \n",
    "        \n",
    "        \n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, triplet_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.triplet_weight = triplet_weight\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels, triplet_loss):\n",
    "        ce_loss = self.ce(logits, labels)\n",
    "        return ce_loss + self.triplet_weight * triplet_loss\n",
    "\n",
    "class FewShotDataset:\n",
    "    def __init__(self, root_dir, transform=None, n_way=15, k_shot=15, n_query=5, n_episodes=1000):\n",
    "        self.dataset = datasets.ImageFolder(root_dir, transform=transform)\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "        \n",
    "        self.label_to_indices = {}\n",
    "        for idx, (_, label) in enumerate(self.dataset):\n",
    "            if label not in self.label_to_indices:\n",
    "                self.label_to_indices[label] = []\n",
    "            self.label_to_indices[label].append(idx)\n",
    "        \n",
    "        self.valid_classes = [\n",
    "            cls for cls, indices in self.label_to_indices.items()\n",
    "            if len(indices) >= self.k_shot + self.n_query\n",
    "        ]\n",
    "        \n",
    "        if len(self.valid_classes) < self.n_way:\n",
    "            raise ValueError(f\"Not enough classes with sufficient samples. Found {len(self.valid_classes)} valid classes, need {self.n_way}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "    \n",
    "    def __getitem__(self, episode_index):\n",
    "        selected_classes = np.random.choice(self.valid_classes, self.n_way, replace=False)\n",
    "\n",
    "        support_images = torch.zeros(self.n_way, self.k_shot, 3, 224, 224)\n",
    "        query_images = []\n",
    "        query_labels = []\n",
    "\n",
    "        for class_idx, class_label in enumerate(selected_classes):\n",
    "            class_indices = self.label_to_indices[class_label]\n",
    "            selected_indices = np.random.choice(\n",
    "                class_indices, \n",
    "                self.k_shot + self.n_query, \n",
    "                replace=False\n",
    "            )\n",
    "\n",
    "            # Support set\n",
    "            for shot_idx, img_idx in enumerate(selected_indices[:self.k_shot]):\n",
    "                img, _ = self.dataset[img_idx]\n",
    "                support_images[class_idx, shot_idx] = img\n",
    "\n",
    "            # Query set\n",
    "            for img_idx in selected_indices[self.k_shot:self.k_shot + self.n_query]:\n",
    "                img, _ = self.dataset[img_idx]\n",
    "                query_images.append(img)\n",
    "                query_labels.append(class_idx)\n",
    "\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "\n",
    "        return support_images, query_images, query_labels\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, criterion, optimizer, device, num_epochs=50):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        self.best_val_f1 = 0\n",
    "        self.train_metrics = []\n",
    "        self.val_metrics = []\n",
    "    \n",
    "    def calculate_metrics(self, labels, preds, loss=None, n_batches=None):\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average='weighted', zero_division=0\n",
    "        )\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "        }\n",
    "        if loss is not None:\n",
    "            metrics['loss'] = loss / n_batches\n",
    "        return metrics\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for support_images, query_images, query_labels in tqdm(self.train_loader):\n",
    "            support_images = support_images.to(self.device)\n",
    "            query_images = query_images.to(self.device)\n",
    "            query_labels = query_labels.to(self.device)\n",
    "            \n",
    "            logits, triplet_loss = self.model(support_images, query_images, mode='train')\n",
    "            loss = self.criterion(logits, query_labels, triplet_loss)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(query_labels.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return self.calculate_metrics(all_labels, all_preds, total_loss, len(self.train_loader))\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for support_images, query_images, query_labels in tqdm(self.val_loader):\n",
    "                support_images = support_images.to(self.device)\n",
    "                query_images = query_images.to(self.device)\n",
    "                query_labels = query_labels.to(self.device)\n",
    "                \n",
    "                logits = self.model(support_images, query_images, mode='test')\n",
    "                loss = self.criterion(logits, query_labels, torch.tensor(0.))\n",
    "                \n",
    "                preds = logits.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(query_labels.cpu().numpy())\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return self.calculate_metrics(all_labels, all_preds, total_loss, len(self.val_loader))\n",
    "    \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for support_images, query_images, query_labels in tqdm(self.test_loader):\n",
    "                support_images = support_images.to(self.device)\n",
    "                query_images = query_images.to(self.device)\n",
    "                query_labels = query_labels.to(self.device)\n",
    "                \n",
    "                logits = self.model(support_images, query_images, mode='test')\n",
    "                preds = logits.argmax(dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(query_labels.cpu().numpy())\n",
    "        \n",
    "        metrics = self.calculate_metrics(all_labels, all_preds)\n",
    "        self.plot_confusion_matrix(all_labels, all_preds, \"Test Set Confusion Matrix\")\n",
    "        return metrics\n",
    "    \n",
    "    def plot_confusion_matrix(self, labels, preds, title):\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(title)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.train_metrics) + 1)\n",
    "        metrics = ['accuracy', 'f1', 'loss']\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            train_values = [m[metric] for m in self.train_metrics]\n",
    "            val_values = [m[metric] for m in self.val_metrics]\n",
    "            \n",
    "            plt.plot(epochs, train_values, 'b-', label='Train')\n",
    "            plt.plot(epochs, val_values, 'r-', label='Validation')\n",
    "            plt.title(f'{metric.capitalize()} vs Epochs')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
    "            \n",
    "            train_metrics = self.train_epoch()\n",
    "            val_metrics = self.validate()\n",
    "            \n",
    "            self.train_metrics.append(train_metrics)\n",
    "            self.val_metrics.append(val_metrics)\n",
    "            \n",
    "            print(f\"Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1']:.4f}\")\n",
    "            print(f\"Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "            \n",
    "            if val_metrics['f1'] > self.best_val_f1:\n",
    "                self.best_val_f1 = val_metrics['f1']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_f1': self.best_val_f1,\n",
    "                }, 'best_model.pth')\n",
    "        \n",
    "        self.plot_metrics()\n",
    "        \n",
    "        checkpoint = torch.load('best_model.pth')\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        test_metrics = self.test()\n",
    "        \n",
    "        print(\"\\nTest Results:\")\n",
    "        print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "        \n",
    "        return test_metrics\n",
    "\n",
    " \n",
    " \n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    n_way = 15\n",
    "    k_shot = 10\n",
    "    n_query = 5\n",
    "    batch_size = 1  # کاهش batch size\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "    \n",
    "    train_dataset = FewShotDataset(path_data+'/train/', transform, n_way, k_shot, n_query, n_episodes=100)\n",
    "    val_dataset = FewShotDataset(path_data+'/val/', transform, n_way, k_shot, n_query, n_episodes=50)\n",
    "    test_dataset = FewShotDataset(path_data+'/test/', transform, n_way, k_shot, n_query, n_episodes=50)\n",
    "    \n",
    "    # حذف num_workers و کاهش batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = InnovativeFewShotViT(n_way=n_way, k_shot=k_shot).to(device)\n",
    "    criterion = CombinedLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    test_metrics = trainer.train()\n",
    "    return test_metrics\n",
    "# if __name__ == \"__main__\":\n",
    "print(\"Starting training...\")\n",
    "test_metrics = main()\n",
    "print(\"\\nTraining completed!\")\n",
    "                                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
