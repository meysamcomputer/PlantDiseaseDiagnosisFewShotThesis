{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ee0cc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 54:12, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.732782</td>\n",
       "      <td>0.164241</td>\n",
       "      <td>0.120279</td>\n",
       "      <td>0.204207</td>\n",
       "      <td>0.164241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.685914</td>\n",
       "      <td>0.235323</td>\n",
       "      <td>0.193445</td>\n",
       "      <td>0.288406</td>\n",
       "      <td>0.235323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.654181</td>\n",
       "      <td>0.265648</td>\n",
       "      <td>0.208060</td>\n",
       "      <td>0.346479</td>\n",
       "      <td>0.265648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.630801</td>\n",
       "      <td>0.278991</td>\n",
       "      <td>0.234754</td>\n",
       "      <td>0.367661</td>\n",
       "      <td>0.278991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.591330</td>\n",
       "      <td>0.310771</td>\n",
       "      <td>0.274267</td>\n",
       "      <td>0.400238</td>\n",
       "      <td>0.310771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.574254</td>\n",
       "      <td>0.332363</td>\n",
       "      <td>0.306214</td>\n",
       "      <td>0.400119</td>\n",
       "      <td>0.332363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.549453</td>\n",
       "      <td>0.334546</td>\n",
       "      <td>0.298515</td>\n",
       "      <td>0.425055</td>\n",
       "      <td>0.334546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.519654</td>\n",
       "      <td>0.352014</td>\n",
       "      <td>0.313141</td>\n",
       "      <td>0.449219</td>\n",
       "      <td>0.352014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.493922</td>\n",
       "      <td>0.372392</td>\n",
       "      <td>0.339356</td>\n",
       "      <td>0.441482</td>\n",
       "      <td>0.372392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.469326</td>\n",
       "      <td>0.387433</td>\n",
       "      <td>0.364914</td>\n",
       "      <td>0.455962</td>\n",
       "      <td>0.387433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.453848</td>\n",
       "      <td>0.379427</td>\n",
       "      <td>0.351763</td>\n",
       "      <td>0.452160</td>\n",
       "      <td>0.379427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.429134</td>\n",
       "      <td>0.398108</td>\n",
       "      <td>0.370016</td>\n",
       "      <td>0.448597</td>\n",
       "      <td>0.398108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.414721</td>\n",
       "      <td>0.401019</td>\n",
       "      <td>0.374497</td>\n",
       "      <td>0.460077</td>\n",
       "      <td>0.401019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.393054</td>\n",
       "      <td>0.402960</td>\n",
       "      <td>0.379314</td>\n",
       "      <td>0.448873</td>\n",
       "      <td>0.402960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.376024</td>\n",
       "      <td>0.416303</td>\n",
       "      <td>0.392073</td>\n",
       "      <td>0.470077</td>\n",
       "      <td>0.416303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.360904</td>\n",
       "      <td>0.403445</td>\n",
       "      <td>0.381808</td>\n",
       "      <td>0.452808</td>\n",
       "      <td>0.403445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.347464</td>\n",
       "      <td>0.413877</td>\n",
       "      <td>0.390331</td>\n",
       "      <td>0.456357</td>\n",
       "      <td>0.413877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.336259</td>\n",
       "      <td>0.417273</td>\n",
       "      <td>0.395909</td>\n",
       "      <td>0.463511</td>\n",
       "      <td>0.417273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.320638</td>\n",
       "      <td>0.426249</td>\n",
       "      <td>0.402376</td>\n",
       "      <td>0.474433</td>\n",
       "      <td>0.426249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.308253</td>\n",
       "      <td>0.424309</td>\n",
       "      <td>0.401596</td>\n",
       "      <td>0.466605</td>\n",
       "      <td>0.424309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.296908</td>\n",
       "      <td>0.434498</td>\n",
       "      <td>0.411019</td>\n",
       "      <td>0.480990</td>\n",
       "      <td>0.434498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.289588</td>\n",
       "      <td>0.429646</td>\n",
       "      <td>0.410255</td>\n",
       "      <td>0.471569</td>\n",
       "      <td>0.429646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.272789</td>\n",
       "      <td>0.438622</td>\n",
       "      <td>0.416689</td>\n",
       "      <td>0.474351</td>\n",
       "      <td>0.438622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.269991</td>\n",
       "      <td>0.431101</td>\n",
       "      <td>0.407807</td>\n",
       "      <td>0.481093</td>\n",
       "      <td>0.431101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.254381</td>\n",
       "      <td>0.443717</td>\n",
       "      <td>0.420854</td>\n",
       "      <td>0.473663</td>\n",
       "      <td>0.443717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.244958</td>\n",
       "      <td>0.446628</td>\n",
       "      <td>0.422155</td>\n",
       "      <td>0.485940</td>\n",
       "      <td>0.446628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.236823</td>\n",
       "      <td>0.441776</td>\n",
       "      <td>0.417547</td>\n",
       "      <td>0.482105</td>\n",
       "      <td>0.441776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.234486</td>\n",
       "      <td>0.443231</td>\n",
       "      <td>0.419378</td>\n",
       "      <td>0.484667</td>\n",
       "      <td>0.443231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.217246</td>\n",
       "      <td>0.447598</td>\n",
       "      <td>0.425505</td>\n",
       "      <td>0.485632</td>\n",
       "      <td>0.447598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.212844</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.422022</td>\n",
       "      <td>0.479013</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.206085</td>\n",
       "      <td>0.456089</td>\n",
       "      <td>0.433760</td>\n",
       "      <td>0.490568</td>\n",
       "      <td>0.456089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.197594</td>\n",
       "      <td>0.456817</td>\n",
       "      <td>0.435284</td>\n",
       "      <td>0.487929</td>\n",
       "      <td>0.456817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.190372</td>\n",
       "      <td>0.452693</td>\n",
       "      <td>0.429662</td>\n",
       "      <td>0.486995</td>\n",
       "      <td>0.452693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.177858</td>\n",
       "      <td>0.458273</td>\n",
       "      <td>0.434868</td>\n",
       "      <td>0.485689</td>\n",
       "      <td>0.458273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.179389</td>\n",
       "      <td>0.458273</td>\n",
       "      <td>0.434913</td>\n",
       "      <td>0.485916</td>\n",
       "      <td>0.458273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.169917</td>\n",
       "      <td>0.457545</td>\n",
       "      <td>0.433232</td>\n",
       "      <td>0.483694</td>\n",
       "      <td>0.457545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.167651</td>\n",
       "      <td>0.447356</td>\n",
       "      <td>0.420442</td>\n",
       "      <td>0.487458</td>\n",
       "      <td>0.447356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.161942</td>\n",
       "      <td>0.458758</td>\n",
       "      <td>0.434955</td>\n",
       "      <td>0.488475</td>\n",
       "      <td>0.458758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.157745</td>\n",
       "      <td>0.458030</td>\n",
       "      <td>0.434914</td>\n",
       "      <td>0.489538</td>\n",
       "      <td>0.458030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.151981</td>\n",
       "      <td>0.456332</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.490264</td>\n",
       "      <td>0.456332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.146845</td>\n",
       "      <td>0.454634</td>\n",
       "      <td>0.432329</td>\n",
       "      <td>0.489126</td>\n",
       "      <td>0.454634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.141754</td>\n",
       "      <td>0.459243</td>\n",
       "      <td>0.433785</td>\n",
       "      <td>0.487267</td>\n",
       "      <td>0.459243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.139354</td>\n",
       "      <td>0.456089</td>\n",
       "      <td>0.432120</td>\n",
       "      <td>0.488326</td>\n",
       "      <td>0.456089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.126658</td>\n",
       "      <td>0.463610</td>\n",
       "      <td>0.440006</td>\n",
       "      <td>0.493337</td>\n",
       "      <td>0.463610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.124400</td>\n",
       "      <td>0.461912</td>\n",
       "      <td>0.437708</td>\n",
       "      <td>0.487834</td>\n",
       "      <td>0.461912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.122521</td>\n",
       "      <td>0.461669</td>\n",
       "      <td>0.436031</td>\n",
       "      <td>0.491562</td>\n",
       "      <td>0.461669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.118265</td>\n",
       "      <td>0.451237</td>\n",
       "      <td>0.424720</td>\n",
       "      <td>0.485002</td>\n",
       "      <td>0.451237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.120069</td>\n",
       "      <td>0.460213</td>\n",
       "      <td>0.434420</td>\n",
       "      <td>0.491453</td>\n",
       "      <td>0.460213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.108890</td>\n",
       "      <td>0.458030</td>\n",
       "      <td>0.433706</td>\n",
       "      <td>0.489929</td>\n",
       "      <td>0.458030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.108342</td>\n",
       "      <td>0.460456</td>\n",
       "      <td>0.433293</td>\n",
       "      <td>0.488500</td>\n",
       "      <td>0.460456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.103843</td>\n",
       "      <td>0.457060</td>\n",
       "      <td>0.428501</td>\n",
       "      <td>0.485863</td>\n",
       "      <td>0.457060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.103030</td>\n",
       "      <td>0.461426</td>\n",
       "      <td>0.434119</td>\n",
       "      <td>0.487235</td>\n",
       "      <td>0.461426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.096617</td>\n",
       "      <td>0.459728</td>\n",
       "      <td>0.432651</td>\n",
       "      <td>0.483984</td>\n",
       "      <td>0.459728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.092170</td>\n",
       "      <td>0.466521</td>\n",
       "      <td>0.439712</td>\n",
       "      <td>0.490455</td>\n",
       "      <td>0.466521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.094969</td>\n",
       "      <td>0.460941</td>\n",
       "      <td>0.435259</td>\n",
       "      <td>0.489696</td>\n",
       "      <td>0.460941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.085130</td>\n",
       "      <td>0.462639</td>\n",
       "      <td>0.435378</td>\n",
       "      <td>0.492319</td>\n",
       "      <td>0.462639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.089039</td>\n",
       "      <td>0.461184</td>\n",
       "      <td>0.436348</td>\n",
       "      <td>0.489770</td>\n",
       "      <td>0.461184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.081836</td>\n",
       "      <td>0.465066</td>\n",
       "      <td>0.438704</td>\n",
       "      <td>0.490673</td>\n",
       "      <td>0.465066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.083665</td>\n",
       "      <td>0.463610</td>\n",
       "      <td>0.437733</td>\n",
       "      <td>0.490830</td>\n",
       "      <td>0.463610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.080573</td>\n",
       "      <td>0.463367</td>\n",
       "      <td>0.437713</td>\n",
       "      <td>0.493142</td>\n",
       "      <td>0.463367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.079352</td>\n",
       "      <td>0.463125</td>\n",
       "      <td>0.437074</td>\n",
       "      <td>0.489011</td>\n",
       "      <td>0.463125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.077903</td>\n",
       "      <td>0.460456</td>\n",
       "      <td>0.433301</td>\n",
       "      <td>0.488672</td>\n",
       "      <td>0.460456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.076804</td>\n",
       "      <td>0.461912</td>\n",
       "      <td>0.436432</td>\n",
       "      <td>0.488301</td>\n",
       "      <td>0.461912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.075621</td>\n",
       "      <td>0.462397</td>\n",
       "      <td>0.436836</td>\n",
       "      <td>0.490054</td>\n",
       "      <td>0.462397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.071640</td>\n",
       "      <td>0.466764</td>\n",
       "      <td>0.441478</td>\n",
       "      <td>0.491652</td>\n",
       "      <td>0.466764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>2.072313</td>\n",
       "      <td>0.463852</td>\n",
       "      <td>0.438046</td>\n",
       "      <td>0.488556</td>\n",
       "      <td>0.463852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.069144</td>\n",
       "      <td>0.463367</td>\n",
       "      <td>0.436774</td>\n",
       "      <td>0.489697</td>\n",
       "      <td>0.463367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.069621</td>\n",
       "      <td>0.464823</td>\n",
       "      <td>0.439765</td>\n",
       "      <td>0.490145</td>\n",
       "      <td>0.464823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.068758</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>0.438887</td>\n",
       "      <td>0.490701</td>\n",
       "      <td>0.465551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.068108</td>\n",
       "      <td>0.462397</td>\n",
       "      <td>0.435923</td>\n",
       "      <td>0.489094</td>\n",
       "      <td>0.462397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.067242</td>\n",
       "      <td>0.465308</td>\n",
       "      <td>0.438937</td>\n",
       "      <td>0.490863</td>\n",
       "      <td>0.465308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.066086</td>\n",
       "      <td>0.462397</td>\n",
       "      <td>0.435815</td>\n",
       "      <td>0.488295</td>\n",
       "      <td>0.462397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.065804</td>\n",
       "      <td>0.463852</td>\n",
       "      <td>0.437208</td>\n",
       "      <td>0.491142</td>\n",
       "      <td>0.463852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.064319</td>\n",
       "      <td>0.463852</td>\n",
       "      <td>0.437641</td>\n",
       "      <td>0.488645</td>\n",
       "      <td>0.463852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.063763</td>\n",
       "      <td>0.464580</td>\n",
       "      <td>0.437966</td>\n",
       "      <td>0.490995</td>\n",
       "      <td>0.464580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.063191</td>\n",
       "      <td>0.463125</td>\n",
       "      <td>0.436214</td>\n",
       "      <td>0.488137</td>\n",
       "      <td>0.463125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.062845</td>\n",
       "      <td>0.464095</td>\n",
       "      <td>0.436713</td>\n",
       "      <td>0.489502</td>\n",
       "      <td>0.464095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.061980</td>\n",
       "      <td>0.463852</td>\n",
       "      <td>0.437104</td>\n",
       "      <td>0.489498</td>\n",
       "      <td>0.463852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.061403</td>\n",
       "      <td>0.464095</td>\n",
       "      <td>0.437177</td>\n",
       "      <td>0.489872</td>\n",
       "      <td>0.464095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.060906</td>\n",
       "      <td>0.464823</td>\n",
       "      <td>0.438573</td>\n",
       "      <td>0.490803</td>\n",
       "      <td>0.464823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.060486</td>\n",
       "      <td>0.463610</td>\n",
       "      <td>0.436680</td>\n",
       "      <td>0.489180</td>\n",
       "      <td>0.463610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.060087</td>\n",
       "      <td>0.464095</td>\n",
       "      <td>0.437522</td>\n",
       "      <td>0.489810</td>\n",
       "      <td>0.464095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.059628</td>\n",
       "      <td>0.463852</td>\n",
       "      <td>0.437207</td>\n",
       "      <td>0.489602</td>\n",
       "      <td>0.463852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.058987</td>\n",
       "      <td>0.464580</td>\n",
       "      <td>0.437793</td>\n",
       "      <td>0.489977</td>\n",
       "      <td>0.464580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.058626</td>\n",
       "      <td>0.463852</td>\n",
       "      <td>0.436987</td>\n",
       "      <td>0.489533</td>\n",
       "      <td>0.463852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.058329</td>\n",
       "      <td>0.464338</td>\n",
       "      <td>0.437602</td>\n",
       "      <td>0.489723</td>\n",
       "      <td>0.464338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.057891</td>\n",
       "      <td>0.464580</td>\n",
       "      <td>0.437783</td>\n",
       "      <td>0.489820</td>\n",
       "      <td>0.464580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.057626</td>\n",
       "      <td>0.464338</td>\n",
       "      <td>0.437595</td>\n",
       "      <td>0.489360</td>\n",
       "      <td>0.464338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.057445</td>\n",
       "      <td>0.464580</td>\n",
       "      <td>0.437834</td>\n",
       "      <td>0.489928</td>\n",
       "      <td>0.464580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.056968</td>\n",
       "      <td>0.464338</td>\n",
       "      <td>0.437681</td>\n",
       "      <td>0.489690</td>\n",
       "      <td>0.464338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.056935</td>\n",
       "      <td>0.464823</td>\n",
       "      <td>0.438157</td>\n",
       "      <td>0.490357</td>\n",
       "      <td>0.464823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.056554</td>\n",
       "      <td>0.464823</td>\n",
       "      <td>0.437974</td>\n",
       "      <td>0.490270</td>\n",
       "      <td>0.464823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.056459</td>\n",
       "      <td>0.464338</td>\n",
       "      <td>0.437491</td>\n",
       "      <td>0.489287</td>\n",
       "      <td>0.464338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.056285</td>\n",
       "      <td>0.464338</td>\n",
       "      <td>0.437508</td>\n",
       "      <td>0.489349</td>\n",
       "      <td>0.464338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.056096</td>\n",
       "      <td>0.464823</td>\n",
       "      <td>0.438070</td>\n",
       "      <td>0.490548</td>\n",
       "      <td>0.464823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.056008</td>\n",
       "      <td>0.464823</td>\n",
       "      <td>0.438081</td>\n",
       "      <td>0.490878</td>\n",
       "      <td>0.464823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.055878</td>\n",
       "      <td>0.464580</td>\n",
       "      <td>0.437764</td>\n",
       "      <td>0.490172</td>\n",
       "      <td>0.464580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.055863</td>\n",
       "      <td>0.463852</td>\n",
       "      <td>0.437037</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>0.463852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>2.055826</td>\n",
       "      <td>0.464338</td>\n",
       "      <td>0.437528</td>\n",
       "      <td>0.490107</td>\n",
       "      <td>0.464338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.190600</td>\n",
       "      <td>2.055809</td>\n",
       "      <td>0.463852</td>\n",
       "      <td>0.436999</td>\n",
       "      <td>0.489384</td>\n",
       "      <td>0.463852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='258' max='258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [258/258 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "{'eval_loss': 2.0558090209960938, 'eval_accuracy': 0.4638524987869966, 'eval_f1': 0.43699885682505457, 'eval_precision': 0.4893838968773778, 'eval_recall': 0.4638524987869966, 'eval_runtime': 30.0614, 'eval_samples_per_second': 137.119, 'eval_steps_per_second': 8.582, 'epoch': 100.0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7e5630ecf1ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pixel_values\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ورودی‌های پیش پردازش شده به مدل می‌دهیم\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mtrain_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         outputs = self.vit(\n\u001b[0m\u001b[0;32m    841\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    617\u001b[0m             \u001b[0mpixel_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         embedding_output = self.embeddings(\n\u001b[0m\u001b[0;32m    620\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool_masked_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m    122\u001b[0m     ) -> torch.Tensor:\n\u001b[0;32m    123\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbool_masked_pos\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m    181\u001b[0m                     \u001b[1;34mf\" ({self.image_size[0]}*{self.image_size[1]}).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 )\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "#!pip install transformers datasets      \n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "#sys.path.insert(0,'C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "\n",
    "#!pip install transformers datasets      \n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "#sys.path.insert(0,'C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# بارگذاری feature extractor از ViT\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# تعریف transform مناسب برای پردازش تصاویر\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # تغییر اندازه به 224x224\n",
    "    transforms.ToTensor(),          # تبدیل تصویر به Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # نرمال‌سازی\n",
    "])\n",
    "\n",
    "# ساخت یک Dataset سفارشی\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        # تبدیل تصویر به PIL.Image و سپس پردازش با ViTImageProcessor\n",
    "        image = transforms.ToPILImage()(image)  # تبدیل از Tensor به PIL.Image\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": label}\n",
    "    \n",
    "path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset_fiveshot'\n",
    "# بارگذاری داده‌ها با استفاده از CustomImageDataset\n",
    "train_dataset = CustomImageDataset(path_data + '/train/', transform=train_transform)\n",
    "test_dataset = CustomImageDataset(path_data + '/test/', transform=train_transform)\n",
    "\n",
    "# بارگذاری مدل ViT برای طبقه‌بندی تصویر\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=16)  # تغییر تعداد کلاس‌ها به 16\n",
    "\n",
    "# تنظیمات آموزش\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit_finetuned\",         # مکانی برای ذخیره مدل نهایی\n",
    "    evaluation_strategy=\"epoch\",          # ارزیابی در پایان هر epoch\n",
    "    learning_rate=2e-5,                   # نرخ یادگیری برای fine-tuning\n",
    "    per_device_train_batch_size=5,       # اندازه بچ برای آموزش\n",
    "    per_device_eval_batch_size=16,        # اندازه بچ برای ارزیابی\n",
    "    num_train_epochs=100,                   # تعداد epochs برای آموزش\n",
    "    weight_decay=0.01,                    # تنظیمات weight decay\n",
    "    save_total_limit=2,                   # ذخیره تنها آخرین ۲ checkpoint\n",
    "    logging_dir='./logs',                 # مسیر ذخیره لاگ‌ها\n",
    ")\n",
    "\n",
    "# تابع محاسبه متریک‌ها\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# راه‌اندازی Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                           # مدل\n",
    "    args=training_args,                    # تنظیمات آموزش\n",
    "    train_dataset=train_dataset,           # داده‌های آموزشی\n",
    "    eval_dataset=test_dataset,             # داده‌های ارزیابی\n",
    "    compute_metrics=compute_metrics        # محاسبه متریک‌ها\n",
    ")\n",
    "\n",
    "# شروع آموزش مدل\n",
    "trainer.train()\n",
    "\n",
    "# ارزیابی مدل پس از آموزش\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(results)\n",
    "\n",
    "# استخراج ویژگی‌ها و برچسب‌ها برای t-SNE\n",
    "model.eval()\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "# استخراج ویژگی‌ها از مدل\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = model(pixel_values=inputs)  # ورودی‌های پیش پردازش شده به مدل می‌دهیم\n",
    "        features = outputs.logits.cpu().numpy()\n",
    "        train_features.extend(features)\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# استفاده از t-SNE برای کاهش ابعاد\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(train_features)\n",
    "\n",
    "# رسم نتیجه t-SNE\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=train_labels, cmap='viridis', s=10)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"t-SNE Visualization of ViT Features\")\n",
    "plt.show()\n",
    "\n",
    "# ذخیره مدل آموزش‌دیده\n",
    "trainer.save_model(\"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/vit_finetuned_model_fiveShot\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import ViTForImageClassification, ViTFeatureExtractor, Trainer, TrainingArguments\n",
    "# from datasets import load_dataset\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Load CIFAR-10 dataset from Hugging Face\n",
    "# dataset = load_dataset(\"cifar10\")\n",
    "\n",
    "# # Split the dataset into train and validation sets\n",
    "# train_dataset = dataset['train']\n",
    "# val_dataset = dataset['test']\n",
    "\n",
    "# # Load pre-trained ViT feature extractor\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# # Preprocessing function for ViT\n",
    "# def preprocess_function(examples):\n",
    "#     return feature_extractor([x['img'] for x in examples], return_tensors=\"pt\")\n",
    "\n",
    "# # Preprocess the dataset\n",
    "# train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "# val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# # Set format for PyTorch\n",
    "# train_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "# val_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "\n",
    "# # Load pre-trained ViT model for image classification\n",
    "# model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=10)\n",
    "\n",
    "# # Define the training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./vit_finetuned\",         # Where to store the final model\n",
    "#     evaluation_strategy=\"epoch\",          # Evaluate at the end of each epoch\n",
    "#     learning_rate=2e-5,                   # Learning rate for fine-tuning\n",
    "#     per_device_train_batch_size=16,       # Batch size for training\n",
    "#     per_device_eval_batch_size=64,        # Batch size for evaluation\n",
    "#     num_train_epochs=3,                   # Number of training epochs\n",
    "#     weight_decay=0.01,                    # Weight decay for regularization\n",
    "#     save_total_limit=2,                   # Save only the last 2 checkpoints\n",
    "#     logging_dir='./logs',                 # Directory for logs\n",
    "# )\n",
    "\n",
    "# # Define the metrics for evaluation\n",
    "# def compute_metrics(p):\n",
    "#     preds = np.argmax(p.predictions, axis=1)\n",
    "#     labels = p.label_ids\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     f1 = f1_score(labels, preds, average='weighted')\n",
    "#     precision = precision_score(labels, preds, average='weighted')\n",
    "#     recall = recall_score(labels, preds, average='weighted')\n",
    "\n",
    "#     return {\n",
    "#         \"accuracy\": acc,\n",
    "#         \"f1\": f1,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall\n",
    "#     }\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,                           # The model to be fine-tuned\n",
    "#     args=training_args,                    # Training arguments\n",
    "#     train_dataset=train_dataset,           # Training dataset\n",
    "#     eval_dataset=val_dataset,              # Validation dataset\n",
    "#     compute_metrics=compute_metrics        # Evaluation metrics\n",
    "# )\n",
    "\n",
    "# # Start fine-tuning\n",
    "# trainer.train()\n",
    "\n",
    "# # Evaluate the model after fine-tuning\n",
    "# results = trainer.evaluate()\n",
    "# print(\"Evaluation Results:\")\n",
    "# print(results)\n",
    "\n",
    "# # Get features and labels for t-SNE\n",
    "# model.eval()\n",
    "# train_features = []\n",
    "# train_labels = []\n",
    "\n",
    "# # Extract features from the model\n",
    "# with torch.no_grad():\n",
    "#     for batch in DataLoader(train_dataset, batch_size=32):\n",
    "#         inputs = batch['input_ids']\n",
    "#         labels = batch['label']\n",
    "#         outputs = model(**inputs)\n",
    "#         features = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "#         train_features.extend(features)\n",
    "#         train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# train_features = np.array(train_features)\n",
    "# train_labels = np.array(train_labels)\n",
    "\n",
    "# # Use t-SNE for dimensionality reduction\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# tsne_results = tsne.fit_transform(train_features)\n",
    "\n",
    "# # Plot the t-SNE result\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=train_labels, cmap='viridis', s=10)\n",
    "# plt.colorbar(scatter)\n",
    "# plt.title(\"t-SNE Visualization of ViT Features\")\n",
    "# plt.show()\n",
    "\n",
    "# # Save the fine-tuned model\n",
    "# trainer.save_model(\"./vit_finetuned_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc8384cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers[torch]\n",
    "# !pip install --upgrade torch\n",
    "#!pip show tokenizers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5320362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers==0.19.1\n",
      "  Using cached tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tokenizers==0.19.1) (0.24.6)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.67.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.3)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "Successfully installed tokenizers-0.19.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.46.3 requires tokenizers<0.21,>=0.20, but you have tokenizers 0.19.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tokenizers\n",
    "!pip install tokenizers==0.19.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f8af1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Using cached tokenizers-0.20.3-cp39-none-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed tokenizers-0.20.3\n",
      "Name: tokenizers\n",
      "Version: 0.20.3\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: Anthony MOI <m.anthony.moi@gmail.com>\n",
      "Author-email: Nicolas Patry <patry.nicolas@protonmail.com>, Anthony Moi <anthony@huggingface.co>\n",
      "License: \n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: huggingface-hub\n",
      "Required-by: transformers\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip show tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee109ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1884749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60c6cf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (4.28.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (1.24.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (0.24.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.28.0) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\tokenizers-0.13.3.dist-info\\\\METADATA'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94056a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbd317fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers==0.19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15cd70ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizers\n",
      "Version: 0.19.1\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: Anthony MOI <m.anthony.moi@gmail.com>\n",
      "Author-email: Nicolas Patry <patry.nicolas@protonmail.com>, Anthony Moi <anthony@huggingface.co>\n",
      "License: \n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: huggingface-hub\n",
      "Required-by: transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c2a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (4.28.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (0.24.6)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2022.7.9)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.28.0) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers==4.28.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (3.3)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed tokenizers-0.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a0c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers==0.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae85211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizers\n",
      "Version: 0.13.3\n",
      "Summary: Fast and Customizable Tokenizers\n",
      "Home-page: https://github.com/huggingface/tokenizers\n",
      "Author: Anthony MOI\n",
      "Author-email: anthony@huggingface.co\n",
      "License: Apache License 2.0\n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae06217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate>={ACCELERATE_MIN_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9a257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
