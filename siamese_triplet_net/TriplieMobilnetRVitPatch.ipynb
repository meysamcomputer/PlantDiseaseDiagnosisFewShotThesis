{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2c6b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mey\\AppData\\Local\\Temp\\ipykernel_1544\\953445759.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  siamese_model = torch.load(\"C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/siamese_Mobilenet_15Shot.h5\", map_location=torch.device(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([1, 197, 768])\n",
      "embedding 0 size: (32, 197, 768)\n",
      "embedding 1 size: (32, 197, 768)\n",
      "embedding 2 size: (32, 197, 768)\n",
      "embedding 3 size: (32, 197, 768)\n",
      "embedding 4 size: (32, 197, 768)\n",
      "embedding 5 size: (32, 197, 768)\n",
      "embedding 6 size: (32, 197, 768)\n",
      "embedding 7 size: (1, 197, 768)\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([32, 197, 768])\n",
      "patch_embeddings size:torch.Size([25, 197, 768])\n",
      "embedding 0 size: (32, 197, 768)\n",
      "embedding 1 size: (32, 197, 768)\n",
      "embedding 2 size: (32, 197, 768)\n",
      "embedding 3 size: (32, 197, 768)\n",
      "embedding 4 size: (32, 197, 768)\n",
      "embedding 5 size: (32, 197, 768)\n",
      "embedding 6 size: (32, 197, 768)\n",
      "embedding 7 size: (32, 197, 768)\n",
      "embedding 8 size: (32, 197, 768)\n",
      "embedding 9 size: (32, 197, 768)\n",
      "embedding 10 size: (32, 197, 768)\n",
      "embedding 11 size: (32, 197, 768)\n",
      "embedding 12 size: (32, 197, 768)\n",
      "embedding 13 size: (32, 197, 768)\n",
      "embedding 14 size: (32, 197, 768)\n",
      "embedding 15 size: (32, 197, 768)\n",
      "embedding 16 size: (32, 197, 768)\n",
      "embedding 17 size: (32, 197, 768)\n",
      "embedding 18 size: (32, 197, 768)\n",
      "embedding 19 size: (32, 197, 768)\n",
      "embedding 20 size: (32, 197, 768)\n",
      "embedding 21 size: (32, 197, 768)\n",
      "embedding 22 size: (32, 197, 768)\n",
      "embedding 23 size: (32, 197, 768)\n",
      "embedding 24 size: (32, 197, 768)\n",
      "embedding 25 size: (32, 197, 768)\n",
      "embedding 26 size: (32, 197, 768)\n",
      "embedding 27 size: (32, 197, 768)\n",
      "embedding 28 size: (32, 197, 768)\n",
      "embedding 29 size: (32, 197, 768)\n",
      "embedding 30 size: (32, 197, 768)\n",
      "embedding 31 size: (32, 197, 768)\n",
      "embedding 32 size: (32, 197, 768)\n",
      "embedding 33 size: (32, 197, 768)\n",
      "embedding 34 size: (32, 197, 768)\n",
      "embedding 35 size: (32, 197, 768)\n",
      "embedding 36 size: (32, 197, 768)\n",
      "embedding 37 size: (32, 197, 768)\n",
      "embedding 38 size: (32, 197, 768)\n",
      "embedding 39 size: (32, 197, 768)\n",
      "embedding 40 size: (32, 197, 768)\n",
      "embedding 41 size: (32, 197, 768)\n",
      "embedding 42 size: (32, 197, 768)\n",
      "embedding 43 size: (32, 197, 768)\n",
      "embedding 44 size: (32, 197, 768)\n",
      "embedding 45 size: (32, 197, 768)\n",
      "embedding 46 size: (32, 197, 768)\n",
      "embedding 47 size: (32, 197, 768)\n",
      "embedding 48 size: (32, 197, 768)\n",
      "embedding 49 size: (32, 197, 768)\n",
      "embedding 50 size: (32, 197, 768)\n",
      "embedding 51 size: (32, 197, 768)\n",
      "embedding 52 size: (32, 197, 768)\n",
      "embedding 53 size: (32, 197, 768)\n",
      "embedding 54 size: (32, 197, 768)\n",
      "embedding 55 size: (32, 197, 768)\n",
      "embedding 56 size: (32, 197, 768)\n",
      "embedding 57 size: (32, 197, 768)\n",
      "embedding 58 size: (32, 197, 768)\n",
      "embedding 59 size: (32, 197, 768)\n",
      "embedding 60 size: (32, 197, 768)\n",
      "embedding 61 size: (32, 197, 768)\n",
      "embedding 62 size: (32, 197, 768)\n",
      "embedding 63 size: (32, 197, 768)\n",
      "embedding 64 size: (32, 197, 768)\n",
      "embedding 65 size: (32, 197, 768)\n",
      "embedding 66 size: (32, 197, 768)\n",
      "embedding 67 size: (32, 197, 768)\n",
      "embedding 68 size: (32, 197, 768)\n",
      "embedding 69 size: (32, 197, 768)\n",
      "embedding 70 size: (32, 197, 768)\n",
      "embedding 71 size: (32, 197, 768)\n",
      "embedding 72 size: (32, 197, 768)\n",
      "embedding 73 size: (32, 197, 768)\n",
      "embedding 74 size: (32, 197, 768)\n",
      "embedding 75 size: (32, 197, 768)\n",
      "embedding 76 size: (32, 197, 768)\n",
      "embedding 77 size: (32, 197, 768)\n",
      "embedding 78 size: (32, 197, 768)\n",
      "embedding 79 size: (32, 197, 768)\n",
      "embedding 80 size: (32, 197, 768)\n",
      "embedding 81 size: (32, 197, 768)\n",
      "embedding 82 size: (32, 197, 768)\n",
      "embedding 83 size: (32, 197, 768)\n",
      "embedding 84 size: (32, 197, 768)\n",
      "embedding 85 size: (32, 197, 768)\n",
      "embedding 86 size: (32, 197, 768)\n",
      "embedding 87 size: (32, 197, 768)\n",
      "embedding 88 size: (32, 197, 768)\n",
      "embedding 89 size: (32, 197, 768)\n",
      "embedding 90 size: (32, 197, 768)\n",
      "embedding 91 size: (32, 197, 768)\n",
      "embedding 92 size: (32, 197, 768)\n",
      "embedding 93 size: (32, 197, 768)\n",
      "embedding 94 size: (32, 197, 768)\n",
      "embedding 95 size: (32, 197, 768)\n",
      "embedding 96 size: (32, 197, 768)\n",
      "embedding 97 size: (32, 197, 768)\n",
      "embedding 98 size: (32, 197, 768)\n",
      "embedding 99 size: (32, 197, 768)\n",
      "embedding 100 size: (32, 197, 768)\n",
      "embedding 101 size: (32, 197, 768)\n",
      "embedding 102 size: (32, 197, 768)\n",
      "embedding 103 size: (32, 197, 768)\n",
      "embedding 104 size: (32, 197, 768)\n",
      "embedding 105 size: (32, 197, 768)\n",
      "embedding 106 size: (32, 197, 768)\n",
      "embedding 107 size: (32, 197, 768)\n",
      "embedding 108 size: (32, 197, 768)\n",
      "embedding 109 size: (32, 197, 768)\n",
      "embedding 110 size: (32, 197, 768)\n",
      "embedding 111 size: (32, 197, 768)\n",
      "embedding 112 size: (32, 197, 768)\n",
      "embedding 113 size: (32, 197, 768)\n",
      "embedding 114 size: (32, 197, 768)\n",
      "embedding 115 size: (32, 197, 768)\n",
      "embedding 116 size: (32, 197, 768)\n",
      "embedding 117 size: (32, 197, 768)\n",
      "embedding 118 size: (32, 197, 768)\n",
      "embedding 119 size: (32, 197, 768)\n",
      "embedding 120 size: (32, 197, 768)\n",
      "embedding 121 size: (32, 197, 768)\n",
      "embedding 122 size: (32, 197, 768)\n",
      "embedding 123 size: (32, 197, 768)\n",
      "embedding 124 size: (32, 197, 768)\n",
      "embedding 125 size: (32, 197, 768)\n",
      "embedding 126 size: (32, 197, 768)\n",
      "embedding 127 size: (32, 197, 768)\n",
      "embedding 128 size: (25, 197, 768)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "import cv2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dataloaders import get_train_transforms, get_val_transforms, get_triplet_dataloader\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "path_data = 'C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "device = torch.cuda.is_available()\n",
    "\n",
    "# استخراج ویژگی‌ها با استفاده از مدل Siamese\n",
    "def generate_embeddings(data_loader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        device = torch.cuda.is_available()\n",
    "        for batch_imgs, batch_labels in data_loader:\n",
    "            \n",
    "            if device:\n",
    "                batch_imgs = batch_imgs.cuda()\n",
    "            \n",
    "            # تقسیم تصویر به پچ‌ها\n",
    "            batch_size = batch_imgs.size(0)\n",
    "            num_patches = (224 // 16) * (224 // 16)\n",
    "            embed_dim = 768\n",
    "            patches = batch_imgs.unfold(2, 16, 16).unfold(3, 16, 16)\n",
    "            patches = patches.contiguous().view(batch_size, 3, -1, 16, 16)\n",
    "            patches = patches.permute(0, 2, 1, 3, 4).contiguous().view(batch_size * num_patches, 3, 16, 16)\n",
    "            \n",
    "            # استخراج امبدینگ پچ‌ها\n",
    "            patch_embeddings = model.get_embedding(patches)\n",
    "            patch_embeddings = patch_embeddings.view(batch_size, num_patches, -1)\n",
    "            \n",
    "            # تغییر اندازه امبدینگ پچ‌ها به 768\n",
    "            patch_embeddings = nn.Linear(patch_embeddings.size(-1), embed_dim)(patch_embeddings)\n",
    "            \n",
    "            # اضافه کردن CLS token\n",
    "            cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)).expand(batch_size, -1, -1)\n",
    "            patch_embeddings = torch.cat((cls_token, patch_embeddings), dim=1)\n",
    "            print(f\"patch_embeddings size:{patch_embeddings.shape}\")\n",
    "            # اضافه کردن position embedding\n",
    "            pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "            patch_embeddings = patch_embeddings + pos_embed\n",
    "            \n",
    "            embeddings.append(patch_embeddings.cpu().numpy())\n",
    "            labels.append(batch_labels.numpy())\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            print(f\"embedding {i} size: {embedding.shape}\")    \n",
    "    return np.concatenate(embeddings), np.concatenate(labels)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "siamese_model = torch.load(\"C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/siamese_Mobilenet_15Shot.h5\", map_location=torch.device(device))\n",
    "siamese_model.eval()\n",
    "\n",
    "# تعریف مدل سفارشی ViT\n",
    "class RefinedViT(nn.Module):\n",
    "    def __init__(self, original_vit_model, embedding_dim, num_classes):\n",
    "        super(RefinedViT, self).__init__()\n",
    "        self.num_patches = (224 // 16) * (224 // 16)\n",
    "        self.embed_dim = 768\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=12,\n",
    "            dim_feedforward=3072,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=12)\n",
    "        \n",
    "        # لایه‌های نهایی\n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc = nn.Linear(self.embed_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # استفاده از CLS token برای طبقه‌بندی\n",
    "        x = x[:, 0]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=path_data + '/train/', transform=get_val_transforms())\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(root=path_data + '/test/', transform=get_val_transforms())\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# بارگذاری مدل ViT از Hugging Face\n",
    "vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model.classifier = torch.nn.Linear(vit_model.config.hidden_size, 15)\n",
    "\n",
    "embedding_dim = vit_model.config.hidden_size\n",
    "model = RefinedViT(vit_model, embedding_dim, num_classes=15)\n",
    "\n",
    "# استخراج ویژگی‌ها از داده‌های آموزش\n",
    "train_embeddings, train_labels = generate_embeddings(train_loader, siamese_model)\n",
    "# استخراج ویژگی‌ها از داده‌های تست\n",
    "test_embeddings, test_labels = generate_embeddings(test_loader, siamese_model)\n",
    "\n",
    "# تبدیل داده‌ها به تنسور\n",
    "X_train, y_train = train_embeddings, train_labels\n",
    "X_val, y_val = test_embeddings, test_labels\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# انتقال مدل به دستگاه CUDA\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# تنظیمات loss function و optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# تنظیمات آموزش\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "\n",
    "# آموزش\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor.to(device))\n",
    "    loss = criterion(outputs, y_train_tensor.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "torch.save(model, \"C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/RVIT_improvedpatchmobilenet.h5\")\n",
    "\n",
    "# ارزیابی مدل\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_val_tensor.to(device))\n",
    "    _, predicted = torch.max(val_outputs.data, 1)\n",
    "\n",
    "# ذخیره مدل\n",
    "torch.save(model.state_dict(), \"C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/RVIT_State_improvedpatchmobilenet.h5\")\n",
    "\n",
    "# محاسبه دقت و امتیاز F1\n",
    "accuracy = accuracy_score(y_val_tensor.cpu(), predicted.cpu())\n",
    "f1 = f1_score(y_val_tensor.cpu(), predicted.cpu(), average='weighted')\n",
    "precision = precision_score(y_val_tensor.cpu(), predicted.cpu(), average='macro')\n",
    "recall = recall_score(y_val_tensor.cpu(), predicted.cpu(), average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0792173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
