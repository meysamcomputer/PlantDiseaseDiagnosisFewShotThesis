{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from transformers import ViTModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "import torchvision\n",
    "from dataloaders import get_train_transforms, get_val_transforms, get_triplet_dataloader\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class InnovativeFewShotViT(nn.Module):\n",
    "    def __init__(self, n_way=15, k_shot=15, embed_dim=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.embedding = nn.Linear(768, embed_dim)\n",
    "        self.support_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.query_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.margin = 1.0\n",
    "        \n",
    "        self.vit.gradient_checkpointing_enable()\n",
    "\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward_one(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        if len(x.shape) > 4:\n",
    "            x = x.reshape(batch_size * x.size(1), 3, 224, 224)\n",
    "        elif len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        x = self.vit(x).last_hidden_state[:, 0]\n",
    "        embedding = self.embedding(x)\n",
    "        return F.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "    def get_prototypes(self, support_embeddings):\n",
    "        support_embeddings = support_embeddings.reshape(support_embeddings.size(0), self.n_way * self.k_shot, -1)\n",
    "        attended_support = self.support_attention(support_embeddings)\n",
    "        prototypes = attended_support.reshape(support_embeddings.size(0), self.n_way, self.k_shot, -1).mean(2)\n",
    "        return prototypes\n",
    "\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward(self, support_set, query, mode='train'):\n",
    "        batch_size = support_set.size(0)\n",
    "        device = support_set.device\n",
    "\n",
    "        # Process support set\n",
    "        support_embeddings = []\n",
    "        for i in range(self.n_way):\n",
    "            way_embeddings = []\n",
    "            support_batch = support_set[:, i].reshape(-1, 3, 224, 224)\n",
    "            emb = self.forward_one(support_batch)\n",
    "            emb = emb.reshape(batch_size, self.k_shot, -1)\n",
    "            support_embeddings.append(emb)\n",
    "        \n",
    "        support_embeddings = torch.stack(support_embeddings, dim=1)\n",
    "        prototypes = self.get_prototypes(support_embeddings)\n",
    "\n",
    "        # Process query set\n",
    "        query = query.reshape(-1, 3, 224, 224)\n",
    "        query_emb = self.forward_one(query)\n",
    "\n",
    "        logits = -torch.cdist(query_emb, prototypes[0])\n",
    "        return logits, torch.tensor(0.0, device=device)\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, triplet_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.triplet_weight = triplet_weight\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels, triplet_loss):\n",
    "        ce_loss = self.ce(logits, labels)\n",
    "        return ce_loss + self.triplet_weight * triplet_loss\n",
    "\n",
    "class FewShotDataset:\n",
    "    def __init__(self, root_dir, transform=None, n_way=15, k_shot=15, n_query=5, n_episodes=1000):\n",
    "        self.dataset = datasets.ImageFolder(root_dir, transform=transform)\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "        \n",
    "        self.label_to_indices = {}\n",
    "        for idx, (_, label) in enumerate(self.dataset):\n",
    "            if label not in self.label_to_indices:\n",
    "                self.label_to_indices[label] = []\n",
    "            self.label_to_indices[label].append(idx)\n",
    "        \n",
    "        self.valid_classes = [\n",
    "            cls for cls, indices in self.label_to_indices.items()\n",
    "            if len(indices) >= self.k_shot + self.n_query\n",
    "        ]\n",
    "        \n",
    "        if len(self.valid_classes) < self.n_way:\n",
    "            raise ValueError(f\"Not enough classes with sufficient samples. Found {len(self.valid_classes)} valid classes, need {self.n_way}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "    \n",
    "    def __getitem__(self, episode_index):\n",
    "        selected_classes = np.random.choice(self.valid_classes, self.n_way, replace=False)\n",
    "\n",
    "        support_images = torch.zeros(self.n_way, self.k_shot, 3, 224, 224)\n",
    "        query_images = []\n",
    "        query_labels = []\n",
    "\n",
    "        for class_idx, class_label in enumerate(selected_classes):\n",
    "            class_indices = self.label_to_indices[class_label]\n",
    "            selected_indices = np.random.choice(\n",
    "                class_indices, \n",
    "                self.k_shot + self.n_query, \n",
    "                replace=False\n",
    "            )\n",
    "\n",
    "            # Support set\n",
    "            for shot_idx, img_idx in enumerate(selected_indices[:self.k_shot]):\n",
    "                img, _ = self.dataset[img_idx]\n",
    "                support_images[class_idx, shot_idx] = img\n",
    "\n",
    "            # Query set\n",
    "            for img_idx in selected_indices[self.k_shot:self.k_shot + self.n_query]:\n",
    "                img, _ = self.dataset[img_idx]\n",
    "                query_images.append(img)\n",
    "                query_labels.append(class_idx)\n",
    "\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "\n",
    "        return support_images, query_images, query_labels\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, criterion, optimizer, device, num_epochs=50):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        self.best_val_f1 = 0\n",
    "        self.train_metrics = []\n",
    "        self.val_metrics = []\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def calculate_metrics(self, labels, preds, loss=None, n_batches=None):\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average='weighted', zero_division=0\n",
    "        )\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "        }\n",
    "        if loss is not None:\n",
    "            metrics['loss'] = loss / n_batches\n",
    "        return metrics\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for support_images, query_images, query_labels in tqdm(self.train_loader):\n",
    "            support_images = support_images.to(self.device)\n",
    "            query_images = query_images.to(self.device)\n",
    "            query_labels = query_labels.to(self.device).view(-1)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits, triplet_loss = self.model(support_images, query_images, mode='train')\n",
    "                loss = self.criterion(logits, query_labels, triplet_loss)\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = logits.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(query_labels.cpu().numpy())\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return self.calculate_metrics(all_labels, all_preds, total_loss, len(self.train_loader))\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for support_images, query_images, query_labels in tqdm(self.val_loader):\n",
    "                support_images = support_images.to(self.device)\n",
    "                query_images = query_images.to(self.device)\n",
    "                query_labels = query_labels.to(self.device).view(-1)  # Flatten labels\n",
    "                \n",
    "                logits, triplet_loss = self.model(support_images, query_images, mode='test')\n",
    "                loss = self.criterion(logits, query_labels, triplet_loss)\n",
    "                \n",
    "                preds = logits.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(query_labels.cpu().numpy())\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return self.calculate_metrics(all_labels, all_preds, total_loss, len(self.val_loader))\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for support_images, query_images, query_labels in tqdm(self.test_loader):\n",
    "                support_images = support_images.to(self.device)\n",
    "                query_images = query_images.to(self.device)\n",
    "                query_labels = query_labels.to(self.device)\n",
    "                \n",
    "                logits, _ = self.model(support_images, query_images, mode='test')\n",
    "                preds = logits.argmax(dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(query_labels.cpu().numpy())\n",
    "        \n",
    "        metrics = self.calculate_metrics(all_labels, all_preds)\n",
    "        self.plot_confusion_matrix(all_labels, all_preds, \"Test Set Confusion Matrix\")\n",
    "        return metrics\n",
    "\n",
    "    def plot_confusion_matrix(self, labels, preds, title):\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(title)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.train_metrics) + 1)\n",
    "        metrics = ['accuracy', 'f1', 'loss']\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            train_values = [m[metric] for m in self.train_metrics]\n",
    "            val_values = [m[metric] for m in self.val_metrics]\n",
    "            \n",
    "            plt.plot(epochs, train_values, 'b-', label='Train')\n",
    "            plt.plot(epochs, val_values, 'r-', label='Validation')\n",
    "            plt.title(f'{metric.capitalize()} vs Epochs')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
    "            \n",
    "            train_metrics = self.train_epoch()\n",
    "            val_metrics = self.validate()\n",
    "            \n",
    "            self.train_metrics.append(train_metrics)\n",
    "            self.val_metrics.append(val_metrics)\n",
    "            \n",
    "            print(f\"Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1']:.4f}\")\n",
    "            print(f\"Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "            \n",
    "            if val_metrics['f1'] > self.best_val_f1:\n",
    "                self.best_val_f1 = val_metrics['f1']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_f1': self.best_val_f1,\n",
    "                }, 'best_model.pth')\n",
    "        \n",
    "        self.plot_metrics()\n",
    "        \n",
    "        checkpoint = torch.load('best_model.pth')\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        test_metrics = self.test()\n",
    "        \n",
    "        print(\"\\nTest Results:\")\n",
    "        print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "        \n",
    "        return test_metrics\n",
    "\n",
    "def main():\n",
    "    n_way = 15\n",
    "    k_shot = 10\n",
    "    n_query = 5\n",
    "    batch_size = 2\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "    \n",
    "    val_dataset = FewShotDataset(path_data+'/val/', transform, n_way, k_shot, n_query, n_episodes=50)\n",
    "    test_dataset = FewShotDataset(path_data+'/test/', transform, n_way, k_shot, n_query, n_episodes=50)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    model = InnovativeFewShotViT(n_way=n_way, k_shot=k_shot).to(device)\n",
    "    criterion = CombinedLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    test_metrics = trainer.train()\n",
    "    return test_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting training...\")\n",
    "    test_metrics = main()\n",
    "    print(\"\\nTraining completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
