{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee0cc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/45 18:12 < 37:43, 0.01 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/65 30:47 < 1:32:21, 0.01 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install transformers datasets      \n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "#sys.path.insert(0,'C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "\n",
    "#!pip install transformers datasets      \n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "#sys.path.insert(0,'C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# بارگذاری feature extractor از ViT\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# تعریف transform مناسب برای پردازش تصاویر\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # تغییر اندازه به 224x224\n",
    "    transforms.ToTensor(),          # تبدیل تصویر به Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # نرمال‌سازی\n",
    "])\n",
    "\n",
    "# ساخت یک Dataset سفارشی\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        # تبدیل تصویر به PIL.Image و سپس پردازش با ViTImageProcessor\n",
    "        image = transforms.ToPILImage()(image)  # تبدیل از Tensor به PIL.Image\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": label}\n",
    "\n",
    "# بارگذاری داده‌ها با استفاده از CustomImageDataset\n",
    "train_dataset = CustomImageDataset(path_data + '/train/', transform=train_transform)\n",
    "test_dataset = CustomImageDataset(path_data + '/test/', transform=train_transform)\n",
    "\n",
    "# بارگذاری مدل ViT برای طبقه‌بندی تصویر\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=16)  # تغییر تعداد کلاس‌ها به 16\n",
    "\n",
    "# تنظیمات آموزش\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit_finetuned\",         # مکانی برای ذخیره مدل نهایی\n",
    "    evaluation_strategy=\"epoch\",          # ارزیابی در پایان هر epoch\n",
    "    learning_rate=2e-5,                   # نرخ یادگیری برای fine-tuning\n",
    "    per_device_train_batch_size=16,       # اندازه بچ برای آموزش\n",
    "    per_device_eval_batch_size=64,        # اندازه بچ برای ارزیابی\n",
    "    num_train_epochs=3,                   # تعداد epochs برای آموزش\n",
    "    weight_decay=0.01,                    # تنظیمات weight decay\n",
    "    save_total_limit=2,                   # ذخیره تنها آخرین ۲ checkpoint\n",
    "    logging_dir='./logs',                 # مسیر ذخیره لاگ‌ها\n",
    ")\n",
    "\n",
    "# تابع محاسبه متریک‌ها\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# راه‌اندازی Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                           # مدل\n",
    "    args=training_args,                    # تنظیمات آموزش\n",
    "    train_dataset=train_dataset,           # داده‌های آموزشی\n",
    "    eval_dataset=test_dataset,             # داده‌های ارزیابی\n",
    "    compute_metrics=compute_metrics        # محاسبه متریک‌ها\n",
    ")\n",
    "\n",
    "# شروع آموزش مدل\n",
    "trainer.train()\n",
    "\n",
    "# ارزیابی مدل پس از آموزش\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(results)\n",
    "\n",
    "# استخراج ویژگی‌ها و برچسب‌ها برای t-SNE\n",
    "model.eval()\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "# استخراج ویژگی‌ها از مدل\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = model(pixel_values=inputs)  # ورودی‌های پیش پردازش شده به مدل می‌دهیم\n",
    "        features = outputs.logits.cpu().numpy()\n",
    "        train_features.extend(features)\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# استفاده از t-SNE برای کاهش ابعاد\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(train_features)\n",
    "\n",
    "# رسم نتیجه t-SNE\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=train_labels, cmap='viridis', s=10)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"t-SNE Visualization of ViT Features\")\n",
    "plt.show()\n",
    "\n",
    "# ذخیره مدل آموزش‌دیده\n",
    "trainer.save_model(\"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/vit_finetuned_model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import ViTForImageClassification, ViTFeatureExtractor, Trainer, TrainingArguments\n",
    "# from datasets import load_dataset\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Load CIFAR-10 dataset from Hugging Face\n",
    "# dataset = load_dataset(\"cifar10\")\n",
    "\n",
    "# # Split the dataset into train and validation sets\n",
    "# train_dataset = dataset['train']\n",
    "# val_dataset = dataset['test']\n",
    "\n",
    "# # Load pre-trained ViT feature extractor\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# # Preprocessing function for ViT\n",
    "# def preprocess_function(examples):\n",
    "#     return feature_extractor([x['img'] for x in examples], return_tensors=\"pt\")\n",
    "\n",
    "# # Preprocess the dataset\n",
    "# train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "# val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# # Set format for PyTorch\n",
    "# train_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "# val_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "\n",
    "# # Load pre-trained ViT model for image classification\n",
    "# model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=10)\n",
    "\n",
    "# # Define the training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./vit_finetuned\",         # Where to store the final model\n",
    "#     evaluation_strategy=\"epoch\",          # Evaluate at the end of each epoch\n",
    "#     learning_rate=2e-5,                   # Learning rate for fine-tuning\n",
    "#     per_device_train_batch_size=16,       # Batch size for training\n",
    "#     per_device_eval_batch_size=64,        # Batch size for evaluation\n",
    "#     num_train_epochs=3,                   # Number of training epochs\n",
    "#     weight_decay=0.01,                    # Weight decay for regularization\n",
    "#     save_total_limit=2,                   # Save only the last 2 checkpoints\n",
    "#     logging_dir='./logs',                 # Directory for logs\n",
    "# )\n",
    "\n",
    "# # Define the metrics for evaluation\n",
    "# def compute_metrics(p):\n",
    "#     preds = np.argmax(p.predictions, axis=1)\n",
    "#     labels = p.label_ids\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     f1 = f1_score(labels, preds, average='weighted')\n",
    "#     precision = precision_score(labels, preds, average='weighted')\n",
    "#     recall = recall_score(labels, preds, average='weighted')\n",
    "\n",
    "#     return {\n",
    "#         \"accuracy\": acc,\n",
    "#         \"f1\": f1,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall\n",
    "#     }\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,                           # The model to be fine-tuned\n",
    "#     args=training_args,                    # Training arguments\n",
    "#     train_dataset=train_dataset,           # Training dataset\n",
    "#     eval_dataset=val_dataset,              # Validation dataset\n",
    "#     compute_metrics=compute_metrics        # Evaluation metrics\n",
    "# )\n",
    "\n",
    "# # Start fine-tuning\n",
    "# trainer.train()\n",
    "\n",
    "# # Evaluate the model after fine-tuning\n",
    "# results = trainer.evaluate()\n",
    "# print(\"Evaluation Results:\")\n",
    "# print(results)\n",
    "\n",
    "# # Get features and labels for t-SNE\n",
    "# model.eval()\n",
    "# train_features = []\n",
    "# train_labels = []\n",
    "\n",
    "# # Extract features from the model\n",
    "# with torch.no_grad():\n",
    "#     for batch in DataLoader(train_dataset, batch_size=32):\n",
    "#         inputs = batch['input_ids']\n",
    "#         labels = batch['label']\n",
    "#         outputs = model(**inputs)\n",
    "#         features = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "#         train_features.extend(features)\n",
    "#         train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# train_features = np.array(train_features)\n",
    "# train_labels = np.array(train_labels)\n",
    "\n",
    "# # Use t-SNE for dimensionality reduction\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# tsne_results = tsne.fit_transform(train_features)\n",
    "\n",
    "# # Plot the t-SNE result\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=train_labels, cmap='viridis', s=10)\n",
    "# plt.colorbar(scatter)\n",
    "# plt.title(\"t-SNE Visualization of ViT Features\")\n",
    "# plt.show()\n",
    "\n",
    "# # Save the fine-tuned model\n",
    "# trainer.save_model(\"./vit_finetuned_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc8384cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers[torch]\n",
    "# !pip install --upgrade torch\n",
    "#!pip show tokenizers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5320362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers==0.19.1\n",
      "  Using cached tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tokenizers==0.19.1) (0.24.6)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.67.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.3)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "Successfully installed tokenizers-0.19.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.46.3 requires tokenizers<0.21,>=0.20, but you have tokenizers 0.19.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tokenizers\n",
    "!pip install tokenizers==0.19.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f8af1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Using cached tokenizers-0.20.3-cp39-none-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed tokenizers-0.20.3\n",
      "Name: tokenizers\n",
      "Version: 0.20.3\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: Anthony MOI <m.anthony.moi@gmail.com>\n",
      "Author-email: Nicolas Patry <patry.nicolas@protonmail.com>, Anthony Moi <anthony@huggingface.co>\n",
      "License: \n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: huggingface-hub\n",
      "Required-by: transformers\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip show tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee109ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1884749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60c6cf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (4.28.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (1.24.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (0.24.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.28.0) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\tokenizers-0.13.3.dist-info\\\\METADATA'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94056a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbd317fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers==0.19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15cd70ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizers\n",
      "Version: 0.19.1\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: Anthony MOI <m.anthony.moi@gmail.com>\n",
      "Author-email: Nicolas Patry <patry.nicolas@protonmail.com>, Anthony Moi <anthony@huggingface.co>\n",
      "License: \n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: huggingface-hub\n",
      "Required-by: transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c2a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (4.28.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.28.0) (0.24.6)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2022.7.9)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.28.0) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mey\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers==4.28.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (3.3)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed tokenizers-0.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a0c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers==0.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae85211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenizers\n",
      "Version: 0.13.3\n",
      "Summary: Fast and Customizable Tokenizers\n",
      "Home-page: https://github.com/huggingface/tokenizers\n",
      "Author: Anthony MOI\n",
      "Author-email: anthony@huggingface.co\n",
      "License: Apache License 2.0\n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae06217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
