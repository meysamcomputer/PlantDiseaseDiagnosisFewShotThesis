{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7d0ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mey\\AppData\\Local\\Temp\\ipykernel_6120\\848687279.py:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  siamese_model = torch.load(\"C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/siamese_model_EmbeddingGEnerator.h5\",map_location=torch.device('cuda'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6120\\848687279.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m \u001b[0msiamese_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/siamese_model_EmbeddingGEnerator.h5\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[0msiamese_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;31m# Generate embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1358\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1360\u001b[1;33m                 return _load(\n\u001b[0m\u001b[0;32m   1361\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1810\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m             typed_storage = load_tensor(\n\u001b[0m\u001b[0;32m   1813\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1782\u001b[0m         \u001b[1;31m# stop wrapping with TypedStorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[1;32m-> 1784\u001b[1;33m             \u001b[0mwrap_storage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1785\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m             \u001b[0m_internal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \"\"\"\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[0mbackend_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_privateuse1_backend_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[0mdevice_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"is_available\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdevice_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m         raise RuntimeError(\n\u001b[0m\u001b[0;32m    509\u001b[0m             \u001b[1;34mf\"Attempting to deserialize object on a {backend_name.upper()} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m             \u001b[1;34mf\"device but torch.{backend_name}.is_available() is False. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dataloaders import get_train_transforms, get_val_transforms, get_triplet_dataloader\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score , precision_score , recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "device = torch.cuda.is_available()\n",
    "\n",
    "# def generate_embeddings(data_loader, model):\n",
    "#     with torch.no_grad():\n",
    "#         #device = 'cuda'\n",
    "#         model.eval()\n",
    "#         #model.to(device)\n",
    "#         embeddings = []\n",
    "#         labels = []\n",
    "#         for batch_imgs, batch_labels in data_loader:\n",
    "#             if device:\n",
    "#                 batch_imgs = batch_imgs.cuda()\n",
    "#             batch_E = model.get_embedding(batch_imgs)\n",
    "#             embeddings.append(batch_E.cpu().numpy())\n",
    "#             labels.append(batch_labels.numpy())\n",
    "#     return np.concatenate(embeddings), np.concatenate(labels)\n",
    "\n",
    "##############################\n",
    "\n",
    "class PatchExtractor(nn.Module):\n",
    "    def __init__(self, patch_size=14, target_patch_size=224):\n",
    "        super(PatchExtractor, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.target_patch_size = target_patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        تصویر ورودی را به پچ‌های 14x14 تقسیم می‌کند و هر پچ را به اندازه 224x224 تغییر می‌دهد.\n",
    "        ورودی: x با ابعاد (batch_size, channels, height, width)\n",
    "        خروجی: پچ‌ها با ابعاد (batch_size, num_patches_h, num_patches_w, channels, target_patch_size, target_patch_size)\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        # اطمینان از اینکه ابعاد تصویر بر patch_size بخش‌پذیر است\n",
    "        assert height % self.patch_size == 0 and width % self.patch_size == 0, \\\n",
    "            \"Image dimensions must be divisible by patch_size\"\n",
    "        \n",
    "        # تعداد پچ‌ها در هر بعد\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        \n",
    "        # استفاده از unfold برای تقسیم تصویر به پچ‌ها\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        # patches shape: (batch_size, channels, num_patches_h, num_patches_w, patch_size, patch_size)\n",
    "        \n",
    "        # تغییر شکل پچ‌ها به (batch_size, num_patches_h, num_patches_w, channels, patch_size, patch_size)\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        \n",
    "        # تغییر اندازه هر پچ به 224x224\n",
    "        resized_patches = F.interpolate(\n",
    "            patches.view(-1, channels, self.patch_size, self.patch_size),  # (batch_size * num_patches_h * num_patches_w, channels, patch_size, patch_size)\n",
    "            size=(self.target_patch_size, self.target_patch_size),  # اندازه هدف\n",
    "            mode='bilinear',  # روش درونیابی\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # تغییر شکل به (batch_size, num_patches_h, num_patches_w, channels, target_patch_size, target_patch_size)\n",
    "        resized_patches = resized_patches.view(batch_size, num_patches_h, num_patches_w, channels, self.target_patch_size, self.target_patch_size)\n",
    "        \n",
    "        return resized_patches\n",
    "\n",
    "def generate_embeddings(data_loader, patch_extractor, model):\n",
    "    \"\"\"\n",
    "    تصاویر را به پچ های 14x14 تقسیم می کند و هر پچ را به امبدینگ تبدیل می کند.\n",
    "    سپس امبدینگ ها را در یک ماتریس به همان ترتیب مکانی قرار می دهد.\n",
    "    \"\"\"\n",
    "    patch_extractor.eval()\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_imgs, batch_labels in data_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch_imgs = batch_imgs.cuda()\n",
    "            # استخراج پچ ها\n",
    "            patches = patch_extractor(batch_imgs)  # (batch_size, num_patches_h, num_patches_w, channels, patch_size, patch_size)\n",
    "            batch_size, num_patches_h, num_patches_w, channels, _, _ = patches.shape\n",
    "\n",
    "            # تبدیل هر پچ به امبدینگ\n",
    "            patch_embeddings = []\n",
    "            for i in range(batch_size):\n",
    "                single_image_patches = patches[i]  # (num_patches_h, num_patches_w, channels, patch_size, patch_size)\n",
    "                single_image_embeddings = []\n",
    "                for h in range(num_patches_h):\n",
    "                    for w in range(num_patches_w):\n",
    "                        # هر پچ را به model.get_embedding می دهیم\n",
    "                        patch = single_image_patches[h, w]  # (channels, patch_size, patch_size)\n",
    "                        patch = patch.unsqueeze(0)  # (1, channels, patch_size, patch_size)\n",
    "                        embedding = model.get_embedding(patch)  # (1, embedding_dim)\n",
    "                        single_image_embeddings.append(embedding.squeeze(0))  # (embedding_dim)\n",
    "                \n",
    "                # تبدیل لیست امبدینگ ها به یک ماتریس\n",
    "                single_image_embeddings = torch.stack(single_image_embeddings, dim=0)  # (num_patches_h * num_patches_w, embedding_dim)\n",
    "                single_image_embeddings = single_image_embeddings.view(num_patches_h, num_patches_w, -1)  # (num_patches_h, num_patches_w, embedding_dim)\n",
    "                patch_embeddings.append(single_image_embeddings)\n",
    "            \n",
    "            # جمع آوری امبدینگ ها برای کل بچ\n",
    "            patch_embeddings = torch.stack(patch_embeddings, dim=0)  # (batch_size, num_patches_h, num_patches_w, embedding_dim)\n",
    "            all_embeddings.append(patch_embeddings.cpu())\n",
    "            all_labels.append(batch_labels)\n",
    "\n",
    "    # Concatenate all embeddings and labels\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)  # (total_images, num_patches_h, num_patches_w, embedding_dim)\n",
    "    all_labels = torch.cat(all_labels, dim=0)  # (total_images)\n",
    "    return all_embeddings, all_labels\n",
    "\n",
    "# Initialize models\n",
    "# if device:\n",
    "#patch_extractor = PatchExtractor(patch_size=14).cuda()\n",
    "patch_extractor = PatchExtractor(patch_size=14)   \n",
    "# Load your data\n",
    "train_data = torchvision.datasets.ImageFolder(root=path_data + '/train/', transform=get_val_transforms())\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "  \n",
    "siamese_model = torch.load(\"C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/siamese_model_EmbeddingGEnerator.h5\",map_location=torch.device('cuda'))\n",
    "siamese_model.eval()\n",
    "# Generate embeddings\n",
    "train_embeddings, train_labels = generate_embeddings(train_loader, patch_extractor, siamese_model)\n",
    "\n",
    "# Now train_embeddings is a tensor of shape (total_images, num_patches_h, num_patches_w, embedding_dim)\n",
    "\n",
    " \n",
    "print(\"siamese_model\")\n",
    "   \n",
    "''' \n",
    "class RefinedViT(nn.Module):\n",
    "    def __init__(self, original_vit_model,embedding_dim,  num_classes):\n",
    "        super(RefinedViT, self).__init__()\n",
    "        # حفظ لایه های اصلی ViT\n",
    "        self.num_patches = (224 // 16) * (224 // 16)  # برای تصویر 224x224 با پچ 16x16\n",
    "        self.embed_dim = 768  # سایز استاندارد برای vit-base\n",
    "        \n",
    "        # تبدیل embedding به پچ ها\n",
    "        self.embedding_to_patch = nn.Linear(1792, self.num_patches * self.embed_dim)\n",
    "       # self.embedding_to_patch = nn.Linear(1280, self.num_patches * self.embed_dim)\n",
    "        #self.embedding_to_patch = nn.Linear(embedding_dim,  self.num_patches * self.embed_dim)\n",
    "        \n",
    "        # Position Embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.embed_dim))\n",
    "        \n",
    "        # CLS Token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=12,  # تعداد heads در self-attention\n",
    "            dim_feedforward=3072,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=12)\n",
    "        \n",
    "        # لایه های نهایی\n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc = nn.Linear(self.embed_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # تبدیل embedding به پچ ها\n",
    "        x = self.embedding_to_patch(x)  # (batch_size, num_patches * embed_dim)\n",
    "        x = x.view(batch_size, self.num_patches, self.embed_dim)  # (batch_size, num_patches, embed_dim)\n",
    "       \n",
    "        # اضافه کردن CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (batch_size, num_patches + 1, embed_dim)\n",
    "        \n",
    "        # اضافه کردن position embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # استفاده از CLS token برای طبقه بندی\n",
    "        x = x[:, 0]  # فقط CLS token\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x  \n",
    "    def get_embedding(self, x):\n",
    "        \"\"\"\n",
    "        دریافت embedding برای استفاده در t-SNE\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # تغییر شکل ورودی\n",
    "        if len(x.shape) == 4:\n",
    "            x = x.view(batch_size, -1)  # تبدیل به بردار یک بعدی\n",
    "        \n",
    "        # تغییر ابعاد برای تطابق با عملیات ضرب ماتریسی\n",
    "        x = x.reshape(batch_size, 1792, -1)  # تقسیم 150528 به 1792 و باقیمانده\n",
    "        x = x.mean(dim=-1)  # میانگین گیری روی بعد آخر\n",
    "        \n",
    "        # حالا x دارای ابعاد (batch_size, 1792) است\n",
    "        x = self.embedding_to_patch(x)  # عملیات خطی\n",
    "        x = x.view(batch_size, self.num_patches, self.embed_dim)\n",
    "        \n",
    "        # اضافه کردن CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # اضافه کردن position embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # استفاده از CLS token\n",
    "        x = x[:, 0]\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "'''\n",
    " \n",
    "\n",
    "class RefinedViT(nn.Module):\n",
    "    def __init__(self, original_vit_model,embedding_dim,  num_classes):\n",
    "        super(RefinedViT, self).__init__()    \n",
    "        # تعداد پچ‌ها برای تصویر 224x224 با پچ 14x14\n",
    "        self.num_patches = (224 // 14) * (224 // 14)  # 256 پچ\n",
    "        self.embed_dim = embedding_dim  # ابعاد امبدینگ هر پچ\n",
    "        \n",
    "        # Position Embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.embed_dim))\n",
    "        \n",
    "        # CLS Token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=12,  # تعداد heads در self-attention\n",
    "            dim_feedforward=3072,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=12)\n",
    "        \n",
    "        # لایه‌های نهایی\n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc = nn.Linear(self.embed_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        ورودی: x با ابعاد (batch_size, num_patches_h, num_patches_w, embed_dim)\n",
    "        \"\"\"\n",
    "        batch_size, num_patches_h, num_patches_w, embed_dim = x.shape\n",
    "        \n",
    "        # تغییر شکل x به (batch_size, num_patches, embed_dim)\n",
    "        x = x.view(batch_size, -1, embed_dim)  # (batch_size, num_patches_h * num_patches_w, embed_dim)\n",
    "        \n",
    "        # اضافه کردن CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (batch_size, num_patches + 1, embed_dim)\n",
    "        \n",
    "        # اضافه کردن position embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # استفاده از CLS token برای طبقه‌بندی\n",
    "        x = x[:, 0]  # فقط CLS token\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=path_data + '/train/', transform=get_val_transforms())\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(root=path_data + '/test/', transform=get_val_transforms())\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# بارگذاری مدل ViT از Hugging Face\n",
    "vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model.classifier = torch.nn.Linear(vit_model.config.hidden_size, 15)  # تعداد کلاس ها\n",
    "\n",
    "embedding_dim = vit_model.config.hidden_size  # ابعاد embedding\n",
    "#model = RefinedViT(vit_model, embedding_dim, num_classes=15)\n",
    "model = RefinedViT(vit_model,embedding_dim , num_classes=15).cuda()\n",
    "print(\"model\")\n",
    "\n",
    "\n",
    "outputs = model(train_embeddings.to(device))\n",
    "\n",
    "# استخراج ویژگی ها از داده های آموزش\n",
    "train_embeddings, train_labels = generate_embeddings(train_loader, siamese_model )\n",
    "# استخراج ویژگی ها از داده های تست\n",
    "test_embeddings, test_labels = generate_embeddings(test_loader, siamese_model )\n",
    " \n",
    "# تبدیل داده ها به تنسور\n",
    "X_train, y_train = train_embeddings, train_labels\n",
    "X_val, y_val = test_embeddings, test_labels\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# انتقال مدل به دستگاه CUDA\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# تنظیمات loss function و optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)  # استفاده از AdamW\n",
    "# انتقال به GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# تنظیمات آموزش\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "print(\"train\")\n",
    "# آموزش\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor.to(device))\n",
    "    loss = criterion(outputs, y_train_tensor.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    " \n",
    "# ارزیابی مدل\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_val_tensor.to(device))\n",
    "    _, predicted = torch.max(val_outputs.data, 1)\n",
    " \n",
    "\n",
    "# محاسبه دقت و امتیاز F1\n",
    "accuracy = accuracy_score(y_val_tensor.cpu(), predicted.cpu())\n",
    "f1 = f1_score(y_val_tensor.cpu(), predicted.cpu(), average='weighted')\n",
    "precision = precision_score(y_val_tensor.cpu(), predicted.cpu(), average='macro') \n",
    "recall = recall_score(y_val_tensor.cpu(), predicted.cpu(), average='macro')\n",
    "\n",
    "# نمایش نتایج\n",
    "print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "print(f'Precision Score: {precision}')\n",
    "print(f'Recall Score: {recall}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13940e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
