{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from transformers import ViTModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    " \n",
    "import torchvision\n",
    "from dataloaders import get_train_transforms, get_val_transforms, get_triplet_dataloader\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        return attn_output.transpose(0, 1)\n",
    "\n",
    "class InnovativeFewShotViT(nn.Module):\n",
    "    def __init__(self, n_way=15, k_shot=15, embed_dim=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.embedding = nn.Linear(768, embed_dim)\n",
    "        self.support_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.query_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.margin = 1.0\n",
    "\n",
    "     \n",
    "    def forward_one(self, x):\n",
    "        # تنظیم ابعاد برای ViT\n",
    "        if len(x.shape) > 4:  # [B, N, C, H, W] یا بیشتر\n",
    "            x = x.view(-1, x.size(-3), x.size(-2), x.size(-1))  # [B*N, C, H, W]\n",
    "        elif len(x.shape) == 3:  # [C, H, W]\n",
    "            x = x.unsqueeze(0)  # [1, C, H, W]\n",
    "\n",
    "        # تبدیل به شکل استاندارد\n",
    "        if x.size(1) != 3:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # اطمینان از ابعاد درست\n",
    "        assert len(x.shape) == 4 and x.size(1) == 3, f\"نادرست shape ورودی: {x.shape}\"\n",
    "\n",
    "        x = self.vit(x).last_hidden_state[:, 0]\n",
    "        embedding = self.embedding(x)\n",
    "        return F.normalize(embedding, p=2, dim=1)    \n",
    "    \n",
    "     \n",
    "\n",
    "    def get_prototypes(self, support_embeddings):\n",
    "        batch_size = support_embeddings.size(0)\n",
    "        support_embeddings = support_embeddings.reshape(batch_size, self.n_way * self.k_shot, -1)\n",
    "        attended_support = self.support_attention(support_embeddings)\n",
    "        prototypes = attended_support.reshape(batch_size, self.n_way, self.k_shot, -1).mean(2)\n",
    "        return prototypes\n",
    "\n",
    "    def forward(self, support_set, query, mode='train'):\n",
    "        batch_size = support_set.size(0)\n",
    "\n",
    "        # Process support set\n",
    "        support_embeddings = torch.zeros(batch_size, self.n_way, self.k_shot, 512, device=support_set.device)\n",
    "        for i in range(self.n_way):\n",
    "            for j in range(self.k_shot):\n",
    "                emb = self.forward_one(support_set[:, i, j])\n",
    "                support_embeddings[:, i, j] = emb\n",
    "\n",
    "        prototypes = self.get_prototypes(support_embeddings)\n",
    "\n",
    "        # Process query\n",
    "        query_emb = []\n",
    "        for q in query:\n",
    "            emb = self.forward_one(q.unsqueeze(0))\n",
    "            query_emb.append(emb)\n",
    "        query_emb = torch.cat(query_emb)\n",
    "\n",
    "        # Match dimensions for loss calculation\n",
    "        logits = -torch.cdist(query_emb, prototypes[0])\n",
    "\n",
    "        return logits, torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, triplet_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.triplet_weight = triplet_weight\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels, triplet_loss):\n",
    "        ce_loss = self.ce(logits, labels)\n",
    "        return ce_loss + self.triplet_weight * triplet_loss\n",
    "\n",
    "class FewShotDataset:\n",
    "    def __init__(self, root_dir, transform=None, n_way=15, k_shot=15, n_query=5, n_episodes=1000):\n",
    "        self.dataset = datasets.ImageFolder(root_dir, transform=transform)\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "        \n",
    "        self.label_to_indices = {}\n",
    "        for idx, (_, label) in enumerate(self.dataset):\n",
    "            if label not in self.label_to_indices:\n",
    "                self.label_to_indices[label] = []\n",
    "            self.label_to_indices[label].append(idx)\n",
    "        \n",
    "        self.valid_classes = [\n",
    "            cls for cls, indices in self.label_to_indices.items()\n",
    "            if len(indices) >= self.k_shot + self.n_query\n",
    "        ]\n",
    "        \n",
    "        if len(self.valid_classes) < self.n_way:\n",
    "            raise ValueError(f\"Not enough classes with sufficient samples. Found {len(self.valid_classes)} valid classes, need {self.n_way}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "    \n",
    "    def __getitem__(self, episode_index):\n",
    "        selected_classes = np.random.choice(self.valid_classes, self.n_way, replace=False)\n",
    "\n",
    "        support_images = torch.zeros(self.n_way, self.k_shot, 3, 224, 224)\n",
    "        query_images = []\n",
    "        query_labels = []\n",
    "\n",
    "        for class_idx, class_label in enumerate(selected_classes):\n",
    "            class_indices = self.label_to_indices[class_label]\n",
    "            selected_indices = np.random.choice(\n",
    "                class_indices, \n",
    "                self.k_shot + self.n_query, \n",
    "                replace=False\n",
    "            )\n",
    "\n",
    "            # Support set\n",
    "            for shot_idx, img_idx in enumerate(selected_indices[:self.k_shot]):\n",
    "                img, _ = self.dataset[img_idx]\n",
    "                support_images[class_idx, shot_idx] = img\n",
    "\n",
    "            # Query set\n",
    "            for img_idx in selected_indices[self.k_shot:self.k_shot + self.n_query]:\n",
    "                img, _ = self.dataset[img_idx]\n",
    "                query_images.append(img)\n",
    "                query_labels.append(class_idx)\n",
    "\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "\n",
    "        return support_images, query_images, query_labels\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, criterion, optimizer, device, num_epochs=50):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        self.best_val_f1 = 0\n",
    "        self.train_metrics = []\n",
    "        self.val_metrics = []\n",
    "    \n",
    "    def calculate_metrics(self, labels, preds, loss=None, n_batches=None):\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average='weighted', zero_division=0\n",
    "        )\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "        }\n",
    "        if loss is not None:\n",
    "            metrics['loss'] = loss / n_batches\n",
    "        return metrics\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for support_images, query_images, query_labels in tqdm(self.train_loader):\n",
    "            support_images = support_images.to(self.device)\n",
    "            query_images = query_images.to(self.device)\n",
    "            query_labels = query_labels.to(self.device).view(-1)  # Flatten labels\n",
    "\n",
    "            logits, triplet_loss = self.model(support_images, query_images, mode='train')\n",
    "            loss = self.criterion(logits, query_labels, triplet_loss)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(query_labels.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return self.calculate_metrics(all_labels, all_preds, total_loss, len(self.train_loader))\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for support_images, query_images, query_labels in tqdm(self.val_loader):\n",
    "                support_images = support_images.to(self.device)\n",
    "                query_images = query_images.to(self.device)\n",
    "                query_labels = query_labels.to(self.device)\n",
    "                \n",
    "                logits = self.model(support_images, query_images, mode='test')\n",
    "                loss = self.criterion(logits, query_labels, torch.tensor(0.))\n",
    "                \n",
    "                preds = logits.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(query_labels.cpu().numpy())\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return self.calculate_metrics(all_labels, all_preds, total_loss, len(self.val_loader))\n",
    "    \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for support_images, query_images, query_labels in tqdm(self.test_loader):\n",
    "                support_images = support_images.to(self.device)\n",
    "                query_images = query_images.to(self.device)\n",
    "                query_labels = query_labels.to(self.device)\n",
    "                \n",
    "                logits = self.model(support_images, query_images, mode='test')\n",
    "                preds = logits.argmax(dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(query_labels.cpu().numpy())\n",
    "        \n",
    "        metrics = self.calculate_metrics(all_labels, all_preds)\n",
    "        self.plot_confusion_matrix(all_labels, all_preds, \"Test Set Confusion Matrix\")\n",
    "        return metrics\n",
    "    \n",
    "    def plot_confusion_matrix(self, labels, preds, title):\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(title)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.train_metrics) + 1)\n",
    "        metrics = ['accuracy', 'f1', 'loss']\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            train_values = [m[metric] for m in self.train_metrics]\n",
    "            val_values = [m[metric] for m in self.val_metrics]\n",
    "            \n",
    "            plt.plot(epochs, train_values, 'b-', label='Train')\n",
    "            plt.plot(epochs, val_values, 'r-', label='Validation')\n",
    "            plt.title(f'{metric.capitalize()} vs Epochs')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
    "            \n",
    "            train_metrics = self.train_epoch()\n",
    "            val_metrics = self.validate()\n",
    "            \n",
    "            self.train_metrics.append(train_metrics)\n",
    "            self.val_metrics.append(val_metrics)\n",
    "            \n",
    "            print(f\"Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1']:.4f}\")\n",
    "            print(f\"Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "            \n",
    "            if val_metrics['f1'] > self.best_val_f1:\n",
    "                self.best_val_f1 = val_metrics['f1']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_f1': self.best_val_f1,\n",
    "                }, 'best_model.pth')\n",
    "        \n",
    "        self.plot_metrics()\n",
    "        \n",
    "        checkpoint = torch.load('best_model.pth')\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        test_metrics = self.test()\n",
    "        \n",
    "        print(\"\\nTest Results:\")\n",
    "        print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "        \n",
    "        return test_metrics\n",
    "\n",
    " \n",
    " \n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    n_way = 15\n",
    "    k_shot = 10\n",
    "    n_query = 5\n",
    "    batch_size = 1  # کاهش batch size\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "    \n",
    "    train_dataset = FewShotDataset(path_data+'/train/', transform, n_way, k_shot, n_query, n_episodes=100)\n",
    "    val_dataset = FewShotDataset(path_data+'/val/', transform, n_way, k_shot, n_query, n_episodes=50)\n",
    "    test_dataset = FewShotDataset(path_data+'/test/', transform, n_way, k_shot, n_query, n_episodes=50)\n",
    "    \n",
    "    # حذف num_workers و کاهش batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = InnovativeFewShotViT(n_way=n_way, k_shot=k_shot).to(device)\n",
    "    criterion = CombinedLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    test_metrics = trainer.train()\n",
    "    return test_metrics\n",
    "# if __name__ == \"__main__\":\n",
    "print(\"Starting training...\")\n",
    "test_metrics = main()\n",
    "print(\"\\nTraining completed!\")\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25758a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
