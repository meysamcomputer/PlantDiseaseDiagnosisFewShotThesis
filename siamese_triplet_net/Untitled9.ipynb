{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from transformers import ViTModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    " \n",
    "import torchvision\n",
    "from dataloaders import get_train_transforms, get_val_transforms, get_triplet_dataloader\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        return attn_output.transpose(0, 1)\n",
    "\n",
    "class InnovativeFewShotViT(nn.Module):\n",
    "   def __init__(self, n_way=15, k_shot=15, embed_dim=512, num_heads=8):\n",
    "       super().__init__()\n",
    "       self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "       self.embedding = nn.Linear(768, embed_dim)\n",
    "       self.support_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "       self.query_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "       self.n_way = n_way\n",
    "       self.k_shot = k_shot\n",
    "       self.margin = 1.0\n",
    "       \n",
    "       # Meta-learning layers\n",
    "       self.meta_encoder = nn.Sequential(\n",
    "           nn.Linear(embed_dim, embed_dim//2),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(embed_dim//2, embed_dim)\n",
    "       )\n",
    "       \n",
    "       # Prototypical Network layers\n",
    "       self.proto_encoder = nn.Sequential(\n",
    "           nn.Linear(embed_dim, embed_dim),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(embed_dim, embed_dim)\n",
    "       )\n",
    "       \n",
    "   def forward_one(self, x):\n",
    "       if len(x.shape) > 4:\n",
    "           x = x.view(-1, x.size(-3), x.size(-2), x.size(-1))\n",
    "       elif len(x.shape) == 3:\n",
    "           x = x.unsqueeze(0)\n",
    "           \n",
    "       if x.size(1) != 3:\n",
    "           x = x.permute(0, 3, 1, 2)\n",
    "           \n",
    "       # ViT feature extraction\n",
    "       x = self.vit(x).last_hidden_state[:, 0]\n",
    "       embedding = self.embedding(x)\n",
    "       \n",
    "       # Meta-learning enhancement\n",
    "       meta_emb = self.meta_encoder(embedding)\n",
    "       enhanced_emb = embedding + meta_emb\n",
    "       \n",
    "       return F.normalize(enhanced_emb, p=2, dim=1)\n",
    "\n",
    "   def get_prototypes(self, support_embeddings):\n",
    "       batch_size = support_embeddings.size(0)\n",
    "       \n",
    "       # Self-attention processing\n",
    "       support_embeddings = support_embeddings.reshape(batch_size, self.n_way * self.k_shot, -1)\n",
    "       attended_support = self.support_attention(support_embeddings)\n",
    "       \n",
    "       # Prototypical Network processing\n",
    "       prototypes = attended_support.reshape(batch_size, self.n_way, self.k_shot, -1)\n",
    "       prototypes = self.proto_encoder(prototypes.mean(2))\n",
    "       return prototypes\n",
    "\n",
    "   def forward(self, support_set, query, mode='train'):\n",
    "       batch_size = support_set.size(0)\n",
    "       \n",
    "       # Support set processing\n",
    "       support_embeddings = torch.zeros(batch_size, self.n_way, self.k_shot, 512, device=support_set.device)\n",
    "       triplet_loss = 0\n",
    "       \n",
    "       # Extract embeddings\n",
    "       for i in range(self.n_way):\n",
    "           class_embeddings = []\n",
    "           for j in range(self.k_shot):\n",
    "               emb = self.forward_one(support_set[:, i, j])\n",
    "               support_embeddings[:, i, j] = emb\n",
    "               \n",
    "               # Triplet loss computation\n",
    "               if j > 0 and mode == 'train':\n",
    "                   anchor = emb\n",
    "                   positive = class_embeddings[-1]\n",
    "                   neg_class = (i + 1) % self.n_way\n",
    "                   neg_emb = self.forward_one(support_set[:, neg_class, j])\n",
    "                   \n",
    "                   pos_dist = F.pairwise_distance(anchor, positive)\n",
    "                   neg_dist = F.pairwise_distance(anchor, neg_emb)\n",
    "                   triplet_loss += F.relu(pos_dist - neg_dist + self.margin).mean()\n",
    "                   \n",
    "               class_embeddings.append(emb)\n",
    "\n",
    "       # Get prototypes\n",
    "       prototypes = self.get_prototypes(support_embeddings)\n",
    "       \n",
    "       # Process queries\n",
    "       query_embeddings = []\n",
    "       for idx in range(query.size(0)):\n",
    "           emb = self.forward_one(query[idx:idx+1])\n",
    "           query_embeddings.append(emb)\n",
    "       query_emb = torch.cat(query_embeddings)\n",
    "       \n",
    "       # Compute similarity scores\n",
    "       logits = -torch.cdist(query_emb, prototypes[0])\n",
    "       \n",
    "       if mode == 'train':\n",
    "           return logits, triplet_loss\n",
    "       return logits, torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "class TripletMarginWithMetaLoss(nn.Module):\n",
    "   def __init__(self, margin=1.0, triplet_weight=0.5):\n",
    "       super().__init__()\n",
    "       self.margin = margin\n",
    "       self.triplet_weight = triplet_weight\n",
    "       self.ce = nn.CrossEntropyLoss()\n",
    "       \n",
    "   def forward(self, logits, labels, triplet_loss):\n",
    "       ce_loss = self.ce(logits, labels)\n",
    "       total_loss = ce_loss + self.triplet_weight * triplet_loss\n",
    "       return total_loss\n",
    "       \n",
    "class FewShotTrainer:\n",
    "   def __init__(self, model, train_loader, val_loader, test_loader, criterion, optimizer, \n",
    "                device, num_epochs=50, scheduler=None):\n",
    "       self.model = model\n",
    "       self.train_loader = train_loader\n",
    "       self.val_loader = val_loader\n",
    "       self.test_loader = test_loader\n",
    "       self.criterion = criterion\n",
    "       self.optimizer = optimizer\n",
    "       self.scheduler = scheduler\n",
    "       self.device = device\n",
    "       self.num_epochs = num_epochs\n",
    "       self.best_val_f1 = 0\n",
    "       self.train_metrics = []\n",
    "       self.val_metrics = []\n",
    "       \n",
    "   def train_epoch(self):\n",
    "       self.model.train()\n",
    "       metrics = defaultdict(float)\n",
    "       \n",
    "       for batch in tqdm(self.train_loader):\n",
    "           support_imgs, query_imgs, labels = [x.to(self.device) for x in batch]\n",
    "           labels = labels.view(-1)\n",
    "           \n",
    "           self.optimizer.zero_grad()\n",
    "           logits, triplet_loss = self.model(support_imgs, query_imgs)\n",
    "           loss = self.criterion(logits, labels, triplet_loss)\n",
    "           \n",
    "           loss.backward()\n",
    "           self.optimizer.step()\n",
    "           \n",
    "           preds = logits.argmax(dim=1)\n",
    "           metrics['loss'] += loss.item()\n",
    "           metrics['acc'] += (preds == labels).float().mean().item()\n",
    "           \n",
    "       return {k: v/len(self.train_loader) for k,v in metrics.items()}\n",
    "   \n",
    "   @torch.no_grad()\n",
    "   def validate(self, loader):\n",
    "       self.model.eval()\n",
    "       metrics = defaultdict(float)\n",
    "       \n",
    "       for batch in tqdm(loader):\n",
    "           support_imgs, query_imgs, labels = [x.to(self.device) for x in batch]\n",
    "           labels = labels.view(-1)\n",
    "           \n",
    "           logits, _ = self.model(support_imgs, query_imgs, mode='val')\n",
    "           loss = self.criterion(logits, labels, torch.tensor(0.0).to(self.device))\n",
    "           \n",
    "           preds = logits.argmax(dim=1)\n",
    "           metrics['loss'] += loss.item()\n",
    "           metrics['acc'] += (preds == labels).float().mean().item()\n",
    "           \n",
    "       return {k: v/len(loader) for k,v in metrics.items()}\n",
    "       \n",
    "   def train(self):\n",
    "       for epoch in range(self.num_epochs):\n",
    "           print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
    "           \n",
    "           train_metrics = self.train_epoch()\n",
    "           val_metrics = self.validate(self.val_loader)\n",
    "           \n",
    "           if self.scheduler:\n",
    "               self.scheduler.step(val_metrics['loss'])\n",
    "               \n",
    "           self.train_metrics.append(train_metrics)\n",
    "           self.val_metrics.append(val_metrics)\n",
    "           \n",
    "           print(f\"Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['acc']:.4f}\")\n",
    "           print(f\"Val - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['acc']:.4f}\")\n",
    "           \n",
    "           if val_metrics['acc'] > self.best_val_f1:\n",
    "               self.best_val_f1 = val_metrics['acc']\n",
    "               torch.save({\n",
    "                   'epoch': epoch,\n",
    "                   'model_state_dict': self.model.state_dict(),\n",
    "                   'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                   'best_acc': self.best_val_f1,\n",
    "               }, 'best_model.pth')\n",
    "               \n",
    "       # Test best model\n",
    "       checkpoint = torch.load('best_model.pth')\n",
    "       self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "       test_metrics = self.validate(self.test_loader)\n",
    "       \n",
    "       print(\"\\nTest Results:\")\n",
    "       print(f\"Loss: {test_metrics['loss']:.4f}\")\n",
    "       print(f\"Accuracy: {test_metrics['acc']:.4f}\")\n",
    "       \n",
    "       return test_metrics\n",
    "\n",
    "def main():\n",
    "   # Hyperparameters\n",
    "   n_way = 15\n",
    "   k_shot = 10 \n",
    "   n_query = 5\n",
    "   batch_size = 4\n",
    "   num_epochs = 50\n",
    "   learning_rate = 3e-4\n",
    "   \n",
    "   transform = transforms.Compose([\n",
    "       transforms.Resize((224, 224)),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "   ])\n",
    "   \n",
    "   # Data loading\n",
    "   path_data = 'path/to/dataset'\n",
    "   train_dataset = FewShotDataset(path_data+'/train/', transform, n_way, k_shot, n_query, n_episodes=200)\n",
    "   val_dataset = FewShotDataset(path_data+'/val/', transform, n_way, k_shot, n_query, n_episodes=100) \n",
    "   test_dataset = FewShotDataset(path_data+'/test/', transform, n_way, k_shot, n_query, n_episodes=100)\n",
    "\n",
    "   train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "   val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "   test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "   \n",
    "   # Model setup\n",
    "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "   model = InnovativeFewShotViT(n_way=n_way, k_shot=k_shot).to(device)\n",
    "   criterion = TripletMarginWithMetaLoss()\n",
    "   optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "   \n",
    "   trainer = FewShotTrainer(\n",
    "       model=model,\n",
    "       train_loader=train_loader, \n",
    "       val_loader=val_loader,\n",
    "       test_loader=test_loader,\n",
    "       criterion=criterion,\n",
    "       optimizer=optimizer,\n",
    "       scheduler=scheduler,\n",
    "       device=device,\n",
    "       num_epochs=num_epochs\n",
    "   )\n",
    "   \n",
    "   return trainer.train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   test_metrics = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
