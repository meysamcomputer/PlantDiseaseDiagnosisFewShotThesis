{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66499e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n",
      "Epoch 0: [0/225] Loss: 0.842478\n",
      "Epoch 0: Validation Loss: 0.928688\n",
      "Epoch 1: [0/225] Loss: 0.998621\n",
      "Epoch 1: Validation Loss: 0.929748\n",
      "Epoch 2: [0/225] Loss: 1.019156\n",
      "Epoch 2: Validation Loss: 0.922473\n",
      "Epoch 3: [0/225] Loss: 0.990100\n",
      "Epoch 3: Validation Loss: 0.927282\n",
      "Epoch 4: [0/225] Loss: 0.846351\n",
      "Epoch 4: Validation Loss: 0.922718\n",
      "Epoch 5: [0/225] Loss: 0.890962\n",
      "Epoch 5: Validation Loss: 0.910829\n",
      "Epoch 6: [0/225] Loss: 0.899605\n",
      "Epoch 6: Validation Loss: 0.909570\n",
      "Epoch 7: [0/225] Loss: 0.859286\n",
      "Epoch 7: Validation Loss: 0.910903\n",
      "Epoch 8: [0/225] Loss: 0.922489\n",
      "Epoch 8: Validation Loss: 0.902706\n",
      "Epoch 9: [0/225] Loss: 0.900027\n",
      "Epoch 9: Validation Loss: 0.904689\n",
      "Epoch 10: [0/225] Loss: 0.905161\n",
      "Epoch 10: Validation Loss: 0.902101\n",
      "Epoch 11: [0/225] Loss: 0.946863\n",
      "Epoch 11: Validation Loss: 0.895931\n",
      "Epoch 12: [0/225] Loss: 1.061231\n",
      "Epoch 12: Validation Loss: 0.900739\n",
      "Epoch 13: [0/225] Loss: 0.892946\n",
      "Epoch 13: Validation Loss: 0.900799\n",
      "Epoch 14: [0/225] Loss: 0.858709\n",
      "Epoch 14: Validation Loss: 0.908194\n",
      "Epoch 15: [0/225] Loss: 0.898273\n",
      "Epoch 15: Validation Loss: 0.896146\n",
      "Epoch 16: [0/225] Loss: 0.783776\n",
      "Epoch 16: Validation Loss: 0.905579\n",
      "Epoch 17: [0/225] Loss: 0.940789\n",
      "Epoch 17: Validation Loss: 0.896117\n",
      "Epoch 18: [0/225] Loss: 0.975586\n",
      "Epoch 18: Validation Loss: 0.895984\n",
      "Epoch 19: [0/225] Loss: 0.823480\n",
      "Epoch 19: Validation Loss: 0.904732\n",
      "Epoch 20: [0/225] Loss: 1.106612\n",
      "Epoch 20: Validation Loss: 0.891590\n",
      "Epoch 21: [0/225] Loss: 0.901455\n",
      "Epoch 21: Validation Loss: 0.907310\n",
      "Epoch 22: [0/225] Loss: 0.943849\n",
      "Epoch 22: Validation Loss: 0.899084\n",
      "Epoch 23: [0/225] Loss: 0.979676\n",
      "Epoch 23: Validation Loss: 0.900522\n",
      "Epoch 24: [0/225] Loss: 1.064023\n",
      "Epoch 24: Validation Loss: 0.882965\n",
      "Epoch 25: [0/225] Loss: 0.870644\n",
      "Epoch 25: Validation Loss: 0.885808\n",
      "Epoch 26: [0/225] Loss: 0.913255\n",
      "Epoch 26: Validation Loss: 0.863454\n",
      "Epoch 27: [0/225] Loss: 0.888971\n",
      "Epoch 27: Validation Loss: 0.870572\n",
      "Epoch 28: [0/225] Loss: 1.014754\n",
      "Epoch 28: Validation Loss: 0.879288\n",
      "Epoch 29: [0/225] Loss: 0.658175\n",
      "Epoch 29: Validation Loss: 0.861615\n",
      "Epoch 30: [0/225] Loss: 0.828798\n",
      "Epoch 30: Validation Loss: 0.878075\n",
      "Epoch 31: [0/225] Loss: 0.669762\n",
      "Epoch 31: Validation Loss: 0.872578\n",
      "Epoch 32: [0/225] Loss: 1.029996\n",
      "Epoch 32: Validation Loss: 0.867044\n",
      "Epoch 33: [0/225] Loss: 0.674605\n",
      "Epoch 33: Validation Loss: 0.871825\n",
      "Epoch 34: [0/225] Loss: 0.767037\n",
      "Epoch 34: Validation Loss: 0.856873\n",
      "Epoch 35: [0/225] Loss: 0.857921\n",
      "Epoch 35: Validation Loss: 0.890999\n",
      "Epoch 36: [0/225] Loss: 1.080703\n",
      "Epoch 36: Validation Loss: 0.892997\n",
      "Epoch 37: [0/225] Loss: 0.750105\n",
      "Epoch 37: Validation Loss: 0.891840\n",
      "Epoch 38: [0/225] Loss: 0.945107\n",
      "Epoch 38: Validation Loss: 0.887010\n",
      "Epoch 39: [0/225] Loss: 0.852818\n",
      "Epoch 39: Validation Loss: 0.882651\n",
      "Epoch 40: [0/225] Loss: 0.889304\n",
      "Epoch 40: Validation Loss: 0.886918\n",
      "Epoch 41: [0/225] Loss: 0.833773\n",
      "Epoch 41: Validation Loss: 0.881162\n",
      "Epoch 42: [0/225] Loss: 0.775477\n",
      "Epoch 42: Validation Loss: 0.890125\n",
      "Epoch 43: [0/225] Loss: 0.987815\n",
      "Epoch 43: Validation Loss: 0.879128\n",
      "Epoch 44: [0/225] Loss: 0.765234\n",
      "Epoch 44: Validation Loss: 0.857292\n",
      "Epoch 45: [0/225] Loss: 1.198446\n",
      "Epoch 45: Validation Loss: 0.864172\n",
      "Epoch 46: [0/225] Loss: 0.811281\n",
      "Epoch 46: Validation Loss: 0.868518\n",
      "Epoch 47: [0/225] Loss: 1.003664\n",
      "Epoch 47: Validation Loss: 0.877595\n",
      "Epoch 48: [0/225] Loss: 0.851770\n",
      "Epoch 48: Validation Loss: 0.863284\n",
      "Epoch 49: [0/225] Loss: 0.850454\n",
      "Epoch 49: Validation Loss: 0.864865\n",
      "Epoch 50: [0/225] Loss: 0.922414\n",
      "Epoch 50: Validation Loss: 0.853279\n",
      "Epoch 51: [0/225] Loss: 0.940064\n",
      "Epoch 51: Validation Loss: 0.848980\n",
      "Epoch 52: [0/225] Loss: 1.054361\n",
      "Epoch 52: Validation Loss: 0.850772\n",
      "Epoch 53: [0/225] Loss: 1.177828\n",
      "Epoch 53: Validation Loss: 0.854899\n",
      "Epoch 54: [0/225] Loss: 0.995648\n",
      "Epoch 54: Validation Loss: 0.858162\n",
      "Epoch 55: [0/225] Loss: 1.032951\n",
      "Epoch 55: Validation Loss: 0.861961\n",
      "Epoch 56: [0/225] Loss: 0.785386\n",
      "Epoch 56: Validation Loss: 0.861080\n",
      "Epoch 57: [0/225] Loss: 0.917388\n",
      "Epoch 57: Validation Loss: 0.874252\n",
      "Epoch 58: [0/225] Loss: 0.891026\n",
      "Epoch 58: Validation Loss: 0.876193\n",
      "Epoch 59: [0/225] Loss: 0.752897\n",
      "Epoch 59: Validation Loss: 0.862331\n",
      "Epoch 60: [0/225] Loss: 0.719854\n",
      "Epoch 60: Validation Loss: 0.865139\n",
      "Epoch 61: [0/225] Loss: 0.913035\n",
      "Epoch 61: Validation Loss: 0.865011\n",
      "Epoch 62: [0/225] Loss: 0.672174\n",
      "Epoch 62: Validation Loss: 0.864726\n",
      "Epoch 63: [0/225] Loss: 0.820318\n",
      "Epoch 63: Validation Loss: 0.854337\n",
      "Epoch 64: [0/225] Loss: 1.013395\n",
      "Epoch 64: Validation Loss: 0.868433\n",
      "Epoch 65: [0/225] Loss: 1.086566\n",
      "Epoch 65: Validation Loss: 0.845323\n",
      "Epoch 66: [0/225] Loss: 0.773526\n",
      "Epoch 66: Validation Loss: 0.851946\n",
      "Epoch 67: [0/225] Loss: 0.910640\n",
      "Epoch 67: Validation Loss: 0.874056\n",
      "Epoch 68: [0/225] Loss: 0.723932\n",
      "Epoch 68: Validation Loss: 0.848618\n",
      "Epoch 69: [0/225] Loss: 0.715754\n",
      "Epoch 69: Validation Loss: 0.843225\n",
      "Epoch 70: [0/225] Loss: 0.704394\n",
      "Epoch 70: Validation Loss: 0.853316\n",
      "Epoch 71: [0/225] Loss: 0.877054\n",
      "Epoch 71: Validation Loss: 0.852790\n",
      "Epoch 72: [0/225] Loss: 0.775118\n",
      "Epoch 72: Validation Loss: 0.853474\n",
      "Epoch 73: [0/225] Loss: 0.690573\n",
      "Epoch 73: Validation Loss: 0.835607\n",
      "Epoch 74: [0/225] Loss: 0.591241\n",
      "Epoch 74: Validation Loss: 0.848358\n",
      "Epoch 75: [0/225] Loss: 1.097526\n",
      "Epoch 75: Validation Loss: 0.866685\n",
      "Epoch 76: [0/225] Loss: 1.049985\n",
      "Epoch 76: Validation Loss: 0.855399\n",
      "Epoch 77: [0/225] Loss: 0.877145\n",
      "Epoch 77: Validation Loss: 0.892645\n",
      "Epoch 78: [0/225] Loss: 0.706477\n",
      "Epoch 78: Validation Loss: 0.861231\n",
      "Epoch 79: [0/225] Loss: 0.857516\n",
      "Epoch 79: Validation Loss: 0.847854\n",
      "Epoch 80: [0/225] Loss: 0.723765\n",
      "Epoch 80: Validation Loss: 0.862808\n",
      "Epoch 81: [0/225] Loss: 0.869626\n",
      "Epoch 81: Validation Loss: 0.861030\n",
      "Epoch 82: [0/225] Loss: 0.854070\n",
      "Epoch 82: Validation Loss: 0.884763\n",
      "Epoch 83: [0/225] Loss: 0.895049\n",
      "Epoch 83: Validation Loss: 0.863360\n",
      "Epoch 84: [0/225] Loss: 0.597484\n",
      "Epoch 84: Validation Loss: 0.873216\n",
      "Epoch 85: [0/225] Loss: 1.044665\n",
      "Epoch 85: Validation Loss: 0.851006\n",
      "Epoch 86: [0/225] Loss: 0.927082\n",
      "Epoch 86: Validation Loss: 0.842473\n",
      "Epoch 87: [0/225] Loss: 0.838751\n",
      "Epoch 87: Validation Loss: 0.846800\n",
      "Epoch 88: [0/225] Loss: 1.023302\n",
      "Epoch 88: Validation Loss: 0.852169\n",
      "Epoch 89: [0/225] Loss: 0.899087\n",
      "Epoch 89: Validation Loss: 0.846141\n",
      "Epoch 90: [0/225] Loss: 0.577587\n",
      "Epoch 90: Validation Loss: 0.843610\n",
      "Epoch 91: [0/225] Loss: 0.974643\n",
      "Epoch 91: Validation Loss: 0.846874\n",
      "Epoch 92: [0/225] Loss: 0.751126\n",
      "Epoch 92: Validation Loss: 0.852335\n",
      "Epoch 93: [0/225] Loss: 0.669450\n",
      "Epoch 93: Validation Loss: 0.854967\n",
      "Epoch 94: [0/225] Loss: 0.809494\n",
      "Epoch 94: Validation Loss: 0.834721\n",
      "Epoch 95: [0/225] Loss: 0.987559\n",
      "Epoch 95: Validation Loss: 0.828429\n",
      "Epoch 96: [0/225] Loss: 0.741112\n",
      "Epoch 96: Validation Loss: 0.860487\n",
      "Epoch 97: [0/225] Loss: 0.791192\n",
      "Epoch 97: Validation Loss: 0.833197\n",
      "Epoch 98: [0/225] Loss: 0.739849\n",
      "Epoch 98: Validation Loss: 0.850392\n",
      "Epoch 99: [0/225] Loss: 0.807643\n",
      "Epoch 99: Validation Loss: 0.844066\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import resnet50\n",
    "from torchvision.models.vgg import vgg16_bn\n",
    "from torchvision.models.densenet import densenet121\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from learn2learn.data import MetaDataset\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=16, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding_network = nn.Sequential(\n",
    "            nn.Linear(patch_size * patch_size * 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embed_dim),\n",
    "            nn.LayerNorm(embed_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # Split image into patches\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5)  # (B, H/16, W/16, C, patch_size, patch_size)\n",
    "        \n",
    "        # Flatten patches while keeping spatial information\n",
    "        patch_vectors = patches.reshape(B, H//self.patch_size, W//self.patch_size, -1)\n",
    "        \n",
    "        # Create embeddings for each patch\n",
    "        embeddings = self.embedding_network(patch_vectors)  # (B, 14, 14, embed_dim)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class EfficientNetPatchB4(nn.Module):\n",
    "    def __init__(self, patch_size=16, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate patch embeddings with spatial information preserved\n",
    "        patch_embeddings = self.patch_embed(x)  # (batch_size, 14, 14, embed_dim)\n",
    "        patch_embeddings = F.normalize(patch_embeddings, p=2, dim=-1)\n",
    "        return patch_embeddings\n",
    "\n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super().__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.embedding_net(x1)\n",
    "        output2 = self.embedding_net(x2)\n",
    "        output3 = self.embedding_net(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)\n",
    "\n",
    "class SpatialTripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(dim=-1)  # Sum over embedding dimension\n",
    "        distance_negative = (anchor - negative).pow(2).sum(dim=-1)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root=None, transforms=None):\n",
    "        img_folder = ImageFolder(root=root)\n",
    "        meta_data = MetaDataset(img_folder)\n",
    "        \n",
    "        self.img_list = img_folder.imgs\n",
    "        self.labels_to_indices = meta_data.labels_to_indices\n",
    "        self.indices_to_labels = meta_data.indices_to_labels\n",
    "        self.labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_anchor = self.img_list[index][0]\n",
    "        label_anchor = self.img_list[index][1]\n",
    "\n",
    "        idx_positive = random.choice(self.labels_to_indices[label_anchor])            \n",
    "        img_positive = self.img_list[idx_positive][0]\n",
    "        \n",
    "        aux_class = random.choice(list(set(self.labels) - set([label_anchor])))\n",
    "        idx_negative = random.choice(self.labels_to_indices[aux_class])\n",
    "        img_negative = self.img_list[idx_negative][0]\n",
    "\n",
    "        img_anchor = Image.open(img_anchor).convert('RGB')\n",
    "        img_positive = Image.open(img_positive).convert('RGB')\n",
    "        img_negative = Image.open(img_negative).convert('RGB')\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_anchor = self.transforms(img_anchor)\n",
    "            img_positive = self.transforms(img_positive)\n",
    "            img_negative = self.transforms(img_negative)\n",
    "\n",
    "        return (img_anchor, img_positive, img_negative), []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "def get_triplet_dataloader(root=None, batch_size=1, transforms=None):\n",
    "    dataset = TripletDataset(root=root, transforms=transforms)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def get_train_transforms():\n",
    "    return T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.RandomHorizontalFlip(0.5),\n",
    "        T.RandomVerticalFlip(0.5),\n",
    "        T.RandomApply([T.RandomRotation(10)], 0.25),\n",
    "        T.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, device, log_interval=100):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            img_a, img_p, img_n = data\n",
    "            img_a, img_p, img_n = img_a.to(device), img_p.to(device), img_n.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(img_a, img_p, img_n)\n",
    "            loss = loss_fn(*embeddings)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(f'Epoch {epoch}: [{batch_idx * len(img_a)}/{len(train_loader.dataset)}] '\n",
    "                      f'Loss: {total_loss / (batch_idx + 1):.6f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in val_loader:\n",
    "                img_a, img_p, img_n = data\n",
    "                img_a, img_p, img_n = img_a.to(device), img_p.to(device), img_n.to(device)\n",
    "                embeddings = model(img_a, img_p, img_n)\n",
    "                val_loss += loss_fn(*embeddings).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch {epoch}: Validation Loss: {val_loss:.6f}')\n",
    "\n",
    "# Initialize model and training\n",
    "path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "embedding_net = EfficientNetPatchB4(patch_size=16, embed_dim=128)\n",
    "triplet_model = TripletNet(embedding_net=embedding_net)\n",
    "loss_fn = SpatialTripletLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(triplet_model.parameters(), lr=1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    triplet_model = triplet_model.cuda()\n",
    "\n",
    "# Create dataloaders\n",
    "triplet_train_loader = get_triplet_dataloader(\n",
    "    root=path_data + '/train/',\n",
    "    batch_size=5,\n",
    "    transforms=get_train_transforms()\n",
    ")\n",
    "triplet_val_loader = get_triplet_dataloader(\n",
    "    root=path_data + '/val/',\n",
    "    batch_size=5,\n",
    "    transforms=get_val_transforms()\n",
    ")\n",
    "\n",
    "# Train\n",
    "n_epochs = 100\n",
    "fit(triplet_train_loader, triplet_val_loader, triplet_model, loss_fn, optimizer, scheduler, n_epochs, device)\n",
    "torch.save(triplet_model, \"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/Patch_emdebbing_generator_triplet_model.h5\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "141d8d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████▌                                                    | 3/8 [00:00<00:00, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████                     | 6/8 [00:00<00:00, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([1, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                                | 2/65 [00:00<00:05, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████                                                                              | 4/65 [00:00<00:05, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                           | 6/65 [00:00<00:05, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▏                                                                        | 8/65 [00:00<00:05, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▌                                                                     | 10/65 [00:00<00:05, 10.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████▋                                                                | 14/65 [00:01<00:04, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▏                                                             | 16/65 [00:01<00:04, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████████▏                                                        | 20/65 [00:01<00:04, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████▊                                                      | 22/65 [00:02<00:04, 10.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████▎                                                   | 24/65 [00:02<00:03, 10.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▊                                                 | 26/65 [00:02<00:03, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████▎                                              | 28/65 [00:02<00:03, 10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████████████████▊                                            | 30/65 [00:02<00:03, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████▉                                       | 34/65 [00:03<00:03,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████▍                                    | 36/65 [00:03<00:02,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████████████████████████████████▍                               | 40/65 [00:03<00:02, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████████████████████████▉                             | 42/65 [00:03<00:02, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████████████████████████████████████                        | 46/65 [00:04<00:01, 11.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████████████▌                     | 48/65 [00:04<00:01, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 52/65 [00:04<00:01, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████              | 54/65 [00:05<00:00, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████████████████████████████████████████████▏        | 58/65 [00:05<00:00, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 60/65 [00:05<00:00, 11.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████▏   | 62/65 [00:05<00:00, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:05<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([28, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                                | 2/129 [00:00<00:10, 11.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                               | 4/129 [00:00<00:10, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▊                                                                              | 6/129 [00:00<00:09, 12.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████                                                                             | 8/129 [00:00<00:09, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▎                                                                          | 10/129 [00:00<00:09, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▌                                                                         | 12/129 [00:00<00:09, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▊                                                                        | 14/129 [00:01<00:09, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████                                                                       | 16/129 [00:01<00:09, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▎                                                                     | 18/129 [00:01<00:09, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▌                                                                    | 20/129 [00:01<00:09, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▊                                                                   | 22/129 [00:01<00:09, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████                                                                  | 24/129 [00:02<00:08, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▎                                                                | 26/129 [00:02<00:08, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▊                                                              | 30/129 [00:02<00:08, 11.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████                                                             | 32/129 [00:02<00:08, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▌                                                          | 36/129 [00:03<00:09,  9.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████████                                                        | 40/129 [00:03<00:08, 10.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████▎                                                      | 42/129 [00:03<00:08, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████▉                                                    | 46/129 [00:04<00:06, 12.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████▏                                                  | 48/129 [00:04<00:06, 12.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▋                                                | 52/129 [00:04<00:06, 12.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████▏                                             | 56/129 [00:04<00:05, 12.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████▍                                            | 58/129 [00:04<00:05, 13.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████▉                                          | 62/129 [00:05<00:04, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████▏                                        | 64/129 [00:05<00:04, 13.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████████████████████▋                                      | 68/129 [00:05<00:04, 12.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████▉                                     | 70/129 [00:05<00:04, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████████████████████████████████▍                                  | 74/129 [00:06<00:04, 11.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████████████████▋                                 | 76/129 [00:06<00:04, 12.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████████████████████████████████▏                              | 80/129 [00:06<00:04, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████████████████████▍                             | 82/129 [00:06<00:04, 11.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████                           | 86/129 [00:07<00:03, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████████████████████████████████▎                         | 88/129 [00:07<00:03, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████▌                        | 90/129 [00:07<00:03, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████████                      | 94/129 [00:07<00:03, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████████████▎                    | 96/129 [00:08<00:03, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████████████                  | 100/129 [00:08<00:02, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████████████████████████████████████████████████████████████▎                | 102/129 [00:08<00:02, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████████████████████████████████████▍               | 104/129 [00:08<00:02, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████████████████▉             | 108/129 [00:09<00:01, 10.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████▏           | 110/129 [00:09<00:01, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|█████████████████████████████████████████████████████████████████████▍          | 112/129 [00:09<00:01, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████▉        | 116/129 [00:10<00:01, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████████████████████████████████████████████████████████████████████▏      | 118/129 [00:10<00:01,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████████████████████████████████████     | 121/129 [00:10<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████▎   | 123/129 [00:10<00:00,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████████████████████████████████████████▉   | 124/129 [00:10<00:00,  9.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:11<00:00, 11.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([32, 14, 14, 128])\n",
      "Size of embeddings: torch.Size([26, 14, 14, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|█▌                                                                               | 10/500 [00:06<05:31,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=0.0479, Val Loss=1.6531, Val Acc=0.5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                             | 20/500 [00:13<05:25,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss=0.0300, Val Loss=1.3057, Val Acc=0.5934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▊                                                                            | 30/500 [00:20<05:12,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss=0.0173, Val Loss=1.2359, Val Acc=0.6329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▍                                                                          | 40/500 [00:27<05:20,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Loss=0.0079, Val Loss=1.0680, Val Acc=0.6744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                         | 50/500 [00:33<04:56,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss=0.0039, Val Loss=1.1141, Val Acc=0.6850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▋                                                                       | 60/500 [00:40<04:48,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Train Loss=0.0026, Val Loss=1.0961, Val Acc=0.6845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▎                                                                     | 70/500 [00:47<04:43,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: Train Loss=0.0017, Val Loss=1.1347, Val Acc=0.6879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▉                                                                    | 80/500 [00:53<04:35,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: Train Loss=0.0013, Val Loss=1.1421, Val Acc=0.6941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████▌                                                                  | 90/500 [01:00<04:29,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: Train Loss=0.0010, Val Loss=1.1501, Val Acc=0.6975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████                                                                | 100/500 [01:06<04:23,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train Loss=0.0008, Val Loss=1.1678, Val Acc=0.6951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████▌                                                              | 110/500 [01:13<04:16,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110: Train Loss=0.0007, Val Loss=1.1871, Val Acc=0.6936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████▏                                                            | 120/500 [01:20<04:09,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120: Train Loss=0.0006, Val Loss=1.2032, Val Acc=0.6946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▊                                                           | 130/500 [01:26<04:08,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130: Train Loss=0.0006, Val Loss=1.2109, Val Acc=0.6951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▍                                                         | 140/500 [01:33<03:56,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140: Train Loss=0.0005, Val Loss=1.2331, Val Acc=0.6908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████                                                        | 150/500 [01:39<03:52,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150: Train Loss=0.0004, Val Loss=1.2364, Val Acc=0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████▌                                                      | 160/500 [01:46<03:44,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160: Train Loss=0.0004, Val Loss=1.2507, Val Acc=0.6917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████▏                                                    | 170/500 [01:53<03:37,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170: Train Loss=0.0004, Val Loss=1.2644, Val Acc=0.6936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████▊                                                   | 180/500 [01:59<03:31,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180: Train Loss=0.0003, Val Loss=1.2763, Val Acc=0.6936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████▍                                                 | 190/500 [02:06<03:23,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190: Train Loss=0.0003, Val Loss=1.2927, Val Acc=0.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████                                                | 200/500 [02:13<03:24,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: Train Loss=0.0003, Val Loss=1.3007, Val Acc=0.6898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████▌                                              | 210/500 [02:19<03:13,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210: Train Loss=0.0003, Val Loss=1.3003, Val Acc=0.6946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████▏                                            | 220/500 [02:26<03:06,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220: Train Loss=0.0003, Val Loss=1.3110, Val Acc=0.6903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████▊                                           | 230/500 [02:33<03:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230: Train Loss=0.0003, Val Loss=1.3125, Val Acc=0.6946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████▍                                         | 240/500 [02:39<02:56,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240: Train Loss=0.0002, Val Loss=1.3284, Val Acc=0.6903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████                                        | 250/500 [02:46<02:45,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250: Train Loss=0.0002, Val Loss=1.3396, Val Acc=0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████▌                                      | 260/500 [02:53<02:38,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260: Train Loss=0.0002, Val Loss=1.3369, Val Acc=0.6956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████▏                                    | 270/500 [02:59<02:31,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270: Train Loss=0.0002, Val Loss=1.3421, Val Acc=0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████████████████▊                                   | 280/500 [03:06<02:25,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280: Train Loss=0.0002, Val Loss=1.3561, Val Acc=0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|██████████████████████████████████████████████▍                                 | 290/500 [03:13<02:18,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290: Train Loss=0.0002, Val Loss=1.3559, Val Acc=0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████                                | 300/500 [03:19<02:11,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300: Train Loss=0.0002, Val Loss=1.3576, Val Acc=0.6956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████▌                              | 310/500 [03:26<02:05,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310: Train Loss=0.0002, Val Loss=1.3721, Val Acc=0.6946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████████████████████▏                            | 320/500 [03:32<01:58,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320: Train Loss=0.0002, Val Loss=1.3654, Val Acc=0.6936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████▊                           | 330/500 [03:39<01:52,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330: Train Loss=0.0002, Val Loss=1.3740, Val Acc=0.6946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████▍                         | 340/500 [03:46<01:45,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340: Train Loss=0.0002, Val Loss=1.3830, Val Acc=0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████                        | 350/500 [03:52<01:38,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350: Train Loss=0.0002, Val Loss=1.3923, Val Acc=0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████████████████▌                      | 360/500 [03:59<01:32,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360: Train Loss=0.0002, Val Loss=1.3914, Val Acc=0.6898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████████████████████████████████████████████████████████▏                    | 370/500 [04:06<01:26,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370: Train Loss=0.0002, Val Loss=1.3894, Val Acc=0.6927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████████████████████████████████████▊                   | 380/500 [04:12<01:21,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380: Train Loss=0.0002, Val Loss=1.3943, Val Acc=0.6917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████████████▍                 | 390/500 [04:19<01:12,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 390: Train Loss=0.0002, Val Loss=1.3931, Val Acc=0.6951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████                | 400/500 [04:25<01:05,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400: Train Loss=0.0001, Val Loss=1.3957, Val Acc=0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████████████████████████████████████▌              | 410/500 [04:32<01:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 410: Train Loss=0.0002, Val Loss=1.3993, Val Acc=0.6941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████████████████████████████████████▏            | 420/500 [04:39<00:53,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 420: Train Loss=0.0001, Val Loss=1.4007, Val Acc=0.6917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████▊           | 430/500 [04:45<00:47,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430: Train Loss=0.0001, Val Loss=1.4003, Val Acc=0.6927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████████████████████████████████████████▍         | 440/500 [04:52<00:39,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 440: Train Loss=0.0002, Val Loss=1.4022, Val Acc=0.6941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████        | 450/500 [04:59<00:32,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450: Train Loss=0.0001, Val Loss=1.4014, Val Acc=0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████████████████████████████████████████▌      | 460/500 [05:05<00:26,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460: Train Loss=0.0001, Val Loss=1.4034, Val Acc=0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████████████████████████████████████▏    | 470/500 [05:12<00:19,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470: Train Loss=0.0002, Val Loss=1.4043, Val Acc=0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████████████████████████████████████████▊   | 480/500 [05:18<00:13,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480: Train Loss=0.0001, Val Loss=1.4053, Val Acc=0.6927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████▍ | 490/500 [05:25<00:06,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490: Train Loss=0.0002, Val Loss=1.4054, Val Acc=0.6927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [05:32<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train Loss=0.0002, Val Loss=1.4054, Val Acc=0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "#sys.path.insert(0,'C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "sys.path.insert(0,'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/')\n",
    "from dataloaders import get_val_transforms\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import dataloaders\n",
    "from dataloaders import get_train_transforms, get_val_transforms, get_triplet_dataloader\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score , precision_score , recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "class RefinedViT(nn.Module):\n",
    "    def __init__(self, num_classes=15):\n",
    "        super(RefinedViT, self).__init__()\n",
    "        self.num_patches = 196  # 14x14\n",
    "        self.embed_dim = 128    # Match previous network\n",
    "        self.num_heads = 8      \n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=10)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc = nn.Linear(self.embed_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(batch_size, 196, -1)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "def generate_embeddings(data_loader, model, device):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_imgs, batch_labels in tqdm(data_loader):\n",
    "            batch_imgs = batch_imgs.to(device)\n",
    "            embeddings = model.get_embedding(batch_imgs)  # Using get_embedding from triplet model\n",
    "            print(f\"Size of embeddings: {embeddings.shape}\")\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            all_labels.append(batch_labels.numpy())\n",
    "    \n",
    "    return np.concatenate(all_embeddings), np.concatenate(all_labels)\n",
    "\n",
    "def train_and_validate(model, train_embeddings, train_labels, val_embeddings, val_labels, \n",
    "                      num_epochs=500, batch_size=32, learning_rate=1e-4):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    X_train = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(val_embeddings, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_X = X_train[i:i+batch_size]\n",
    "            batch_y = y_train[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "            val_acc = (torch.argmax(val_outputs, dim=1) == y_val).float().mean().item()\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), 'best_model_refined_vit.pth')\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss={total_train_loss/len(X_train):.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    path_data =  'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "    \n",
    "    # Load triplet model\n",
    "    #triplet_model = torch.load('C:/Users/Mey/Documents/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/Patch_emdebbing_generator_triplet_model.h5', map_location=device)\n",
    "    triplet_model.eval()\n",
    "    \n",
    "    # Data loaders\n",
    "    transform = get_val_transforms()\n",
    "    train_data = torchvision.datasets.ImageFolder(root=f'{path_data}/train/', transform=transform)\n",
    "    val_data = torchvision.datasets.ImageFolder(root=f'{path_data}/val/', transform=transform)\n",
    "    test_data = torchvision.datasets.ImageFolder(root=f'{path_data}/test/', transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    train_embeddings, train_labels = generate_embeddings(train_loader, triplet_model, device)\n",
    "    val_embeddings, val_labels = generate_embeddings(val_loader, triplet_model, device)\n",
    "    test_embeddings, test_labels = generate_embeddings(test_loader, triplet_model, device)\n",
    "    \n",
    "    # Train RefinedViT\n",
    "    model = RefinedViT(num_classes=15)\n",
    "    trained_model = train_and_validate(model, train_embeddings, train_labels, val_embeddings, val_labels)\n",
    "    \n",
    "    return trained_model, (test_embeddings, test_labels)\n",
    "\n",
    "\n",
    "model, test_data = main()  \n",
    "torch.save(model, \"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/Patch_Rvit_triplet_model.h5\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195d036-4cf9-4c6c-934e-4f1b1c084a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import AdamW\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Improved RefinedViT Model\n",
    "class RefinedViT(nn.Module):\n",
    "    def __init__(self, num_classes=15):\n",
    "        super(RefinedViT, self).__init__()\n",
    "        self.num_patches = 196  # 14x14\n",
    "        self.embed_dim = 128    # Match previous network\n",
    "        self.num_heads = 8      \n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=12)  # Increased layers\n",
    "        \n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(batch_size, 196, -1)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Generate Embeddings\n",
    "def generate_embeddings(data_loader, model, device):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_imgs, batch_labels in tqdm(data_loader):\n",
    "            batch_imgs = batch_imgs.to(device)\n",
    "            embeddings = model.get_embedding(batch_imgs)  # Using get_embedding from triplet model\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            all_labels.append(batch_labels.numpy())\n",
    "    \n",
    "    return np.concatenate(all_embeddings), np.concatenate(all_labels)\n",
    "\n",
    "# Train and Validate\n",
    "def train_and_validate(model, train_embeddings, train_labels, val_embeddings, val_labels, \n",
    "                      num_epochs=500, batch_size=32, learning_rate=1e-4):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    X_train = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(val_embeddings, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_X = X_train[i:i+batch_size]\n",
    "            batch_y = y_train[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "            val_acc = (torch.argmax(val_outputs, dim=1) == y_val).float().mean().item()\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), 'best_model_refined_vit.pth')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss={total_train_loss/len(X_train):.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "    \n",
    "    # Load triplet model\n",
    "    triplet_model = torch.load('f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/Patch_emdebbing_generator_triplet_model.h5', map_location=device)\n",
    "    triplet_model.eval()\n",
    "    \n",
    "    # Data loaders\n",
    "    transform = get_val_transforms()\n",
    "    train_data = ImageFolder(root=f'{path_data}/train/', transform=transform)\n",
    "    val_data = ImageFolder(root=f'{path_data}/val/', transform=transform)\n",
    "    test_data = ImageFolder(root=f'{path_data}/test/', transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    train_embeddings, train_labels = generate_embeddings(train_loader, triplet_model, device)\n",
    "    val_embeddings, val_labels = generate_embeddings(val_loader, triplet_model, device)\n",
    "    test_embeddings, test_labels = generate_embeddings(test_loader, triplet_model, device)\n",
    "    \n",
    "    # Train RefinedViT\n",
    "    model = RefinedViT(num_classes=15)\n",
    "    trained_model = train_and_validate(model, train_embeddings, train_labels, val_embeddings, val_labels)\n",
    "    \n",
    "    return trained_model, (test_embeddings, test_labels)\n",
    "\n",
    "# Run the main function\n",
    "model, test_data = main()  \n",
    "torch.save(model, \"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/Patch_Rvit_triplet_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdb3ab-a3f1-4d23-9f9a-8fc7d256296f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5148\\3255058837.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  triplet_model = torch.load('f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/Patch_emdebbing_generator_triplet_model.h5', map_location=device)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000018B0F2AA9D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pc\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\pc\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1435, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 11.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:05<00:00, 11.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:10<00:00, 11.85it/s]\n",
      "  1%|█▏                                                                                | 7/500 [00:08<10:06,  1.23s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import AdamW\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Improved RefinedViT Model\n",
    "class RefinedViT(nn.Module):\n",
    "    def __init__(self, num_classes=15):\n",
    "        super(RefinedViT, self).__init__()\n",
    "        self.num_patches = 196  # 14x14\n",
    "        self.embed_dim = 128    # Match previous network\n",
    "        self.num_heads = 16      \n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=12)  # Increased layers\n",
    "        \n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(batch_size, 196, -1)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Generate Embeddings\n",
    "def generate_embeddings(data_loader, model, device):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_imgs, batch_labels in tqdm(data_loader):\n",
    "            batch_imgs = batch_imgs.to(device)\n",
    "            embeddings = model.get_embedding(batch_imgs)  # Using get_embedding from triplet model\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            all_labels.append(batch_labels.numpy())\n",
    "    \n",
    "    return np.concatenate(all_embeddings), np.concatenate(all_labels)\n",
    "\n",
    "# Train and Validate\n",
    "def train_and_validate(model, train_embeddings, train_labels, val_embeddings, val_labels, \n",
    "                      num_epochs=500, batch_size=32, learning_rate=1e-4):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    X_train = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(val_embeddings, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_X = X_train[i:i+batch_size]\n",
    "            batch_y = y_train[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "            val_acc = (torch.argmax(val_outputs, dim=1) == y_val).float().mean().item()\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), 'best_model_refined_vit.pth')\n",
    "                patience_counter = 0\n",
    "            # else:\n",
    "            #     patience_counter += 1\n",
    "            #     if patience_counter >= early_stopping_patience:\n",
    "            #         print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            #         break\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss={total_train_loss/len(X_train):.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    path_data = 'f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/src/dataset'\n",
    "    \n",
    "    # Load triplet model\n",
    "    triplet_model = torch.load('f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/Patch_emdebbing_generator_triplet_model.h5', map_location=device)\n",
    "    triplet_model.eval()\n",
    "    \n",
    "    # Data loaders\n",
    "    transform = get_val_transforms()\n",
    "    train_data = ImageFolder(root=f'{path_data}/train/', transform=transform)\n",
    "    val_data = ImageFolder(root=f'{path_data}/val/', transform=transform)\n",
    "    test_data = ImageFolder(root=f'{path_data}/test/', transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    train_embeddings, train_labels = generate_embeddings(train_loader, triplet_model, device)\n",
    "    val_embeddings, val_labels = generate_embeddings(val_loader, triplet_model, device)\n",
    "    test_embeddings, test_labels = generate_embeddings(test_loader, triplet_model, device)\n",
    "    \n",
    "    # Train RefinedViT\n",
    "    model = RefinedViT(num_classes=15)\n",
    "    trained_model = train_and_validate(model, train_embeddings, train_labels, val_embeddings, val_labels)\n",
    "    \n",
    "    return trained_model, (test_embeddings, test_labels)\n",
    "\n",
    "# Run the main function\n",
    "model, test_data = main()  \n",
    "torch.save(model, \"f:/Meysam-Khodarahi/PlantDiseaseDiagnosisFewShotLearning/siamese_triplet_net/Patch_Rvit_triplet_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65d7f4-c44d-4f09-a7cd-be7f78141f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
